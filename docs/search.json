[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R as GIS for Economists",
    "section": "",
    "text": "Welcome\nThis book is specifically aimed at spatial data processing for empirical statistical projects, where spatial variables become part of the analysis dataset. Over the years, I have seen many students and researchers spend excessive time just processing spatial data, often by endlessly clicking through the ArcGIS (or QGIS) user interface. From an academic productivity perspective, this is a waste of time. My hope is that this book helps researchers become more proficient in spatial data processing, ultimately enhancing productivity in economics and other scientific fields where spatial data is essential.\nAbout me\nI am an Associate Professor in the Department of Agricultural Economics at the University of Nebraska-Lincoln. My research interests lie in precision agriculture, water economics, and agricultural policy. You can visit my personal website here.\nContributors of the book\nHere is the list of contributors to the book and the parts they contributed to:\n\n\nKyle Butts, PhD candidate, University of Colorado Boulder\n\nSection 1.6 African Economy and Slaves: Nunn 2008, 1.7 Terrain Ruggedness and Economic Development in Africa: Nunn 2012, and 1.8 TseTse fly suitability index: Alsan 2015\n\n\n\n\nBowen Chen, Data Scientist, Bunge, Missouri\n\nSection 8.2 CDL with CropScapeR\n\n\n\n\nShinya Uryu, Assistant Professor, Center for Design-Oriented AI Education and Research, Tokushima University, Japan (Github account: https://github.com/uribo)\n\nSection 7  Creating Maps using ggplot2\n\n\n\n\nGal Koss, Graduate Student, Colorado State University\n\nSection 4.6 Working with netCDFs and 8.5 gridMET\n\n\n\n\nJude Bayham, Assistant Professor, Colorado State University\n\nSection 4.6 Working with netCDFs and 8.5 gridMET\n\n\n\n\nComments and Suggestions?\nAny constructive comments and suggestions about how I can improve the book are all welcome. Please send me an email at tmieno2@unl.edu or create an issue on the Github page of this book.\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/00-preface.html",
    "href": "chapters/00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "How is this book any different from other online books and resources?\nWe are seeing an explosion of online (and free) resources that teach how to use R for spatial data processing.1 Here is an incomplete list of such resources:\nThanks to all these resources, it has become much easier to self-teach R for GIS work than 10 years ago when I first started using R for GIS. Even though I have not read through all these resources carefully, I am pretty sure every topic found in this book can also be found somewhere in these resources (except the demonstrations). So, you may wonder why on earth you can benefit from reading this book. It all boils down to search costs. Researchers in different disciplines require different sets of spatial data skills. The available resources are typically very general covering so many topics, some of which economists are unlikely to use. It is particularly hard for those who do not have much experience in GIS to identify whether particular skills are essential or not. So, they could spend so much time learning something that is not really useful. The value of this book lies in its deliberate incomprehensiveness. It only packages materials that satisfy the need of most economists, cutting out many topics that are likely to be of limited use for economists.\nFor those who are looking for more comprehensive treatments of spatial data handling and processing in one book, I personally like Geocomputation with R a lot. Increasingly, the developer of R packages created a website dedicated to their R packages, where you can often find vignettes (tutorials), like Simple Features for R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00-preface.html#how-is-this-book-any-different-from-other-online-books-and-resources",
    "href": "chapters/00-preface.html#how-is-this-book-any-different-from-other-online-books-and-resources",
    "title": "Preface",
    "section": "",
    "text": "1 This phenomenon is largely thanks to packages like bookdown (Xie 2016), blogdown (Xie, Hill, and Thomas 2017), and pkgdown (Wickham and Hesselberth 2020) that has significantly lowered the cost of professional contents creation than before. Indeed, this book was built taking advantage of the bookdown package.\nGeocomputation with R\nSpatial Data Science\nSpatial Data Science with R\nIntroduction to GIS using R\nCode for An Introduction to Spatial Analysis and Mapping in R\nIntroduction to GIS in R\nIntro to GIS and Spatial Analysis\nIntroduction to Spatial Data Programming with R\nReproducible GIS analysis with R\nR for Earth-System Science\nRspatial\nNEON Data Skills\nSimple Features for R\n\nGeospatial Health Data: Modeling and Visualization with R-INLA and Shiny",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00-preface.html#topics-covered-in-this-book",
    "href": "chapters/00-preface.html#topics-covered-in-this-book",
    "title": "Preface",
    "section": "Topics covered in this book",
    "text": "Topics covered in this book\nThe book starts with the very basics of spatial data handling (e.g., importing and exporting spatial datasets) and moves on to more practical spatial data operations (e.g., spatial data join) that are useful for research projects. Some parts of this books are still under development. Right now, Chapters 1 through 8, parts of Chapter 9, and Appendix A are available.\n\nChapter 1: Demonstrations of R as GIS\n\ngroundwater pumping and groundwater level\nprecision agriculture\nland use and weather\ncorn planted acreage and railroads\ngroundwater pumping and weather\nslave trade and economic development in Africa\nterrain ruggedness and economic development in Africa\nTseTse fly and economic developtment in Africa\n\n\nChapter 2: The basics of vector data handling using sf package\n\nspatial data structure in sf\n\nimport and export vector data\n(re)projection of spatial datasets\nsingle-layer geometrical operations (e.g., create buffers, find centroids)\nother miscellaneous basic operations\n\n\nChapter 3: Spatial interactions of vector datasets\n\nunderstand topological relations of multiple sf objects\nspatially subset a layer based on another layer\nextracting values from one layer to another layer\n\n\nChapter 4: The basics of raster data handling using the raster and terra packages\n\nunderstand object classes by the terra and raster packages\nimport and export raster data\nstack raster data\nquick plotting\nhandle netCDF files\n\n\nChapter 5: Spatial interactions of vector and raster datasets\n\ncropping a raster layer to the geographic extent of a vector layer\nextracting values from a raster layer to a vector layer\n\n\nChapter 6: Speed things up\n\nmake raster data extraction faster by parallelization\n\n\nChapter 7: Spatiotemporal raster data handling with the stars package\nChapter 8: Creating Maps using the ggplot2 package\n\nuse the ggplot2 packages to create maps\n\n\nChapter 9: Download and process publicly available spatial datasets (partially available)\n\nUSDA NASS QuickStat (tidyUSDA) - available\nPRISM (prism) - available\nDaymet (daymetr) - available\ngridMET - available\nCropland Data Layer (CropScapeR) - available\nSSURGO (tidycensus) - under construction\nCensus (tidycensus) - under construction\n\n\nAppendix A: Loop and parallel computation\nAppendix B: Cheatsheet - under construction\n\nAs you can see above, this book does not spend any time on the very basics of GIS concepts. Before you start reading the book, you should know the followings at least (it’s not much):\n\nWhat Geographic Coordinate System (GCS), Coordinate Reference System (CRS), and projection are (this is a good resource)\nDistinctions between vector and raster data (this is a simple summary of the difference)\n\nThis book is about spatial data processing and does not provide detailed explanations on non-spatial R operations, assuming some basic knowledge of R. In particular, the dplyr and data.table packages are extensively used for data wrangling. For data wrangling using tidyverse (a collection of packages including dplyr), see R for Data Science. For data.table, this is a good resource.\nFinally, this book does not cover spatial statistics or spatial econometrics at all. This book is about spatial data processing. Spatial analysis is something you do after you have processed spatial data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00-preface.html#conventions-of-the-book-and-some-notes",
    "href": "chapters/00-preface.html#conventions-of-the-book-and-some-notes",
    "title": "Preface",
    "section": "Conventions of the book and some notes",
    "text": "Conventions of the book and some notes\nHere are some notes of the conventions of this book and notes for R beginners and those who are not used to reading rmarkdown-generated html documents.\nTexts in gray boxes\nThey are one of the following:\n\nobjects defined on R during demonstrations\nR functions\nR packages\n\nWhen it is a function, I always put parentheses at the end like this: st_read(). Sometimes, I combine a package and function in one like this: sf::st_read(). This means it is a function called st_read() from the sf package.\nColored Boxes\nCodes are in blue boxes, and outcomes are in red boxes.\nCodes:\n\nrunif(5)\n\nOutcomes:\n\n\n[1] 0.4915999 0.7632047 0.6585179 0.7324614 0.2124914\n\n\nParentheses around codes\nSometimes you will see codes enclosed by parenthesis like this:\n\n(\n  a &lt;- runif(5)\n)\n\n[1] 0.5868415 0.7597267 0.2599178 0.9837541 0.8517167\n\n\nThe parentheses prints what’s inside of a newly created object (here a) without explicitly evaluating the object. So, basically I am signaling that we will be looking inside of the object that was just created.\nThis one prints nothing.\n\na &lt;- runif(5)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00-preface.html#session-information",
    "href": "chapters/00-preface.html#session-information",
    "title": "Preface",
    "section": "Session Information",
    "text": "Session Information\nHere is the session information when compiling the book:\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.1       htmltools_0.5.8.1 rstudioapi_0.16.0 codetools_0.2-20 \n [9] rmarkdown_2.27    knitr_1.48        jsonlite_1.8.8    xfun_0.46        \n[13] digest_0.6.36     rlang_1.1.4       evaluate_0.24.0  \n\n\n\n\n\n\n\n\nWickham, Hadley, and Jay Hesselberth. 2020. Pkgdown: Make Static HTML Documentation for a Package. https://CRAN.R-project.org/package=pkgdown.\n\n\nXie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown.\n\n\nXie, Yihui, Alison Presmanes Hill, and Amber Thomas. 2017. Blogdown: Creating Websites with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/blogdown.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html",
    "href": "chapters/01-Demonstration.html",
    "title": "1  R as GIS: Demonstrations",
    "section": "",
    "text": "Before you start\nThe primary objective of this chapter is to showcase the power of R as GIS through demonstrations via mock-up econometric research projects (first five demonstrations) and creating variables used in published articles (the last three demonstrations)1. Each demonstration project consists of a project overview (objective, datasets used, econometric model, and GIS tasks involved) and demonstration. This is really NOT a place you learn the nuts and bolts of how R does spatial operations. Indeed, we intentionally do not explain all the details of how the R codes work. We reiterate that the main purpose of the demonstrations is to get you a better idea of how R can be used to process spatial data to help your research projects involving spatial datasets. Finally, note that the mock-up projects use extremely simple econometric models that completely lacks careful thoughts you would need in real research projects. So, don’t waste your time judging the econometric models, and just focus on GIS tasks. If you are not familiar with html documents generated by rmarkdown, you might benefit from reading the conventions of the book in the Preface. Finally, for those who are interested in replicating the demonstrations, directions for replication are provided below. However, I would suggest focusing on the narratives for the first time around, learn the nuts and bolts of spatial operations from Chapters 2 through 5, and then come back to replicate them.",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#before-you-start",
    "href": "chapters/01-Demonstration.html#before-you-start",
    "title": "1  R as GIS: Demonstrations",
    "section": "",
    "text": "1 Note that this lecture does not deal with spatial econometrics at all. This lecture is about spatial data processing, not spatial econometrics.Target Audience\nThe target audience of this chapter is those who are not very familiar with R as GIS. Knowledge of R certainly helps. But, I tried to write in a way that R beginners can still understand the power of R as GIS. Do not get bogged down by all the complex-looking R codes. Just focus on the narratives and figures to get a sense of what R can do.\nDirection for replication\n\nDatasets\nRunning the codes in this chapter involves reading datasets from a disk. All the datasets that will be imported are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory\n\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo1",
    "href": "chapters/01-Demonstration.html#sec-demo1",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.1 The impact of groundwater pumping on depth to water table",
    "text": "1.1 The impact of groundwater pumping on depth to water table\n\n1.1.1 Project Overview\n\nObjective:\n\nUnderstand the impact of groundwater pumping on groundwater level.\n\n\nDatasets\n\nGroundwater pumping by irrigation wells in Chase, Dundy, and Perkins Counties in the southwest corner of Nebraska\nGroundwater levels observed at USGS monitoring wells located in the three counties and retrieved from the National Water Information System (NWIS) maintained by USGS using the dataRetrieval package.\n\n\nEconometric Model\nIn order to achieve the project objective, we will estimate the following model:\n\\[\ny_{i,t} - y_{i,t-1} = \\alpha + \\beta gw_{i,t-1} + v\n\\]\nwhere \\(y_{i,t}\\) is the depth to groundwater table2 in March3 in year \\(t\\) at USGS monitoring well \\(i\\), and \\(gw_{i,t-1}\\) is the total amount of groundwater pumping that happened within the 2-mile radius of the monitoring well \\(i\\).\n2 the distance from the surface to the top of the aquifer3 For our geographic focus of southwest Nebraska, corn is the dominant crop type. Irrigation for corn happens typically between April through September. For example, this means that changes in groundwater level (\\(y_{i,2012} - y_{i,2011}\\)) captures the impact of groundwater pumping that occurred April through September in 2011.\nGIS tasks\n\nread an ESRI shape file as an sf (spatial) object\n\nuse sf::st_read()\n\n\n\ndownload depth to water table data using the dataRetrieval package developed by USGS\n\nuse dataRetrieval::readNWISdata() and dataRetrieval::readNWISsite()\n\n\n\ncreate a buffer around USGS monitoring wells\n\nuse sf::st_buffer()\n\n\n\nconvert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects\n\nuse sf::st_as_sf() and sf::st_set_crs()\n\n\n\nreproject an sf object to another CRS\n\nuse sf::st_transform()\n\n\n\nidentify irrigation wells located inside the buffers and calculate total pumping\n\nuse sf::st_join()\n\n\n\ncreate maps\n\nuse the ggplot2 package\n\n\n\n\nPreparation for replication\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  dataRetrieval, # download USGS NWIS data\n  lubridate, # Date object handling\n  lfe, # fast regression with many fixed effects\n  tmap # mapping\n)\n\n\n1.1.2 Project Demonstration\nThe geographic focus of the project is the southwest corner of Nebraska consisting of Chase, Dundy, and Perkins County (see Figure 1.1 for their locations within Nebraska). Let’s read a shape file of the three counties represented as polygons. We will use it later to spatially filter groundwater level data downloaded from NWIS.\n\nthree_counties &lt;-\n  sf::st_read(dsn = \"Data\", layer = \"urnrd\") %&gt;%\n  #--- project to WGS84/UTM 14N ---#\n  sf::st_transform(32614)\n\nReading layer `urnrd' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395\nGeodetic CRS:  NAD83\n\n\n\n\n\n\nCode#--- map the three counties ---#\nggplot() +\n  geom_sf(data = NE_county) +\n  geom_sf(data = three_counties, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 1.1: The location of Chase, Dundy, and Perkins County in Nebraska\n\n\n\n\n\nWe have already collected groundwater pumping data, so let’s import it.\n\n#--- groundwater pumping data ---#\n(\n  urnrd_gw &lt;- readRDS(\"Data/urnrd_gw_pumping.rds\")\n)\n\n       well_id  year  vol_af      lon     lat\n         &lt;num&gt; &lt;int&gt;   &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n    1:    1706  2007 182.566 245322.3 4542717\n    2:    2116  2007  46.328 245620.9 4541125\n    3:    2583  2007  38.380 245660.9 4542523\n    4:    2597  2007  70.133 244816.2 4541143\n    5:    3143  2007 135.870 243614.0 4541579\n   ---                                       \n18668:    2006  2012 148.713 284782.5 4432317\n18669:    2538  2012 115.567 284462.6 4432331\n18670:    2834  2012  15.766 283338.0 4431341\n18671:    2834  2012 381.622 283740.4 4431329\n18672:    4983  2012      NA 284636.0 4432725\n\n\nwell_id is the unique irrigation well identifier, and vol_af is the amount of groundwater pumped in acre-feet. This dataset is just a regular data.frame with coordinates. We need to convert this dataset into a object of class sf so that we can later identify irrigation wells located within a 2-mile radius of USGS monitoring wells (see Figure 1.2) for the spatial distribution of the irrigation wells.\n\nurnrd_gw_sf &lt;-\n  urnrd_gw %&gt;%\n  #--- convert to sf ---#\n  sf::st_as_sf(coords = c(\"lon\", \"lat\")) %&gt;%\n  #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---#\n  sf::st_set_crs(32614)\n\n#--- now sf ---#\nurnrd_gw_sf\n\nSimple feature collection with 18672 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   well_id year  vol_af                 geometry\n1     1706 2007 182.566 POINT (245322.3 4542717)\n2     2116 2007  46.328 POINT (245620.9 4541125)\n3     2583 2007  38.380 POINT (245660.9 4542523)\n4     2597 2007  70.133 POINT (244816.2 4541143)\n5     3143 2007 135.870   POINT (243614 4541579)\n6     5017 2007 196.799 POINT (243539.9 4543146)\n7     1706 2008 171.250 POINT (245322.3 4542717)\n8     2116 2008 171.650 POINT (245620.9 4541125)\n9     2583 2008  46.100 POINT (245660.9 4542523)\n10    2597 2008 124.830 POINT (244816.2 4541143)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Spatial distribution of irrigation wells\n\n\n\n\n\nHere are the rest of the steps we will take to create a regression-ready dataset for our analysis.\n\ndownload groundwater level data observed at USGS monitoring wells from National Water Information System (NWIS) using the dataRetrieval package\nidentify the irrigation wells located within the 2-mile radius of the USGS wells and calculate the total groundwater pumping that occurred around each of the USGS wells by year\nmerge the groundwater pumping data to the groundwater level data\n\n\nLet’s download groundwater level data from NWIS first. The following code downloads groundwater level data for Nebraska from Jan 1, 1990, through Jan 1, 2016 (This would take a while if you try to run this yourself).\n\n#--- download groundwater level data ---#\nNE_gwl &lt;-\n  lapply(\n    1990:2015,\n    \\(year) {\n      dataRetrieval::readNWISdata(\n        stateCd = \"Nebraska\",\n        startDate = paste0(year, \"-01-01\"),\n        endDate = paste0(year + 1, \"-01-01\"),\n        service = \"gwlevels\"\n      )\n    }\n  ) %&gt;%\n  dplyr::bind_rows() %&gt;%\n  dplyr::select(site_no, lev_dt, lev_va) %&gt;%\n  dplyr::rename(date = lev_dt, dwt = lev_va)\n\n#--- take a look ---#\nhead(NE_gwl, 10)\n\n\n\n           site_no       date   dwt\n1  400008097545301 2000-11-08 17.40\n2  400008097545301 2008-10-09 13.99\n3  400008097545301 2009-04-09 11.32\n4  400008097545301 2009-10-06 15.54\n5  400008097545301 2010-04-12 11.15\n6  400008100050501 1990-03-15 24.80\n7  400008100050501 1990-10-04 27.20\n8  400008100050501 1991-03-08 24.20\n9  400008100050501 1991-10-07 26.90\n10 400008100050501 1992-03-02 24.70\n\n\nsite_no is the unique monitoring well identifier, date is the date of groundwater level monitoring, and dwt is depth to water table.\nWe calculate the average groundwater level in March by USGS monitoring well (right before the irrigation season starts):\n\n#--- Average depth to water table in March ---#\nNE_gwl_march &lt;-\n  NE_gwl %&gt;%\n  dplyr::mutate(\n    date = as.Date(date),\n    month = lubridate::month(date),\n    year = lubridate::year(date),\n  ) %&gt;%\n  #--- select observation in March ---#\n  dplyr::filter(year &gt;= 2007, month == 3) %&gt;%\n  #--- gwl average in March ---#\n  dplyr::group_by(site_no, year) %&gt;%\n  dplyr::summarize(dwt = mean(dwt))\n\n#--- take a look ---#\nhead(NE_gwl_march, 10)\n\n# A tibble: 10 × 3\n# Groups:   site_no [2]\n   site_no          year   dwt\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 400032101022901  2008 118. \n 2 400032101022901  2009 117. \n 3 400032101022901  2010 118. \n 4 400032101022901  2011 118. \n 5 400032101022901  2012 118. \n 6 400032101022901  2013 118. \n 7 400032101022901  2014 116. \n 8 400032101022901  2015 117. \n 9 400038099244601  2007  24.3\n10 400038099244601  2008  21.7\n\n\nSince NE_gwl is missing geographic coordinates for the monitoring wells, we will download them using the readNWISsite() function and select only the monitoring wells that are inside the three counties.\n\n#--- get the list of site ids ---#\nNE_site_ls &lt;- NE_gwl$site_no %&gt;% unique()\n\n#--- get the locations of the site ids ---#\nsites_info &lt;-\n  readNWISsite(siteNumbers = NE_site_ls) %&gt;%\n  dplyr::select(site_no, dec_lat_va, dec_long_va) %&gt;%\n  #--- turn the data into an sf object ---#\n  sf::st_as_sf(coords = c(\"dec_long_va\", \"dec_lat_va\")) %&gt;%\n  #--- NAD 83 ---#\n  sf::st_set_crs(4269) %&gt;%\n  #--- project to WGS UTM 14 ---#\n  sf::st_transform(32614) %&gt;%\n  #--- keep only those located inside the three counties ---#\n  .[three_counties, ]\n\n\nWe now identify irrigation wells that are located within the 2-mile radius of the monitoring wells4. We first create polygons of 2-mile radius circles around the monitoring wells (see Figure 1.3).\n4 This can alternatively be done using the sf::st_is_within_distance() function.\nbuffers &lt;- sf::st_buffer(sites_info, dist = 2 * 1609.34) # in meter\n\n\nCodeggplot() +\n  geom_sf(data = three_counties) +\n  geom_sf(data = sites_info, size = 0.5) +\n  geom_sf(data = buffers, fill = NA, col = \"red\") +\n  theme_void()\n\n\n\n\n\n\nFigure 1.3: 2-mile buffers around USGS monitoring wells\n\n\n\n\nWe now identify which irrigation wells are inside each of the buffers and get the associated groundwater pumping values. The sf::st_join() function from the sf package will do the trick.\n\n#--- find irrigation wells inside the buffer and calculate total pumping  ---#\npumping_nearby &lt;- sf::st_join(buffers, urnrd_gw_sf)\n\nLet’s take a look at a USGS monitoring well (site_no = \\(400012101323401\\)).\n\ndplyr::filter(pumping_nearby, site_no == 400012101323401, year == 2010)\n\nSimple feature collection with 7 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444\nProjected CRS: WGS 84 / UTM zone 14N\n             site_no well_id year  vol_af                       geometry\n9.3  400012101323401    6331 2010      NA POLYGON ((286128 4431225, 2...\n9.24 400012101323401    1883 2010 180.189 POLYGON ((286128 4431225, 2...\n9.25 400012101323401    2006 2010  79.201 POLYGON ((286128 4431225, 2...\n9.26 400012101323401    2538 2010  68.205 POLYGON ((286128 4431225, 2...\n9.27 400012101323401    2834 2010      NA POLYGON ((286128 4431225, 2...\n9.28 400012101323401    2834 2010 122.981 POLYGON ((286128 4431225, 2...\n9.29 400012101323401    4983 2010      NA POLYGON ((286128 4431225, 2...\n\n\nAs you can see, this well has seven irrigation wells within its 2-mile radius in 2010.\nNow, we will get total nearby pumping by monitoring well and year.\n\n(\n  total_pumping_nearby &lt;-\n    pumping_nearby %&gt;%\n    sf::st_drop_geometry() %&gt;%\n    #--- calculate total pumping by monitoring well ---#\n    dplyr::group_by(site_no, year) %&gt;%\n    dplyr::summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %&gt;%\n    #--- NA means 0 pumping ---#\n    dplyr::mutate(\n      nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping)\n    )\n)\n\n# A tibble: 2,396 × 3\n# Groups:   site_no [401]\n   site_no          year nearby_pumping\n   &lt;chr&gt;           &lt;int&gt;          &lt;dbl&gt;\n 1 400012101323401  2007           571.\n 2 400012101323401  2008           772.\n 3 400012101323401  2009           500.\n 4 400012101323401  2010           451.\n 5 400012101323401  2011           545.\n 6 400012101323401  2012          1028.\n 7 400130101374401  2007           485.\n 8 400130101374401  2008           515.\n 9 400130101374401  2009           351.\n10 400130101374401  2010           374.\n# ℹ 2,386 more rows\n\n\n\nWe now merge nearby pumping data to the groundwater level data, and transform the data to obtain the dataset ready for regression analysis.\n\n#--- regression-ready data ---#\nreg_data &lt;-\n  NE_gwl_march %&gt;%\n  #--- pick monitoring wells that are inside the three counties ---#\n  dplyr::filter(site_no %in% unique(sites_info$site_no)) %&gt;%\n  #--- merge with the nearby pumping data ---#\n  dplyr::left_join(., total_pumping_nearby, by = c(\"site_no\", \"year\")) %&gt;%\n  #--- lead depth to water table ---#\n  dplyr::arrange(site_no, year) %&gt;%\n  dplyr::group_by(site_no) %&gt;%\n  dplyr::mutate(\n    #--- lead depth ---#\n    dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year),\n    #--- first order difference in dwt  ---#\n    dwt_dif = dwt_lead1 - dwt\n  )\n\n#--- take a look ---#\ndplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping)\n\n# A tibble: 2,022 × 4\n# Groups:   site_no [230]\n   site_no          year dwt_dif nearby_pumping\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 400130101374401  2011  NA               358.\n 2 400134101483501  2007   2.87           2038.\n 3 400134101483501  2008   0.780          2320.\n 4 400134101483501  2009  -2.45           2096.\n 5 400134101483501  2010   3.97           2432.\n 6 400134101483501  2011   1.84           2634.\n 7 400134101483501  2012  -1.35            985.\n 8 400134101483501  2013  44.8              NA \n 9 400134101483501  2014 -26.7              NA \n10 400134101483501  2015  NA                NA \n# ℹ 2,012 more rows\n\n\n\nFinally, we estimate the model using fixest::feols() from the fixest package (see here for an introduction).\n\n#--- OLS with site_no and year FEs (error clustered by site_no) ---#\n(\nreg_dwt &lt;-\n  fixest::feols(\n    dwt_dif ~ nearby_pumping | site_no + year,\n    cluster = \"site_no\",\n    data = reg_data\n  )\n)\n\nOLS estimation, Dep. Var.: dwt_dif\nObservations: 1,342\nFixed-effects: site_no: 225,  year: 6\nStandard-errors: Clustered (site_no) \n               Estimate Std. Error t value  Pr(&gt;|t|)    \nnearby_pumping 0.000716    7.6e-05 9.38543 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.35804     Adj. R2: 0.286321\n                Within R2: 0.07157",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-PA",
    "href": "chapters/01-Demonstration.html#sec-demo-PA",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.2 Precision Agriculture",
    "text": "1.2 Precision Agriculture\n\n1.2.1 Project Overview\n\nObjectives:\n\nUnderstand the impact of nitrogen on corn yield\nUnderstand how electric conductivity (EC) affects the marginal impact of nitrogen on corn\n\n\nDatasets:\n\nThe experimental design of an on-farm randomized nitrogen trail on an 80-acre field\nData generated by the experiment\n\nAs-applied nitrogen rate\nYield measures\n\n\nElectric conductivity\n\n\nEconometric Model:\nHere is the econometric model, we would like to estimate:\n\\[\nyield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i\n\\]\nwhere \\(yield_i\\), \\(N_i\\), \\(EC_i\\), and \\(v_i\\) are corn yield, nitrogen rate, EC, and error term at subplot \\(i\\). Subplots which are obtained by dividing experimental plots into six of equal-area compartments.\n\nGIS tasks\n\nread spatial data in various formats: R data set (rds), shape file, and GeoPackage file\n\nuse sf::st_read()\n\n\n\ncreate maps using the ggplot2 package\n\nuse ggplot2::geom_sf()\n\n\n\ncreate subplots within experimental plots\n\nuser-defined function that makes use of st_geometry()\n\n\n\nidentify corn yield, as-applied nitrogen, and electric conductivity (EC) data points within each of the experimental plots and find their averages\n\nuse sf::st_join() and sf::aggregate()\n\n\n\n\n\nPreparation for replication\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  ggplot2, # for map creation\n  fixest, # OLS regression\n  patchwork # arrange multiple plots\n)\n\n\nRun the following code to define the theme for map:\n\n\ntheme_for_map &lt;-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )\n\n\n1.2.2 Project Demonstration\nWe have already run a whole-field randomized nitrogen experiment on a 80-acre field. Let’s import the trial design data\n\n#--- read the trial design data ---#\ntrial_design_16 &lt;- readRDS(\"Data/trial_design.rds\")\n\nFigure 1.4 is the map of the trial design generated using ggplot2 package.\n\n#--- map of trial design ---#\nggplot(data = trial_design_16) +\n  geom_sf(aes(fill = factor(NRATE))) +\n  scale_fill_brewer(name = \"N\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n\n\n\n\n\nFigure 1.4: The Experimental Design of the Randomize Nitrogen Trial\n\n\n\n\n\nWe have collected yield, as-applied NH3, and EC data. Let’s read in these datasets:5\n5 Here we are demonstrating that R can read spatial data in different formats. R can read spatial data of many other formats. Here, we are reading a shapefile (.shp) and GeoPackage file (.gpkg).\n#--- read yield data (sf data saved as rds) ---#\nyield &lt;- readRDS(\"Data/yield.rds\")\n\n#--- read NH3 data (GeoPackage data) ---#\nNH3_data &lt;- sf::st_read(\"Data/NH3.gpkg\")\n\n#--- read ec data (shape file) ---#\nec &lt;- sf::st_read(dsn = \"Data\", \"ec\")\n\nFigure 1.5 shows the spatial distribution of the three variables. A map of each variable was made first, and then they are combined into one figure using the patchwork package6.\n6 Here is its Github page. See the bottom of the page to find vignettes.\n#--- yield map ---#\ng_yield &lt;-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = yield, aes(color = yield), size = 0.5) +\n  scale_color_distiller(name = \"Yield\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- NH3 map ---#\ng_NH3 &lt;-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) +\n  scale_color_distiller(name = \"NH3\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- NH3 map ---#\ng_ec &lt;-\n  ggplot() +\n  geom_sf(data = trial_design_16) +\n  geom_sf(data = ec, aes(color = ec), size = 0.5) +\n  scale_color_distiller(name = \"EC\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\n#--- stack the figures vertically and display (enabled by the patchwork package) ---#\ng_yield / g_NH3 / g_ec\n\n\n\n\n\n\nFigure 1.5: Spatial distribution of yield, NH3, and EC\n\n\n\n\n\nInstead of using plot as the observation unit, we would like to create subplots inside each of the plots and make them the unit of analysis because it would avoid masking the within-plot spatial heterogeneity of EC. Here, we divide each plot into six subplots.\nThe following function generate subplots by supplying a trial design and the number of subplots you would like to create within each plot:\n\ngen_subplots &lt;- function(plot, num_sub) {\n\n  #--- extract geometry information ---#\n  geom_mat &lt;- sf::st_geometry(plot)[[1]][[1]]\n\n  #--- upper left ---#\n  top_start &lt;- (geom_mat[2, ])\n\n  #--- upper right ---#\n  top_end &lt;- (geom_mat[3, ])\n\n  #--- lower right ---#\n  bot_start &lt;- (geom_mat[1, ])\n\n  #--- lower left ---#\n  bot_end &lt;- (geom_mat[4, ])\n\n  top_step_vec &lt;- (top_end - top_start) / num_sub\n  bot_step_vec &lt;- (bot_end - bot_start) / num_sub\n\n  # create a list for the sub-grid\n\n  subplots_ls &lt;- list()\n\n  for (j in 1:num_sub) {\n    rec_pt1 &lt;- top_start + (j - 1) * top_step_vec\n    rec_pt2 &lt;- top_start + j * top_step_vec\n    rec_pt3 &lt;- bot_start + j * bot_step_vec\n    rec_pt4 &lt;- bot_start + (j - 1) * bot_step_vec\n\n    rec_j &lt;- rbind(rec_pt1, rec_pt2, rec_pt3, rec_pt4, rec_pt1)\n\n    temp_quater_sf &lt;- list(st_polygon(list(rec_j))) %&gt;%\n      sf::st_sfc(.) %&gt;%\n      sf::st_sf(., crs = 26914)\n\n    subplots_ls[[j]] &lt;- temp_quater_sf\n  }\n\n  return(do.call(\"rbind\", subplots_ls))\n}\n\nLet’s run the function to create six subplots within each of the experimental plots.\n\n#--- generate subplots ---#\nsubplots &lt;-\n  lapply(\n    1:nrow(trial_design_16),\n    function(x) gen_subplots(trial_design_16[x, ], 6)\n  ) %&gt;%\n  do.call(\"rbind\", .)\n\nFigure 1.6 is a map of the subplots generated.\n\n#--- here is what subplots look like ---#\nggplot(subplots) +\n  geom_sf() +\n  theme_for_map\n\n\n\n\n\n\nFigure 1.6: Map of the subplots\n\n\n\n\n\nWe now identify the mean value of corn yield, nitrogen rate, and EC for each of the subplots using sf::aggregate() and sf::st_join().\n\n(\n  reg_data &lt;- subplots %&gt;%\n    #--- yield ---#\n    sf::st_join(., aggregate(yield, ., mean), join = sf::st_equals) %&gt;%\n    #--- nitrogen ---#\n    sf::st_join(., aggregate(NH3_data, ., mean), join = sf::st_equals) %&gt;%\n    #--- EC ---#\n    sf::st_join(., aggregate(ec, ., mean), join = sf::st_equals)\n)\n\nSimple feature collection with 816 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734\nProjected CRS: NAD83 / UTM zone 14N\nFirst 10 features:\n      yield   aa_NH3       ec                       geometry\n1  220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,...\n2  218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,...\n3  220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,...\n4  215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,...\n5  216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,...\n6  227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,...\n7  226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,...\n8  225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,...\n9  221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5...\n10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,...\n\n\nHere are the visualization of the subplot-level data (Figure 1.7):\n\ng_sub_yield &lt;-\n  ggplot() +\n  geom_sf(data = reg_data, aes(fill = yield), color = NA) +\n  scale_fill_distiller(name = \"Yield\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\ng_sub_NH3 &lt;-\n  ggplot() +\n  geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) +\n  scale_fill_distiller(name = \"NH3\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\ng_sub_ec &lt;-\n  ggplot() +\n  geom_sf(data = reg_data, aes(fill = ec), color = NA) +\n  scale_fill_distiller(name = \"EC\", palette = \"OrRd\", direction = 1) +\n  theme_for_map\n\ng_sub_yield / g_sub_NH3 / g_sub_ec\n\n\n\n\n\n\nFigure 1.7: Spatial distribution of subplot-level yield, NH3, and EC\n\n\n\n\n\nLet’s estimate the model and see the results:\n\n(\nfixest::feols(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3 * ec) + I(aa_NH3^2 * ec), data = reg_data)\n)\n\nOLS estimation, Dep. Var.: yield\nObservations: 784\nStandard-errors: IID \n                   Estimate Std. Error   t value  Pr(&gt;|t|)    \n(Intercept)      327.993335 125.637771  2.610627 0.0092114 ** \naa_NH3            -1.222961   1.307782 -0.935142 0.3500049    \nI(aa_NH3^2)        0.003580   0.003431  1.043559 0.2970131    \nI(aa_NH3 * ec)     0.002150   0.002872  0.748579 0.4543370    \nI(aa_NH3^2 * ec)  -0.000011   0.000015 -0.712709 0.4762394    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 5.69371   Adj. R2: 0.005198",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo3",
    "href": "chapters/01-Demonstration.html#sec-demo3",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.3 Land Use and Weather",
    "text": "1.3 Land Use and Weather\n\n1.3.1 Project Overview\n\nObjective\n\nUnderstand the impact of past precipitation on crop choice in Iowa (IA).\n\n\nDatasets\n\nIA county boundary\nRegular grids over IA, created using sf::st_make_grid()\n\nPRISM daily precipitation data downloaded using prism package\nLand use data from the Cropland Data Layer (CDL) for IA in 2015, downloaded using CropScapeR package\n\n\nEconometric Model\nThe econometric model we would like to estimate is:\n\\[\nCS_i = \\alpha + \\beta_1 PrN_{i} + \\beta_2 PrC_{i} + v_i\n\\]\nwhere \\(CS_i\\) is the area share of corn divided by that of soy in 2015 for grid \\(i\\) (we will generate regularly-sized grids in the Demo section), \\(PrN_i\\) is the total precipitation observed in April through May and September in 2014, \\(PrC_i\\) is the total precipitation observed in June through August in 2014, and \\(v_i\\) is the error term. To run the econometric model, we need to find crop share and weather variables observed at the grids. We first tackle the crop share variable, and then the precipitation variable.\n\nGIS tasks\n\ndownload Cropland Data Layer (CDL) data by USDA NASS\n\nuse CropScapeR::GetCDLData()\n\n\n\ndownload PRISM weather data\n\nuse prism::get_prism_dailys()\n\n\n\ncrop PRISM data to the geographic extent of IA\n\nuse terra::crop()\n\n\n\nread PRISM data\n\nuse terra::rast()\n\n\n\nextract the CRS of PRISM data\n\nuse terra::crs()\n\n\n\ncreate regular grids within IA, which become the observation units of the econometric analysis\n\nuse sf::st_make_grid()\n\n\n\nremove grids that share small area with IA\n\nuse sf::st_intersection() and sf::st_area\n\n\n\nassign crop share and weather data to each of the generated IA grids (parallelized)\n\nuse exactextractr::exact_extract() and future.apply::future_lapply()\n\n\n\ncreate maps\n\nuse the tmap package\n\n\n\n\nPreparation for replication\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  terra, # raster data operations\n  exactextractr, # fast raster data extraction for polygons\n  maps, # to get county boundary data\n  data.table, # data wrangling\n  dplyr, # data wrangling\n  lubridate, # Date object handling\n  tmap, # for map creation\n  future.apply, # parallel computation\n  CropScapeR, # download CDL data\n  prism, # download PRISM data\n  stringr, # string manipulation\n  fixest # OLS regression\n)\n\n\n1.3.2 Project Demonstration\nThe geographic focus of this project is Iowas. Let’s get Iowa state border (see Figure 1.8 for its map).\n\n#--- IA state boundary ---#\nIA_boundary &lt;- sf::st_as_sf(tigris::counties(state = \"Iowa\", cb = TRUE, progress_bar = FALSE))\n\n\n\n\n\nCode#--- map IA state border ---#\ntm_shape(IA_boundary) +\n  tm_polygons() +\n  tm_layout(frame = FALSE)\n\n\n\n\n\n\nFigure 1.8: Iowa state boundary\n\n\n\n\n7 We by no means are saying that this is the right geographical unit of analysis. This is just about demonstrating how R can be used for analysis done at the higher spatial resolution than county.The unit of analysis is artificial grids that we create over Iowa. The grids are regularly-sized rectangles except around the edge of the Iowa state border7. So, let’s create grids and remove those that do not overlap much with Iowa (Figure 1.9 shows what the generated grids look like).\n\n#--- create regular grids (40 cells by 40 columns) over IA ---#\nIA_grids &lt;-\n  IA_boundary %&gt;%\n  #--- create grids ---#\n  sf::st_make_grid(, n = c(40, 40)) %&gt;%\n  #--- convert to sf ---#\n  sf::st_as_sf() %&gt;%\n  #--- assign grid id for future merge ---#\n  dplyr::mutate(grid_id = 1:nrow(.)) %&gt;%\n  #--- make some of the invalid polygons valid ---#\n  sf::st_make_valid() %&gt;%\n  #--- drop grids that do not overlap with Iowa ---#\n  .[IA_boundary, ]\n\n\n\n\n\nCode#--- plot the grids over the IA state border ---#\ntm_shape(IA_boundary) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_shape(IA_grids) +\n  tm_borders(col = \"blue\") +\n  tm_layout(frame = FALSE)\n\n\n\n\n\n\nFigure 1.9: Map of regular grids generated over IA\n\n\n\n\n\nLet’s work on crop share data. You can download CDL data using the CropScapeR::GetCDLData() function.\n\n#--- download the CDL data for IA in 2015 ---#\n(\n  cdl_IA_2015 &lt;- CropScapeR::GetCDLData(aoi = 19, year = 2015, type = \"f\")\n)\n\nThe cells (30 meter by 30 meter) of the imported raster layer take a value ranging from 0 to 255. Corn and soybean are represented by 1 and 5, respectively.\nFigure 1.10 shows the map of one of the IA grids and the CDL cells it overlaps with.\n\nCodetemp_grid &lt;- IA_grids[100, ]\n\nextent_grid &lt;-\n  temp_grid %&gt;%\n  sf::st_transform(., terra::crs(IA_cdl_2015)) %&gt;%\n  sf::st_bbox()\n\nraster_ovelap &lt;- terra::crop(IA_cdl_2015, extent_grid)\n\nggplot() +\n  tidyterra::geom_spatraster(data = raster_ovelap, aes(fill = Layer_1)) +\n  geom_sf(data = temp_grid, fill = NA, color = \"red\", linewidth = 1) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\nFigure 1.10: Spatial overlap of an IA grid and CDL layer\n\n\n\n\nWe would like to extract all the cell values within the red border.\nWe use exactextractr::exact_extract() to identify which cells of the CDL raster layer fall within each of the IA grids and extract land use type values. We then find the share of corn and soybean for each of the grids.\n\n#--- reproject grids to the CRS of the CDL data ---#\nIA_grids_rp_cdl &lt;- sf::st_transform(IA_grids, terra::crs(IA_cdl_2015))\n\n#--- extract crop type values and find frequencies ---#\ncdl_extracted &lt;-\n  exactextractr::exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %&gt;%\n  lapply(., function(x) data.table(x)[, .N, by = value]) %&gt;%\n  #--- combine the list of data.tables into one data.table ---#\n  data.table::rbindlist(idcol = TRUE) %&gt;%\n  #--- find the share of each land use type ---#\n  .[, share := N / sum(N), by = .id] %&gt;%\n  .[, N := NULL] %&gt;%\n  #--- keep only the share of corn and soy ---#\n  .[value %in% c(1, 5), ]\n\nWe then find the corn to soy ratio for each of the IA grids.\n\n#--- find corn/soy ratio ---#\ncorn_soy &lt;-\n  cdl_extracted %&gt;%\n  #--- long to wide ---#\n  data.table::dcast(.id ~ value, value.var = \"share\") %&gt;%\n  #--- change variable names ---#\n  data.table::setnames(c(\".id\", \"1\", \"5\"), c(\"grid_id\", \"corn_share\", \"soy_share\")) %&gt;%\n  #--- corn share divided by soy share ---#\n  .[, c_s_ratio := corn_share / soy_share]\n\n\nWe are still missing daily precipitation data at the moment. We have decided to use daily weather data from PRISM. Daily PRISM data is a raster data with the cell size of 4 km by 4 km. Figure 1.11 presents precipitation data downloaded for April 1, 2010. It covers the entire contiguous U.S.\n\nCodeprism_ex &lt;- readRDS(\"Data/prism_ex.rds\")\n\nplot(prism_ex)\n\n\n\n\n\n\nFigure 1.11: Map of PRISM raster data layer\n\n\n\n\nLet’s now download PRISM data (You do not have to run this code to get the data. It is included in the data folder for replication here). This can be done using the get_prism_dailys() function from the prism package.8\n\n8 prism Github page\noptions(prism.path = \"Data/PRISM\")\n\nprism::get_prism_dailys(\n  type = \"ppt\",\n  minDate = \"2014-04-01\",\n  maxDate = \"2014-09-30\",\n  keepZip = FALSE\n)\n\nWhen we use get_prism_dailys() to download data9, it creates one folder for each day. So, I have about 180 folders inside the folder I designated as the download destination above with the options() function.\n\n9 For this project, monthly PRISM data could have been used, which can be downloaded using the prism::get_prism_monthlys() function. But, in many applications, daily data is necessary, so how to download and process them is illustrated here.\nWe now try to extract precipitation value by day for each of the IA grids by geographically overlaying IA grids onto the PRISM data layer and identify which PRISM cells each of the IA grid encompass. 10.\n10 Be cautious about using sf::st_buffer() for spatial objects in geographic coordinates (latitude, longitude) in practice. Significant distortion will be introduced to the buffer due to the fact that one degree in latitude and longitude means different distances at the latitude of IA. Here, I am just creating a buffer to extract PRISM cells to display on the map.\n#--- read a PRISM dataset ---#\nprism_whole &lt;- terra::rast(\"Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil\")\n\n#--- align the CRS ---#\nIA_grids_rp_prism &lt;- sf::st_transform(IA_grids, terra::crs(prism_whole))\n\n#--- crop the PRISM data for the 1st IA grid ---#\nsf_use_s2(FALSE)\nPRISM_1 &lt;- terra::crop(prism_whole, sf::st_buffer(IA_grids_rp_prism[1, ], dist = 0.05))\n\nFigure 1.12 shows how the first IA grid (in red) overlaps with the PRISM cells. As you can see, some PRISM grids are fully inside the analysis grid, while others are partially inside it. So, when assigning precipitation values to grids, we will use the coverage-weighted mean of precipitations.\n\n#--- map them ---#\nggplot() +\n  tidyterra::geom_spatraster(data = PRISM_1) +\n  scale_fill_viridis_c(name = \"Precipitation\") +\n  geom_sf(data = IA_grids_rp_prism[1, ], fill = NA, color = \"red\") +\n  theme_void()\n\n\n\n\n\n\nFigure 1.12: Spatial overlap of an IA grid over PRISM cells\n\n\n\n\nUnlike the CDL layer, we have 183 raster layers to process. Fortunately, we can process many raster files at the same time very quickly by first “stacking” many raster files first and then applying the exactextractr::exact_extract() function. Using future.apply::future_lapply(), we let \\(6\\) cores take care of this task with each processing 31 files, except one of them handling only 28 files.11\n11 Parallelization of extracting values from many raster layers for polygons are discussed in much more detail in Chapter 9.We first get all the paths to the PRISM files.\n\n#--- get all the dates ---#\ndates_ls &lt;- seq(as.Date(\"2014-04-01\"), as.Date(\"2014-09-30\"), \"days\")\n\n#--- remove hyphen ---#\ndates_ls_no_hyphen &lt;- stringr::str_remove_all(dates_ls, \"-\")\n\n#--- get all the prism file names ---#\nfolder_name &lt;- paste0(\"PRISM_ppt_stable_4kmD2_\", dates_ls_no_hyphen, \"_bil\")\nfile_name &lt;- paste0(\"PRISM_ppt_stable_4kmD2_\", dates_ls_no_hyphen, \"_bil.bil\")\nfile_paths &lt;- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n#--- take a look ---#\nhead(file_paths)\n\n[1] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil\"\n[2] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil\"\n[3] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil\"\n[4] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil\"\n[5] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil\"\n[6] \"Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil\"\n\n\nWe now prepare for parallelized extractions and then implement them using future_apply() (you can have a look at Appendix A to familiarize yourself with parallel computation using the future.apply package).\n\n#--- define the number of cores to use ---#\nnum_core &lt;- 6\n\n#--- prepare some parameters for parallelization ---#\nfile_len &lt;- length(file_paths)\nfiles_per_core &lt;- ceiling(file_len / num_core)\n\n#--- prepare for parallel processing ---#\nfuture::plan(multicore, workers = num_core)\n\n#--- reproject IA grids to the CRS of PRISM data ---#\nIA_grids_reprojected &lt;- sf::st_transform(IA_grids, terra::crs(prism_whole))\n\nHere is the function that we run in parallel over 6 cores.\n\n#--- define the function to extract PRISM values by block of files ---#\nextract_by_block &lt;- function(i, files_per_core) {\n\n  #--- files processed by core  ---#\n  start_file_index &lt;- (i - 1) * files_per_core + 1\n\n  #--- indexes for files to process ---#\n  file_index &lt;- seq(\n    from = start_file_index,\n    to = min((start_file_index + files_per_core), file_len),\n    by = 1\n  )\n\n  #--- extract values ---#\n  data_temp &lt;- \n    file_paths[file_index] %&gt;% # get file names\n    #--- read as a multi-layer raster ---#\n    terra::rast() %&gt;%\n    #--- extract ---#\n    exactextractr::exact_extract(., IA_grids_reprojected) %&gt;%\n    #--- combine into one data set ---#\n    data.table::rbindlist(idcol = \"ID\") %&gt;%\n    #--- wide to long ---#\n    data.table::melt(id.var = c(\"ID\", \"coverage_fraction\")) %&gt;%\n    #--- calculate \"area\"-weighted mean ---#\n    .[, .(value = sum(value * coverage_fraction) / sum(coverage_fraction)), by = .(ID, variable)]\n\n  return(data_temp)\n}\n\nNow, let’s run the function in parallel and calculate precipitation by period.\n\n#--- run the function ---#\nprecip_by_period &lt;-\n  future.apply::future_lapply(\n    1:num_core,\n    function(x) extract_by_block(x, files_per_core)\n  ) %&gt;%\n  data.table::rbindlist() %&gt;%\n  #--- recover the date ---#\n  .[, variable := as.Date(str_extract(variable, \"[0-9]{8}\"), \"%Y%m%d\")] %&gt;%\n  #--- change the variable name to date ---#\n  data.table::setnames(\"variable\", \"date\") %&gt;%\n  #--- define critical period ---#\n  .[, critical := \"non_critical\"] %&gt;%\n  .[month(date) %in% 6:8, critical := \"critical\"] %&gt;%\n  #--- total precipitation by critical dummy  ---#\n  .[, .(precip = sum(value)), by = .(ID, critical)] %&gt;%\n  #--- wide to long ---#\n  data.table::dcast(ID ~ critical, value.var = \"precip\")\n\nWe now have grid-level crop share and precipitation data.\n\nLet’s merge them and run regression.12\n12 We can match on grid_id from corn_soy and ID from “precip_by_period” because grid_id is identical with the row number and ID variables were created so that the ID value of \\(i\\) corresponds to \\(i\\) th row of IA_grids.\n#--- crop share ---#\nreg_data &lt;- corn_soy[precip_by_period, on = c(grid_id = \"ID\")]\n\n#--- OLS ---#\n(\nreg_results &lt;- fixest::feols(c_s_ratio ~ critical + non_critical, data = reg_data)\n)\n\nOLS estimation, Dep. Var.: c_s_ratio\nObservations: 1,218\nStandard-errors: IID \n              Estimate Std. Error   t value  Pr(&gt;|t|)    \n(Intercept)   2.700847   0.161461 16.727519 &lt; 2.2e-16 ***\ncritical     -0.002230   0.000262 -8.501494 &lt; 2.2e-16 ***\nnon_critical -0.000265   0.000314 -0.844844   0.39836    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.741986   Adj. R2: 0.056195\n\n\nAgain, do not read into the results as the econometric model is terrible.",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-railroad",
    "href": "chapters/01-Demonstration.html#sec-demo-railroad",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.4 The Impact of Railroad Presence on Corn Planted Acreage",
    "text": "1.4 The Impact of Railroad Presence on Corn Planted Acreage\n\n1.4.1 Project Overview\n\nObjective\n\nUnderstand the impact of railroad on corn planted acreage in Illinois\n\n\nDatasets\n\nUSDA corn planted acreage for Illinois downloaded from the USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA package\nUS railroads (line data) downloaded from here\n\n\n\nEconometric Model\nWe will estimate the following model:\n\\[\n  y_i = \\beta_0 + \\beta_1 RL_i + v_i\n\\]\nwhere \\(y_i\\) is corn planted acreage in county \\(i\\) in Illinois, \\(RL_i\\) is the total length of railroad, and \\(v_i\\) is the error term.\n\nGIS tasks\n\nDownload USDA corn planted acreage by county as a spatial dataset (sf object)\n\nuse tidyUSDA::getQuickStat()\n\n\n\nImport US railroad shape file as a spatial dataset (sf object)\n\nuse sf:st_read()\n\n\n\nSpatially subset (crop) the railroad data to the geographic boundary of Illinois\n\nuse sf_1[sf_2, ]\n\n\n\nFind railroads for each county (cross-county railroad will be chopped into pieces for them to fit within a single county)\n\nuse sf::st_intersection()\n\n\n\nCalculate the travel distance of each railroad piece\n\nuse sf::st_length()\n\n\n\ncreate maps using the ggplot2 package\n\nuse ggplot2::geom_sf()\n\n\n\n\n\nPreparation for replication\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  tidyUSDA, # access USDA NASS data\n  sf, # vector data operations\n  dplyr, # data wrangling\n  ggplot2, # for map creation\n  keyring # API management\n)\n\n\nRun the following code to define the theme for map:\n\n\ntheme_for_map &lt;-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid.major = element_line(color = \"transparent\"),\n    panel.grid.minor = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )\n\n\n1.4.2 Project Demonstration\nWe first download corn planted acreage data for 2018 from USDA NASS QuickStat service using tidyUSDA package13.\n13 In order to actually download the data, you need to obtain the API key here. Once the API key was obtained, it was stored using keyring::set_key(), which was named “usda_nass_qs_api”. In the code to the left, the API key was retrieved using keyring::key_get(\"usda_nass_qs_api\") in the code. For your replication, replace key_get(\"usda_nass_qs_api\") with your own API key.\n(\n  IL_corn_planted &lt;-\n    getQuickstat(\n      #--- use your own API key here fore replication ---#\n      key = keyring::key_get(\"usda_nass_qs_api\"),\n      program = \"SURVEY\",\n      data_item = \"CORN - ACRES PLANTED\",\n      geographic_level = \"COUNTY\",\n      state = \"ILLINOIS\",\n      year = \"2018\",\n      geometry = TRUE\n    ) %&gt;%\n    #--- keep only some of the variables ---#\n    dplyr::select(year, NAME, county_code, short_desc, Value)\n)\n\n\n\nSimple feature collection with 90 features and 5 fields (with 6 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year        NAME county_code           short_desc  Value\n1  2018      Bureau         011 CORN - ACRES PLANTED 264000\n2  2018     Carroll         015 CORN - ACRES PLANTED 134000\n3  2018       Henry         073 CORN - ACRES PLANTED 226500\n4  2018  Jo Daviess         085 CORN - ACRES PLANTED  98500\n5  2018         Lee         103 CORN - ACRES PLANTED 236500\n6  2018      Mercer         131 CORN - ACRES PLANTED 141000\n7  2018        Ogle         141 CORN - ACRES PLANTED 217000\n8  2018      Putnam         155 CORN - ACRES PLANTED  32300\n9  2018 Rock Island         161 CORN - ACRES PLANTED  68400\n10 2018  Stephenson         177 CORN - ACRES PLANTED 166500\n                         geometry\n1  MULTIPOLYGON (((-89.16654 4...\n2  MULTIPOLYGON (((-89.97844 4...\n3  MULTIPOLYGON (((-89.85722 4...\n4  MULTIPOLYGON (((-90.21448 4...\n5  MULTIPOLYGON (((-89.05201 4...\n6  MULTIPOLYGON (((-91.11419 4...\n7  MULTIPOLYGON (((-88.94215 4...\n8  MULTIPOLYGON (((-89.46647 4...\n9  MULTIPOLYGON (((-90.33554 4...\n10 MULTIPOLYGON (((-89.62982 4...\n\n\nA nice thing about this function is that the data is downloaded as an sf object with county geometry with geometry = TRUE. So, you can immediately plot it (Figure 1.13) and use it for later spatial interactions without having to merge the downloaded data to an independent county boundary data.\n\nggplot(IL_corn_planted) +\n  geom_sf(aes(fill = Value / 1000)) +\n  scale_fill_distiller(name = \"Planted Acreage (1000 acres)\", palette = \"YlOrRd\", trans = \"reverse\") +\n  theme(legend.position = \"bottom\") +\n  theme_for_map\n\n\n\n\n\n\nFigure 1.13: Map of Con Planted Acreage in Illinois in 2018\n\n\n\n\n\nLet’s import the U.S. railroad data and reproject to the CRS of IL_corn_planted:\n\nrail_roads &lt;-\n  sf::st_read(\"Data/tl_2015_us_rails.shp\") %&gt;%\n  #--- reproject to the CRS of IL_corn_planted ---#\n  sf::st_transform(st_crs(IL_corn_planted))\n\nReading layer `tl_2015_us_rails' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/tl_2015_us_rails.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 180958 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006\nGeodetic CRS:  NAD83\n\n\nFigure 1.14 shows is what it looks like:\n\nggplot(rail_roads) +\n  geom_sf() +\n  theme_for_map\n\n\n\n\n\n\nFigure 1.14: Map of Railroads\n\n\n\n\nWe now crop it to the Illinois state border (Figure 1.15):\n\nrail_roads_IL &lt;- rail_roads[IL_corn_planted, ]\n\n\nggplot() +\n  geom_sf(data = rail_roads_IL) +\n  theme_for_map\n\n\n\n\n\n\nFigure 1.15: Map of railroads in Illinois\n\n\n\n\nLet’s now find railroads for each county, where cross-county railroads will be chopped into pieces so each piece fits completely within a single county, using st_intersection().\n\nrails_IL_segmented &lt;- st_intersection(rail_roads_IL, IL_corn_planted)\n\nHere are the railroads for Richland County:\n\nggplot() +\n  geom_sf(data = dplyr::filter(IL_corn_planted, NAME == \"Richland\")) +\n  geom_sf(\n    data = dplyr::filter(rails_IL_segmented, NAME == \"Richland\"),\n    aes(color = LINEARID)\n  ) +\n  theme(legend.position = \"bottom\") +\n  theme_for_map\n\n\n\n\n\n\n\nWe now calculate the travel distance (Great-circle distance) of each railroad piece using st_length() and then sum them up by county to find total railroad length by county.\n\n(\n  rail_length_county &lt;-\n    mutate(\n      rails_IL_segmented,\n      length_in_m = as.numeric(st_length(rails_IL_segmented))\n    ) %&gt;%\n    #--- geometry no longer needed ---#\n    st_drop_geometry() %&gt;%\n    #--- group by county ID ---#\n    group_by(county_code) %&gt;%\n    #--- sum rail length by county ---#\n    summarize(length_in_m = sum(length_in_m))\n)\n\n# A tibble: 82 × 2\n   county_code length_in_m\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 001              77221.\n 2 003              77293.\n 3 007              36757.\n 4 011             255425.\n 5 015             161620.\n 6 017              30585.\n 7 019             389226.\n 8 021             155749.\n 9 023              78592.\n10 025              92030.\n# ℹ 72 more rows\n\n\n\nWe merge the railroad length data to the corn planted acreage data and estimate the model.\n\nreg_data &lt;- left_join(IL_corn_planted, rail_length_county, by = \"county_code\")\n\n\n(\nfixest::feols(Value ~ length_in_m, data = reg_data)\n)\n\nOLS estimation, Dep. Var.: Value\nObservations: 82\nStandard-errors: IID \n                Estimate  Std. Error t value   Pr(&gt;|t|)    \n(Intercept) 1.081534e+05 11419.16623 9.47121 1.0442e-14 ***\nlength_in_m 9.245200e-02     0.04702 1.96621 5.2742e-02 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 68,193.4   Adj. R2: 0.034174",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-gw-ir",
    "href": "chapters/01-Demonstration.html#sec-demo-gw-ir",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.5 Groundwater use for agricultural irrigation",
    "text": "1.5 Groundwater use for agricultural irrigation\n\n1.5.1 Project Overview\n\nObjective\n\nUnderstand the impact of monthly precipitation on groundwater use for agricultural irrigation\n\n\nDatasets\n\nAnnual groundwater pumping by irrigation wells in Kansas for 2010 and 2011 (originally obtained from the Water Information Management & Analysis System (WIMAS) database)\nDaymet daily precipitation and maximum temperature downloaded using daymetr package\n\n\nEconometric Model\nThe econometric model we would like to estimate is:\n\\[\n   y_{i,t}  = \\alpha +  P_{i,t} \\beta + T_{i,t} \\gamma + \\phi_i + \\eta_t + v_{i,t}\n\\]\nwhere \\(y\\) is the total groundwater extracted in year \\(t\\), \\(P_{i,t}\\) and \\(T_{i,t}\\) is the collection of monthly total precipitation and mean maximum temperature April through September in year \\(t\\), respectively, \\(\\phi_i\\) is the well fixed effect, \\(\\eta_t\\) is the year fixed effect, and \\(v_{i,t}\\) is the error term.\n\nGIS tasks\n\ndownload Daymet precipitation and maximum temperature data for each well from within R in parallel\n\nuse daymetr::download_daymet() and future.apply::future_lapply()\n\n\n\n\n\nPreparation for replication\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  daymetr, # get Daymet data\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  ggplot2, # for map creation\n  RhpcBLASctl, # to get the number of available cores\n  future.apply, # parallelization\n  lfe # fast regression with many fixed effects\n)\n\n\n1.5.2 Project Demonstration\nWe have already collected annual groundwater pumping data by irrigation wells in 2010 and 2011 in Kansas from the Water Information Management & Analysis System (WIMAS) database. Let’s read in the groundwater use data.\n\n#--- read in the data ---#\n(\n  gw_KS_sf &lt;- readRDS(\"Data/gw_KS_sf.rds\")\n)\n\nSimple feature collection with 56225 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id year   af_used                   geometry\n1        1 2010  67.00000 POINT (-100.4423 37.52046)\n2        1 2011 171.00000 POINT (-100.4423 37.52046)\n3        3 2010  30.93438 POINT (-100.7118 39.91526)\n4        3 2011  12.00000 POINT (-100.7118 39.91526)\n5        7 2010   0.00000 POINT (-101.8995 38.78077)\n6        7 2011   0.00000 POINT (-101.8995 38.78077)\n7       11 2010 154.00000 POINT (-101.7114 39.55035)\n8       11 2011 160.00000 POINT (-101.7114 39.55035)\n9       12 2010  28.17239 POINT (-95.97031 39.16121)\n10      12 2011  89.53479 POINT (-95.97031 39.16121)\n\n\nWe have 28553 wells in total, and each well has records of groundwater pumping (af_used) for years 2010 and 2011.\n\nKS_counties &lt;- tigris::counties(state = \"Kansas\", cb= TRUE, progress_bar = FALSE)\n\nFigure 1.16 shows the spatial distribution of the wells.\n\n\n\n\n\nggplot() +\n  geom_sf(data = KS_counties) +\n  geom_sf(data = gw_KS_sf, size = 0.1) +\n  theme_void()\n\n\n\n\n\n\nFigure 1.16: Spatial distribution of the wells in Kansas\n\n\n\n\n\nWe now need to get monthly precipitation and maximum temperature data. We have decided that we use Daymet weather data. Here we use the download_daymet() function from the daymetr package14 that allows us to download all the weather variables for a specified geographic location and time period15. We write a wrapper function that downloads Daymet data and then processes it to find monthly total precipitation and mean maximum temperature16. We then loop over the 56225 wells, which is parallelized using the future.apply::future_apply() function17. This process takes about half an hour on my Mac with parallelization on 18 cores. The data is available in the data repository for this course (named as “all_daymet.rds”).\n14 daymetr vignette15 See Section 8.4 for a fuller explanation of how to use the daymetr package.16 This may not be ideal for a real research project because the original raw data is not kept. It is often the case that your econometric plan changes on the course of your project (e.g., using other weather variables or using different temporal aggregation of weather variables instead of monthly aggregation). When this happens, you need to download the same data all over again.17 For parallelized computation, see Appendix A\n#--- get the geographic coordinates of the wells ---#\nwell_locations &lt;-\n  gw_KS_sf %&gt;%\n  unique(by = \"well_id\") %&gt;%\n  dplyr::select(well_id) %&gt;%\n  cbind(., st_coordinates(.))\n\n#--- define a function that downloads Daymet data by well and process it ---#\nget_daymet &lt;- function(i) {\n  temp_site &lt;- well_locations[i, ]$well_id\n  temp_long &lt;- well_locations[i, ]$X\n  temp_lat &lt;- well_locations[i, ]$Y\n\n  data_temp &lt;-\n    daymetr::download_daymet(\n      site = temp_site,\n      lat = temp_lat,\n      lon = temp_long,\n      start = 2010,\n      end = 2011,\n      #--- if TRUE, tidy data is returned ---#\n      simplify = TRUE,\n      #--- if TRUE, the downloaded data can be assigned to an R object ---#\n      internal = TRUE\n    ) %&gt;%\n    data.table() %&gt;%\n    #--- keep only precip and tmax ---#\n    .[measurement %in% c(\"prcp..mm.day.\", \"tmax..deg.c.\"), ] %&gt;%\n    #--- recover calender date from Julian day ---#\n    .[, date := as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\")] %&gt;%\n    #--- get month ---#\n    .[, month := month(date)] %&gt;%\n    #--- keep only April through September ---#\n    .[month %in% 4:9, ] %&gt;%\n    .[, .(site, year, month, date, measurement, value)] %&gt;%\n    #--- long to wide ---#\n    data.table::dcast(site + year + month + date ~ measurement, value.var = \"value\") %&gt;%\n    #--- change variable names ---#\n    data.table::setnames(c(\"prcp..mm.day.\", \"tmax..deg.c.\"), c(\"prcp\", \"tmax\")) %&gt;%\n    #--- find the total precip and mean tmax by month-year ---#\n    .[, .(prcp = sum(prcp), tmax = mean(tmax)), by = .(month, year)] %&gt;%\n    .[, well_id := temp_site]\n\n  return(data_temp)\n  gc()\n}\n\nHere is what one run (for the first well) of get_daymet() returns\n\n#--- one run ---#\n(\n  returned_data &lt;- get_daymet(1)[]\n)\n\n    month  year  prcp     tmax well_id\n    &lt;num&gt; &lt;int&gt; &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n 1:     4  2010 40.72 20.71700       1\n 2:     5  2010 93.60 24.41677       1\n 3:     6  2010 70.45 32.59933       1\n 4:     7  2010 84.58 33.59903       1\n 5:     8  2010 66.41 34.17323       1\n 6:     9  2010 15.58 31.25800       1\n 7:     4  2011 24.04 21.86367       1\n 8:     5  2011 25.59 26.51097       1\n 9:     6  2011 23.15 35.37533       1\n10:     7  2011 35.10 38.60548       1\n11:     8  2011 36.66 36.94871       1\n12:     9  2011  9.59 28.31800       1\n\n\nWe get the number of cores you can use by RhpcBLASctl::get_num_procs() and parallelize the loop over wells using future.apply::future_lapply().18\n18 For Mac users, parallel::mclapply() is a good alternative.\n#--- prepare for parallelization ---#\nnum_cores &lt;- RhpcBLASctl::get_num_procs() - 2 # number of cores\nplan(multicore, workers = num_cores) # set up cores\n\n#--- run get_daymet with parallelization ---#\n(\n  all_daymet &lt;-\n    future_lapply(\n      1:nrow(well_locations),\n      get_daymet\n    ) %&gt;%\n    rbindlist()\n)\n\n\n\n        month  year  prcp     tmax well_id\n        &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n     1:     4  2010    42 20.96667       1\n     2:     5  2010    94 24.19355       1\n     3:     6  2010    70 32.51667       1\n     4:     7  2010    89 33.50000       1\n     5:     8  2010    63 34.17742       1\n    ---                                   \n336980:     5  2011    18 26.11290   78051\n336981:     6  2011    25 34.61667   78051\n336982:     7  2011     6 38.37097   78051\n336983:     8  2011    39 36.66129   78051\n336984:     9  2011    23 28.45000   78051\n\n\n\nBefore merging the Daymet data, we need to reshape the data into a wide format to get monthly precipitation and maximum temperature as columns.\n\n#--- long to wide ---#\ndaymet_to_merge &lt;-\n  all_daymet %&gt;%\n  data.table::dcast(\n    well_id + year ~ month,\n    value.var = c(\"prcp\", \"tmax\")\n  )\n\n#--- take a look ---#\ndaymet_to_merge\n\nKey: &lt;well_id, year&gt;\n       well_id  year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9   tmax_4\n         &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n    1:       1  2010     42     94     70     89     63     15 20.96667\n    2:       1  2011     25     26     23     35     37      9 21.91667\n    3:       3  2010     85     62    109    112     83     41 19.93333\n    4:       3  2011     80    104     44    124    118     14 18.40000\n    5:       7  2010     44     83     23     99    105     13 18.81667\n   ---                                                                 \n56160:   78049  2011     27      6     38     37     34     36 22.81667\n56161:   78050  2010     35     48     68    111     56      9 21.38333\n56162:   78050  2011     26      7     44     38     34     35 22.76667\n56163:   78051  2010     30     62     48     29     76      3 21.05000\n56164:   78051  2011     33     18     25      6     39     23 21.90000\n         tmax_5   tmax_6   tmax_7   tmax_8   tmax_9\n          &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n    1: 24.19355 32.51667 33.50000 34.17742 31.43333\n    2: 26.30645 35.16667 38.62903 36.90323 28.66667\n    3: 21.64516 30.73333 32.80645 33.56452 28.93333\n    4: 22.62903 30.08333 35.08065 32.90323 25.81667\n    5: 22.14516 31.30000 33.12903 32.67742 30.16667\n   ---                                             \n56160: 26.70968 35.01667 38.32258 36.54839 28.80000\n56161: 24.85484 33.16667 33.88710 34.40323 32.11667\n56162: 26.70968 34.91667 38.32258 36.54839 28.83333\n56163: 24.14516 32.90000 33.83871 34.38710 31.56667\n56164: 26.11290 34.61667 38.37097 36.66129 28.45000\n\n\nNow, let’s merge the weather data to the groundwater pumping dataset.\n\n(\n  reg_data &lt;-\n    data.table(gw_KS_sf) %&gt;%\n    #--- keep only the relevant variables ---#\n    .[, .(well_id, year, af_used)] %&gt;%\n    #--- join ---#\n    daymet_to_merge[., on = c(\"well_id\", \"year\")]\n)\n\n       well_id  year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9   tmax_4\n         &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n    1:       1  2010     42     94     70     89     63     15 20.96667\n    2:       1  2011     25     26     23     35     37      9 21.91667\n    3:       3  2010     85     62    109    112     83     41 19.93333\n    4:       3  2011     80    104     44    124    118     14 18.40000\n    5:       7  2010     44     83     23     99    105     13 18.81667\n   ---                                                                 \n56221:   79348  2011     NA     NA     NA     NA     NA     NA       NA\n56222:   79349  2011     NA     NA     NA     NA     NA     NA       NA\n56223:   79367  2011     NA     NA     NA     NA     NA     NA       NA\n56224:   79372  2011     NA     NA     NA     NA     NA     NA       NA\n56225:   80930  2011     NA     NA     NA     NA     NA     NA       NA\n         tmax_5   tmax_6   tmax_7   tmax_8   tmax_9   af_used\n          &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;\n    1: 24.19355 32.51667 33.50000 34.17742 31.43333  67.00000\n    2: 26.30645 35.16667 38.62903 36.90323 28.66667 171.00000\n    3: 21.64516 30.73333 32.80645 33.56452 28.93333  30.93438\n    4: 22.62903 30.08333 35.08065 32.90323 25.81667  12.00000\n    5: 22.14516 31.30000 33.12903 32.67742 30.16667   0.00000\n   ---                                                       \n56221:       NA       NA       NA       NA       NA  76.00000\n56222:       NA       NA       NA       NA       NA 182.00000\n56223:       NA       NA       NA       NA       NA   0.00000\n56224:       NA       NA       NA       NA       NA 134.00000\n56225:       NA       NA       NA       NA       NA  23.69150\n\n\n\nLet’s run regression and display the results.\n\n#--- run FE ---#\n(\nreg_results &lt;-\n  fixest::feols(\n    af_used ~ # dependent variable\n      prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9\n        + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |\n        well_id + year, # FEs\n    cluster = \"well_id\",\n    data = reg_data\n  )\n)\n\nOLS estimation, Dep. Var.: af_used\nObservations: 55,754\nFixed-effects: well_id: 28,082,  year: 2\nStandard-errors: Clustered (well_id) \n         Estimate Std. Error  t value   Pr(&gt;|t|)    \nprcp_4  -0.053140   0.016771 -3.16849 1.5339e-03 ** \nprcp_5   0.111666   0.010073 11.08556  &lt; 2.2e-16 ***\nprcp_6  -0.072990   0.008357 -8.73415  &lt; 2.2e-16 ***\nprcp_7   0.013705   0.010259  1.33596 1.8157e-01    \nprcp_8   0.092795   0.013574  6.83648 8.2826e-12 ***\nprcp_9  -0.176970   0.025319 -6.98969 2.8165e-12 ***\ntmax_4   9.158658   1.226777  7.46563 8.5309e-14 ***\ntmax_5  -7.504898   1.061903 -7.06741 1.6154e-12 ***\ntmax_6  15.133513   1.360248 11.12555  &lt; 2.2e-16 ***\ntmax_7   3.968575   1.617572  2.45341 1.4157e-02 *  \ntmax_8   3.419550   1.065659  3.20886 1.3341e-03 ** \ntmax_9 -11.803178   1.800990 -6.55372 5.7094e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 33.0     Adj. R2: 0.882825\n             Within R2: 0.071675\n\n\nThat’s it. Do not bother to try to read into the regression results. Again, this is just an illustration of how R can be used to prepare a regression-ready dataset with spatial variables.",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-slave",
    "href": "chapters/01-Demonstration.html#sec-demo-slave",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.6 African Economy and Slaves: Nunn 2008",
    "text": "1.6 African Economy and Slaves: Nunn 2008\n\n1.6.1 Project Overview\n\nObjective:\nCreate some of the variables used in Nunn (2008) here\n\nthe distance variable used as an instrument (distance to the nearest trade center for each country in Africa)\n\nthe number of slaves for each of the countries in Africa\n\nThe tutorial is originally from http://mkudamatsu.github.io/gis_lecture4.html.\n\nDatasets\n\nCoast lines of the world\nCountry boundary in Africa\nBoundary of ethnic regions in Africa\n\n\nGIS tasks\n\nread an ESRI shape file as an sf (spatial) object\n\nuse sf::st_read()\n\n\n\nsimply a spatial object (reduce the number of points representing it)\n\nuse rmapshaper::ms_simplify()\n\n\n\nfind the closest point on the boundary of polygons\n\nuse sf::st_nearest_points()\n\n\n\nfind the centroid of a polygon\n\nuse sf::st_centroid()\n\n\n\ncombine multiple lines into a single line\n\nuse sf::st_union()\n\n\n\nidentify the last point of a line\n\nuse lwgeom::st_endpoint()\n\n\n\ncalculate the distance between two spatial objects\n\nuse sf::st_distance()\n\n\n\nimplement area-weighted spatial interpolation\n\nuse sf::st_interpolate_aw()\n\n\n\ndrop geometry from an sf object\n\nuse sf::st_drop_geometry()\n\n\n\nconvert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects\n\nuse sf::st_as_sf() and sf::st_set_crs()\n\n\n\nreproject an sf object to another CRS\n\nuse sf::st_transform()\n\nuse sf::st_join()\n\n\n\ncreate maps\n\nuse the ggplot2 package\n\n\n\n\nPreparation for replication\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  tidyverse, # data wrangling\n  patchwork, # plot arrangement\n  units,\n  rmapshaper,\n  lwgeom,\n  tictoc\n)\n\n\n1.6.2 Project Demonstration\nWe first read all the GIS data we will be using in this demonstration and then re-project them to epsg:3857, which is Pseudo-mercator.\ncoast line\n\ncoast &lt;-\n  sf::st_read(\"Data/nunn_2008/10m-coastline/10m_coastline.shp\") %&gt;%\n  st_transform(3857)\n\nReading layer `10m_coastline' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/nunn_2008/10m-coastline/10m_coastline.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4177 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -180 ymin: -85.22198 xmax: 180 ymax: 83.6341\nGeodetic CRS:  WGS 84\n\n\n\nAfrican countries\n\ncountries &lt;-\n  sf::st_read(\"Data/nunn_2008/gadm36_africa/gadm36_africa.shp\") %&gt;%\n  st_transform(3857)\n\nReading layer `gadm36_africa' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/nunn_2008/gadm36_africa/gadm36_africa.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 54 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -25.3618 ymin: -34.83514 xmax: 63.50347 ymax: 37.55986\nGeodetic CRS:  WGS 84\n\n\n\nethnic regions\n\nethnic_regions &lt;-\n  sf::st_read(\"Data/nunn_2008/Murdock_shapefile/borders_tribes.shp\") %&gt;%\n  st_transform(3857)\n\nReading layer `borders_tribes' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/nunn_2008/Murdock_shapefile/borders_tribes.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 843 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -25.35875 ymin: -34.82223 xmax: 63.50018 ymax: 37.53944\nGeodetic CRS:  WGS 84\n\n# lat/long for slave trade centers\ntrade_centers &lt;- read_csv(\"Data/nunn_2008/nunn2008.csv\")\n\n\n1.6.2.1 Calculate the distance to the nearest trade center\nWe first simplify geometries of the African countries using rmapshaper::ms_simplify(), so the code run faster, while maintaining borders between countries19. As comparison, sf::st_simplify() does not ensure borders between countries remain (see this by running countries_simp_sf &lt;- sf::st_simplify(countries) and plot(countries_simp_sf$geometry)).\n19 The keep option allows you to determine the degree of simplification, with keep = 1 being remains the same, keep = 0.001 is quite drastic. The default value is 0.05.\ncountries_simp &lt;- rmapshaper::ms_simplify(countries)\n\n\n(\n  g_countries &lt;-\n    ggplot(data = countries_simp) +\n    geom_sf() +\n    theme_void()\n)\n\n\n\n\n\n\n\nWe now finds the centroid of each country using st_centroid().\n\ncountries_centroid &lt;- st_centroid(countries)\n\nThe red points represent the centroids.\n\n\n\n\n\n\n\n\nNow, for the centroid of each country, we find its closest point on the coast line using sf::st_nrearest_points(). sf::st_nearest_points(x, y) loops through each geometry in x and returns the closest point in each feature of y. So, we first union the coast lines so we only get the single closest point on the coast.\n\n(\n  coast_union &lt;- sf::st_union(coast)\n)\n\nGeometry set for 1 feature \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -20037510 ymin: -20261860 xmax: 20037510 ymax: 18428920\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nNotice that coast_union is now has a single feature while coast has 4177 lines. Now, we are ready to use st_nearest_points.\n\nminum_dist_to_coast &lt;- sf::st_nearest_points(countries_centroid, coast_union)\n\nAs you can see below, this returns a line between the centroid and the coast.\n\n(\n  g_min_dist_line &lt;-\n    ggplot() +\n    geom_sf(data = countries_simp) +\n    geom_sf(data = minum_dist_to_coast, color = \"red\") +\n    theme_void()\n)\n\n\n\n\n\n\n\nHowever, what we need is the end point of the lines. We can use lwgeom::st_endpoint() to extract such a point20.\n20 The lwgeom package is a companion to the sf package. Its package website is here\nclosest_pt_on_coast &lt;- lwgeom::st_endpoint(minum_dist_to_coast)\n\nThe end points are represented as blue points in the figure below.\n\ng_min_dist_line +\n  geom_sf(\n    data = closest_pt_on_coast,\n    color = \"blue\",\n    size = 2\n  ) +\n  theme_void()\n\n\n\n\n\n\n\nLet’s make closest_pt_on_coast as part of countries_simp by assigning it to a new column named nearest_pt.\n\ncountries_simp$nearest_pt &lt;- closest_pt_on_coast\n\nLet’s now calculate the distance between the closest point on the coast to the nearest slave trade center. Before doing so, we first need to convert trade_centers to an sf object. At the moment, it is merely a data.frame and cannot be used for spatial operations like calculating distance. Since the lon, lat are in epsg:4326, we first create an sf using the GRS and then reproejct it to epsg:3857, so it has the same CRS as the other sf objects.\n\n(\n  trade_centers_sf &lt;-\n    trade_centers %&gt;%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %&gt;%\n    st_transform(crs = 3857)\n)\n\nSimple feature collection with 9 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -9168273 ymin: -2617513 xmax: -4288027 ymax: 4418219\nProjected CRS: WGS 84 / Pseudo-Mercator\n# A tibble: 9 × 3\n  name           fallingrain_name             geometry\n* &lt;chr&gt;          &lt;chr&gt;                     &lt;POINT [m]&gt;\n1 Virginia       Virginia Beach     (-8458055 4418219)\n2 Havana         Habana             (-9168273 2647748)\n3 Haiti          Port au Prince     (-8051739 2100853)\n4 Kingston       Kingston           (-8549337 2037549)\n5 Dominica       Roseau             (-6835017 1723798)\n6 Martinique     Fort-de-France     (-6799394 1644295)\n7 Guyana         Georgetown        (-6473228 758755.9)\n8 Salvador       Salvador da Bahia (-4288027 -1457447)\n9 Rio de Janeiro Rio               (-4817908 -2617513)\n\n\n\nggplot() +\n  geom_sf(data = trade_centers_sf, color = \"red\") +\n  geom_sf(data = countries_simp, aes(geometry = geometry)) +\n  theme_void()\n\n\n\n\n\n\n\nIn this demonstration, we calculate distance “as the bird flies” rather than “as the boat travels” using sf::st_distance(). 21. sf::st_distance(x, y) returns the distance between each of the elements in x and y in a matrix form.\n21 This is not ideal, but calculating maritime routes has yet to be implemented in the sf framework. If you are interested in calculating maritime routes, you can follow https://www.r-bloggers.com/computing-maritime-routes-in-r/. This requires the sp package\ntrade_dist &lt;- sf::st_distance(countries_simp$nearest_pt, trade_centers_sf)\n\nhead(trade_dist)\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]    [,8]\n[1,] 11524640 11416031 10179765 10629529  8907267  8846828  8274449 5824949\n[2,] 13748687 13887733 12676043 13149129 11406957 11355782 10886895 8647735\n[3,]  9581167  9745043  8548826  9031060  7289700  7244017  6860791 5164743\n[4,]  9292324  9417143  8215166  8694951  6952594  6905440  6507429 4805015\n[5,] 12274855 11991450 10748178 11171531  9490901  9423172  8757351 6013466\n[6,] 10332371 10485152  9283995  9763730  8021308  7973939  7565252 5706017\n        [,9]\n[1,] 6484009\n[2,] 9349100\n[3,] 6192456\n[4,] 5845456\n[5,] 6435144\n[6,] 6658041\n\n\nWe can get the minimum distance as follows,\n\n(\n  min_trade_dist &lt;- apply(trade_dist, 1, min)\n)\n\n [1]  5824949  8647735  5164743  4805015  6013466  5706017  4221530  5705994\n [9]  5811638  5671835  9131093  3696804  9435913  6967219  9273570  9254110\n[17]  5063714  9432182  5602645  4726523  3760754  3930290  3833379  5631878\n[25]  8922914  3810618  8119209  7962929  6404745  9748744  4377720  8406030\n[33]  4503451 10754102  8407292  6009943  5246859  5574562  8689308  9188164\n[41]  3940639  3718730  9859817  9263852  5252289  8052799  5705994  4938291\n[49]  7715612  8640368  8835414  7876588  8170477  8168325\n\n\nLet’s assign these values to a new column named distance_to_trade_center in countries_simp while converting the unit to kilometer from meter.\n\ncountries_simp$distance_to_trade_center &lt;- min_trade_dist / 1000\n\nFigure below color-differentiate countries by their distance to the closest trade center.\n\nggplot() +\n  geom_sf(data = trade_centers_sf, color = \"red\") +\n  geom_sf(\n    data = countries_simp,\n    aes(geometry = geometry, fill = distance_to_trade_center)\n  ) +\n  scale_fill_viridis_c(name = \"Distance to trade center\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n1.6.2.2 Calculate slaves exported from each country in Africa\nNunn (2008) used data on the number slaves exported from each ethnic region in ethnic_regions. Note that many ethnic regions intersects more than one countries as can be seen below, where the red and black lines represent country and ethnic region borders, respectively.\n\nggplot() +\n  geom_sf(\n    data = countries_simp,\n    aes(geometry = geometry),\n    color = \"red\"\n  ) +\n  geom_sf(\n    data = ethnic_regions,\n    aes(geometry = geometry),\n    color = \"grey60\",\n    fill = NA\n  ) +\n  theme_void()\n\n\n\n\n\n\n\nSo, we need to assign a mapping from the amount of slaves exported from tribal regions to countries. We achieve this by means of “area weighted interpolation.” For example, if a country has 40% of a tribal region, then it will be assigned 40% of the slaves exported. The main assumption is that the distribution of slaves traded in a region is uniform.\nUnfortunately, ethnic region data is not available on Prof. Nunn’s website. So, we generate fake data in this demonstration. Mean is increasing as we move west and normalized by area of region\n\nset.seed(3834)\n#--- calculate area ---#\nethnic_regions$area &lt;- sf::st_area(ethnic_regions)\n\n#--- generate fake trade numbers ---#\nethnic_regions$slaves_traded &lt;-\n  rnorm(\n    n = nrow(ethnic_regions),\n    mean = ethnic_regions$area / mean(ethnic_regions$area) * 200 * (60 - ethnic_regions$LON),\n    sd = 100\n  )\n\n\nggplot() +\n  geom_sf(\n    data = ethnic_regions,\n    aes(fill = slaves_traded)\n  ) +\n  scale_fill_viridis_c(name = \"# of slaves traded\") +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nLet’s implement area-weighted interpolation on this fake dataset using sf::st_interpolate_aw(). Since we would like to sum the number of exported slaves from each ethnic region, we use extensive = TRUE option.22\n22 extensive= FALSE does a weighted mean. More information is available here\ncountries_simp$slaves_traded &lt;-\n  sf::st_interpolate_aw(\n    st_make_valid(ethnic_regions[, \"slaves_traded\"]),\n    st_make_valid(countries_simp),\n    extensive = TRUE\n  ) %&gt;%\n  sf::st_drop_geometry() %&gt;%\n  dplyr::pull(slaves_traded)\n\nThe left and right panel of the figure below shows the number of exported slaves by ethnic region and by country, respectively.\n\nethnic_regions_plot &lt;-\n  ggplot(ethnic_regions) +\n  geom_sf(aes(geometry = geometry, fill = slaves_traded), color = NA) +\n  scale_fill_viridis_c(name = \"# of slaves traded\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\ncountries_plot &lt;-\n  ggplot(countries_simp) +\n  geom_sf(aes(geometry = geometry, fill = slaves_traded), color = NA) +\n  scale_fill_viridis_c(name = \"# of slaves traded\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\nethnic_regions_plot | countries_plot",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-tri",
    "href": "chapters/01-Demonstration.html#sec-demo-tri",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.7 Terrain Ruggedness and Economic Development in Africa: Nunn 2012",
    "text": "1.7 Terrain Ruggedness and Economic Development in Africa: Nunn 2012\n\n1.7.1 Project Overview\n\nObjective:\nNunn and Puga (2012) showed empirically that the ruggedness of the terrain has had a positive impacts on economic developement in African countries. In this demonstration, we calculate Terrain Ruggedness Index (TRI) for African countries from the world elevation data.\n\nDatasets\n\nWorld elevation data\nWorld country borders\n\n\nGIS tasks\n\nread a raster file\n\nuse terra::rast()\n\n\n\nimport world country border data\n\nuse rnaturalearth::ne_countries()\n\n\n\ncrop a raster data to a particular region\n\nuse terra::crop()\n\n\n\nreplace cell values\n\nuse terra::subst()\n\n\n\ncalculate TRI\n\nuse terra::focal()\n\n\n\ncreate maps\n\nuse the tmap package\n\n\n\n\nPreparation for replication\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  tidyverse, # data wrangling\n  stars,\n  tmap,\n  raster,\n  rnaturalearth,\n  skimr\n)\n\n\n1.7.2 Project Demonstration\nWe first read the world elevation data using terra::rast().\n\n(\n  dem &lt;- terra::rast(\"Data/nunn_2012/GDEM-10km-BW.tif\")\n)\n\nclass       : SpatRaster \ndimensions  : 2160, 4320, 1  (nrow, ncol, nlyr)\nresolution  : 0.08333333, 0.08333333  (x, y)\nextent      : -180.0042, 179.9958, -89.99583, 90.00417  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : GDEM-10km-BW.tif \nname        : GDEM-10km-BW \n\n\n\ntm_shape(dem) +\n  tm_raster()\n\n\n\n\n\n\n\nIn this dataset, the elevation of the ocean floor is recorded as 0, so let’s replace an elevation of 0 with NAs. This avoids a problem associated with calculating TRI later.\n\ndem &lt;- terra::subst(dem, 0, NA)\n\n\ntm_shape(dem) +\n  tm_raster()\n\n\n\n\n\n\n\nNow, since our interest is in Africa, let’s just crop the raster data to its African portion. To do so, we first get the world map using rnaturalearth::ne_countries() and filter out the non-African countries.\n\nafrica_sf &lt;-\n  #--- get an sf of all the countries in the world ---#\n  rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\") %&gt;%\n  #--- filter our non-African countries ---#\n  filter(continent == \"Africa\")\n\nWe can now apply terra::crop() to dem based on the bounding box of africa_sf.\n\nafrica_dem &lt;- terra::crop(dem, africa_sf)\n\nHere is a map of the crop data.\n\ntm_shape(africa_dem) +\n  tm_raster()\n\n\n\n\n\n\n\nNow, we are ready to calculte TRI, which is defined as\n\\[\n  Ruggedness_{r,c} = \\sqrt{\\sum_{i=r-1}^{r+1} \\sum_{j= r-1}^{r+1} (e_{i,j} - e_{r,c})^2}\n\\]\nWe are going to loop through the raster cells and calculate TRI. To do so, we make use of terra::focal() It allows you to apply a function to every cell of a raster and bring a matrix of the surrounding values as an input to the function. For example terra::focal(raster, w = 3, fun = any_function) will pass to your any_function a 3 by 3 matrix of the raster values centered at the point.\nLet’s define a function that calculates TRI for a given matrix.\n\ncalc_tri &lt;- function(matr) {\n  # matr is a length 9 matrix\n  center &lt;- matr[5]\n  sum_squares &lt;- sum((matr - center)^2, na.rm = TRUE)\n  return(sqrt(sum_squares))\n}\n\nNow, let’s calculate TRI.\n\ntri_africa &lt;-\n  terra::focal(\n    africa_dem,\n    w = 3,\n    fun = calc_tri\n  )\n\nHere is the map of the calculated TRI.\n\ntm_shape(tri_africa) +\n  tm_raster()",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/01-Demonstration.html#sec-demo-tsetse",
    "href": "chapters/01-Demonstration.html#sec-demo-tsetse",
    "title": "1  R as GIS: Demonstrations",
    "section": "\n1.8 TseTse fly suitability index: Alsan 2015",
    "text": "1.8 TseTse fly suitability index: Alsan 2015\n\n1.8.1 Project Overview\n\nObjective:\nCreate TseTse fly suitability index used in Alsan (2015) (find the paper here) from temperature and humidity raster datasets.\n\nDatasets\n\ndaily temperature and humidity datasets\n\n\nGIS tasks\n\nread raster data files in the NetCDF format\n\nuse stars::read_ncdf()\n\n\n\naggregate raster data\n\nuse stars::st_apply()\n\n\n\nread vector data in the geojson format\n\nuse stars::st_read()\n\n\n\nshift (rotate) longitude of a raster dataset\n\nuse terra::rotate()\n\n\n\nconvert stars objects to SpatRaster objects, and vice versa\n\nuse as(, \"SpatRaster\")\n\nuse stars::st_as_stars()\n\n\n\ndefine variables inside a stars object\n\nuse mutate()\n\n\n\nsubset (crop) raster data to a region specified by a vector dataset\n\nuse []\n\n\n\nspatially aggregate raster data by regions specified by a vector dataset\n\nuse aggregate()\n\nuse exactextractr::exact_extract()\n\n\n\ncreate maps\n\nuse the ggplot2 package\n\n\n\n\nPreparation for replication\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  tidyverse, # data wrangling\n  stars,\n  exactextractr\n)\n\n\n1.8.2 Project Demonstration\nMarcella Alsan’s 2015 AER paper “The Effect of the TseTse Fly on African Development” tests a hypothesis about historical African economic development. The hypothesis considers the TseTse fly – a fly indigenous to Africa that is lethal to crops. The theory posits that the fly prevented agricultural surplus and hence stunted historical economic development in impacted regions of Africa. To test the hypothesis, Alsan (2015) whats to compare African tribes that were in regions highly affected by the TseTse fly to areas not highly affected. To do so, they use a “sutability index” that is based on an areas average temperature and humidity.\nFor this replication, we will recreate the suitability index and aggregate the data at the tribe level. For reference, the original figure from the article is reproduced in Figure 1.17.\n\n\n\n\n\n\n\nFigure 1.17: TseTse Suitability Index from Alsan (2015)\n\n\n\n\n\n1.8.2.1 Scientific Details\nUnderstanding the details of the suitablity index is not necessary, but below I implement the following to derive the index. Feel free to skip this if you would like.\nLet B represent the birth rate, which is temperature dependent, and M represent the mortality of adult flies from desication. The growth rate, \\(\\lambda\\) is defined as: \\[\n    \\lambda= \\max(B - M, 0)\n\\]\nThe formula for the birth rate and the mortality rate are determined by scientific experiments and have the following form \\[\n    B(t) = (-0.0058 * t^2 + .2847 t -2.467)\n\\] \\[\n    M(t, h) = -0.0003 * satdef^2 + 0.0236 * satdef + .235,\n\\] where \\(t\\) is temperature, \\(h\\) is humidity and \\(satdef\\) is defined as:\n\\[\n    satdef = \\frac{100-h}{100} \\left( 6.1078 * exp(\\frac{17.2694t}{t+237}) \\right)\n\\]\nA “second form of mortality that is not due to climate, but rather attributable to competition among flies, is introduce. This is known as density dependent mortality, \\(\\Delta\\); and can be expressed as:” \\[\n    \\Delta = \\phi (N)^\\psi\n\\]\nThis yields a steady state equilibrium population of \\[\n    N^* = (\\frac{\\lambda}{\\phi})^{1/\\psi}\n\\] which is calibrated with = 0.025 = 1.25.\nLastly, the TseTse Suitability Index is the Z-score of \\(N^*\\).\n\n1.8.2.2 Load and Prepare Data\nTo generate the data, we first download historical weather data in 1871 from NOAA-CIRES 20th Century Reanalysis. This data comes in a NetCDF file format (with extension .nc). We can use stars::read_ncdf() read a NetCDF file.\n\n# NOAA-CIRES 20th Century Reanalysis version 2\n\n#--- temperature ---#\n(\n  temp_raw &lt;- stars::read_ncdf(\"data/alsan_2015/air.sig995.1871.nc\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n          Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nair [K] 235.13  260.45 278.93 277.0255  294.93 308.33\ndimension(s):\n     from  to         offset  delta  refsys x/y\nlon     1 180             -1      2  WGS 84 [x]\nlat     1  91             91     -2  WGS 84 [y]\ntime    1 365 1871-01-01 UTC 1 days POSIXct    \n\n#--- humidity ---#\n(\n  humidity_raw &lt;- stars::read_ncdf(\"data/alsan_2015/rhum.sig995.1871.nc\")\n  \n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median     Mean 3rd Qu. Max.\nrhum [%]   11      74     78 77.37066      84  100\ndimension(s):\n     from   to         offset   delta  refsys x/y\nlon     1  180             -1       2  WGS 84 [x]\nlat     1   91             91      -2  WGS 84 [y]\ntime    1 1460 1871-01-01 UTC 6 hours POSIXct    \n\n\nSince these raster files contain daily observations (see the time dimension above), Alsan aggregates them to the annual level. To average across a dimension (e.g. time), we will use the function stars::st_apply.\n\n# Aggregate to annual average\ntemp &lt;-\n  stars::st_apply(\n    X = temp_raw, MARGIN = c(\"lon\", \"lat\"), FUN = mean\n  ) %&gt;%\n  # Rename \"mean\" attribute which was created from st_apply\n  # Convert Kelvin to Celsius\n  mutate(temp = mean - 273.15) %&gt;%\n  dplyr::select(temp)\n\nhumidity &lt;-\n  stars::st_apply(\n    X = humidity_raw, MARGIN = c(\"lon\", \"lat\"), FUN = mean\n  ) %&gt;%\n  # Rename \"mean\" attribute which was created from st_apply\n  mutate(hum = mean) %&gt;%\n  dplyr::select(hum)\n\nWe then combine the two to a single weather dataset.\n\n(\n  weather &lt;- c(temp, humidity)\n)\n\nstars object with 2 dimensions and 2 attributes\nattribute(s):\n           Min.   1st Qu.    Median      Mean  3rd Qu.     Max.\ntemp  -47.37911 -4.803978  8.705419  5.683404 22.29556 30.72751\nhum    18.93425 73.694521 75.642466 74.541109 80.79110 96.53836\ndimension(s):\n    from  to offset delta refsys x/y\nlon    1 180     -1     2 WGS 84 [x]\nlat    1  91     91    -2 WGS 84 [y]\n\n\nThe second piece of data needed is a shape file containing the Tribal boundaries. The original drawings come from Murdock (1959), but were digitized by Nathan Nunn and coauthors and is available here.\n\n# African Tribes, originally from Murdock (1959)\ntribes &lt;-\n  sf::st_read(\"Data/alsan_2015/Murdock_shapefile/borders_tribes.geojson\") %&gt;%\n  #--- reproject to the CRS of temp_raw ---#\n  sf::st_transform(st_crs(temp_raw))\n\nReading layer `borders_tribes' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/alsan_2015/Murdock_shapefile/borders_tribes.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 812 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -17.22583 ymin: -34.76862 xmax: 51.27722 ymax: 37.33028\nGeodetic CRS:  WGS 84\n\n# Africa\nafrica &lt;-\n  sf::st_read(\"Data/alsan_2015/africa.geojson\") %&gt;%\n  #--- reproject to the CRS of temp_raw ---#\n  st_transform(st_crs(temp_raw))\n\nReading layer `africa' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/alsan_2015/africa.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -17.12917 ymin: -34.77313 xmax: 51.27278 ymax: 37.33521\nGeodetic CRS:  WGS 84\n\n\nHere is the map of triabl boundaries superimposed on top of Africa.\n\nggplot() +\n  geom_sf(data = tribes) +\n  geom_sf(data = africa, color = \"#F26D21\", fill = NA) +\n  coord_sf() +\n  theme_map()\n\n\n\n\n\n\n\nThere is a common problem in working with raster data for countries that cross the Prime Meridian. To see the problem, let’s plot our weather data:\n\nggplot() +\n  geom_stars(data = weather, mapping = aes(x = lon, y = lat, fill = temp)) +\n  geom_sf(data = africa, color = \"#F26D21\", fill = NA) +\n  coord_sf() +\n  scale_fill_distiller(type = \"seq\", palette = \"Greys\", direction = 1, na.value = NA) +\n  theme_map()\n\n\n\n\n\n\n\nAs you can see, the portion of Africa east of the Prime Meridian is wrapped to the other side of the map. That is because there are two ways to handle longitude: either from [0,360] or [-180,180]. Since our data is in [0,360] form, Africa will be cut in half. To convert, we can use the terra::rotate() function in the terra package. This means that you first need to convert weather, which is a stars, to a SpatRaster object, and then apply terra::rotate() to it.\n\n(\n  weather_raster &lt;-\n    #--- convert to SpatRaster ---#\n    as(weather, \"SpatRaster\") %&gt;%\n    terra::rotate()\n)\n\nclass       : SpatRaster \ndimensions  : 91, 180, 2  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : -181, 179, -91, 91  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       :      temp,      hum \nmin values  : -47.37911, 18.93425 \nmax values  :  30.72751, 96.53836 \n\n\nConversion back to a stars object with multiple layers needs some work. Specfically, we turn each layer into a stars object and then combine them using c().\n\nweather &lt;-\n  c(\n    st_as_stars(weather_raster$temp),\n    st_as_stars(weather_raster$hum)\n  )\n\nThings are looking good now.\n\nggplot() +\n  geom_stars(data = weather, mapping = aes(x = x, y = y, fill = temp)) +\n  geom_sf(data = africa, color = \"#F26D21\", fill = NA) +\n  coord_sf() +\n  scale_fill_distiller(\n    type = \"seq\", palette = \"Greys\", direction = 1, na.value = NA\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nCalculate TseTse Suitability Index\nFollowing the scientific formulae above, we add those variables to the stars object using mutate() function as if you are wrangling a data.frame.\n\nweather &lt;-\n  weather %&gt;%\n  mutate(\n    B = -0.0058 * temp^2 + .2847 * temp - 2.467,\n    satdef = (6.1078 * exp((17.2694 * temp) / (temp + 237))) - (hum / 100) * (6.1078 * exp((17.2694 * temp) / (temp + 237))),\n    M = -0.0003 * satdef^2 + 0.0236 * satdef + .235,\n    lambda = B - M,\n    lambda = ifelse(lambda &lt; 0, 0, lambda),\n    Nstar = (lambda / 0.025)^(1 / 1.25),\n  )\n\nLet’s subset weather to Africa and then calculate tsetse.\n\nsf_use_s2(FALSE)\nweather_africa &lt;-\n  #--- subset to Africa ---#\n  weather[tribes] %&gt;%\n  #--- calculate TseTse suitability index---#\n  mutate(\n    tsetse = (Nstar - mean(Nstar, na.rm = TRUE)) / sd(Nstar, na.rm = TRUE)\n  )\n\nHere is the map of TseTse suitability index.\n\nggplot() +\n  geom_stars(\n    data = weather_africa,\n    mapping = aes(x = x, y = y, fill = tsetse)\n  ) +\n  coord_sf() +\n  scale_fill_distiller(\n    type = \"seq\", palette = \"Greys\", direction = -1, na.value = NA\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nNow that we have our raster of the standardized TseTse suitability index, we want to aggregate this to the tribal level. This can be done using aggregate().\n\nagg_tsetse &lt;-\n  aggregate(\n    x = weather_africa[\"tsetse\"],\n    by = tribes,\n    FUN = mean,\n    na.rm = TRUE,\n    as_points = FALSE\n  )\n\nHowever, we are going to run into a problem due to the size of the tribes relative to the size of the raster cells.\n\nplot(agg_tsetse)\n\n\n\n\n\n\n\nNotice all the holes in the map that we have!23. This problem can be fixed using the exactextractr package. Since it does not work with stars object, we need to convert weather to a SpatRaster objective from the terra package (you can alternatively convert to a raster object).\n23 This problem is documented well in this thread\ntribes$tsetse &lt;-\n  exactextractr::exact_extract(\n    x = as(weather[\"Nstar\"], \"SpatRaster\"),\n    # x = as(weather[\"Nstar\"], \"Raster\"),\n    y = tribes,\n    fun = \"mean\",\n    progress = FALSE # not display progress bar\n  )\n\n\nggplot() +\n  geom_sf(data = tribes, aes(fill = tsetse)) +\n  coord_sf() +\n  scale_fill_distiller(\n    type = \"seq\",\n    palette = \"Greys\",\n    direction = 1,\n    na.value = \"red\"\n  )\n\n\n\n\n\n\n\nThere we have it! We successfully dealt with a slew of issues but now have created the tsetse susceptability index at the tribal level!\n\n\n\n\n\n\nAlsan, Marcella. 2015. “The Effect of the Tsetse Fly on African Development.” American Economic Review 105 (1): 382–410.\n\n\nNunn, Nathan. 2008. “The Long-Term Effects of Africa’s Slave Trades.” The Quarterly Journal of Economics 123 (1): 139–76.\n\n\nNunn, Nathan, and Diego Puga. 2012. “Ruggedness: The Blessing of Bad Geography in Africa.” Review of Economics and Statistics 94 (1): 20–36.",
    "crumbs": [
      "Demonstrations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R as GIS: Demonstrations</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html",
    "href": "chapters/02-VectorDataBasics.html",
    "title": "2  Vector Data Handling with sf",
    "section": "",
    "text": "Before you start\nIn this chapter we learn how to use the sf package to handle and operate on spatial datasets. The sf package uses the class of simple feature (sf)1 for spatial objects in R. We first learn how sf objects store and represent spatial datasets. We then move on to the following practical topics:\nTo test your knowledge, you can work on coding exercises at Section 2.10.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#before-you-start",
    "href": "chapters/02-VectorDataBasics.html#before-you-start",
    "title": "2  Vector Data Handling with sf",
    "section": "",
    "text": "1 Yes, it is the same as the package name.\nread and write a shapefile and spatial data in other formats (and why you might not want to use the shapefile system any more, but use other alternative formats)\nproject/re-project spatial objects\nconvert sf objects into sp objects, vice versa2\n\nconfirm that dplyr works well with sf objects\nimplement non-interactive (does not involve two sf objects) geometric operations on sf objects\n\ncreate buffers\nfind the area of polygons\nfind the centroid of polygons\ncalculate the length of lines\n\n\n\n2 The sf package is a successor of the sp package, which has been one of the most popular and powerful spatial packages in R for more than a decade. There are still some spatial R packages that work with sp but not with sf. For this reason, we will learn how to convert sp objects into sf objects, and vice vera.\nDirection for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder to replicate demonstrations in Chapter 1, then skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nPackages\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  tmap, # make maps\n  mapview, # create an interactive map\n  patchwork # arranging maps\n)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#spatial-data-structure",
    "href": "chapters/02-VectorDataBasics.html#spatial-data-structure",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.1 Spatial Data Structure",
    "text": "2.1 Spatial Data Structure\nHere we learn how the sf package stores spatial data along with the definition of three key sf object classes: simple feature geometry (sfg), simple feature geometry list-column (sfc), and simple feature (sf). The sf package provides a simply way of storing geographic information and the attributes of the geographic units in a single dataset. This special type of dataset is called simple feature (sf). It is best to take a look at an example to see how this is achieved. We use North Carolina county boundaries with county attributes (Figure 2.1).\n\n#--- a dataset that comes with the sf package ---#\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\n\n\n\n\nCodeggplot(nc) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 2.1: North Carolina county boundary\n\n\n\n\nAs you can see below, this dataset is of class sf (and data.frame at the same time).\n\nclass(nc)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNow, let’s take a look inside of nc.\n\n#--- take a look at the data ---#\nhead(nc)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n\n\nJust like a regular data.frame, you see a number of variables (attributes) except that you have a variable called geometry at the end. Each row represents a single geographic unit (here, county). Ashe County (1st row) has area of \\(0.114\\), FIPS code of \\(37009\\), and so on. And the entry in geometry column at the first row represents the geographic information of Ashe County. An entry in the geometry column is a simple feature geometry (sfg), which is an \\(R\\) object that represents the geographic information of a single geometric feature (county in this example). There are different types of sfgs (POINT, LINESTRING, POLYGON, MULTIPOLYGON, etc). Here, sfgs representing counties in NC are of type MULTIPOLYGON. Let’s take a look inside the sfg for Ashe County using st_geometry().\n\nsf::st_geometry(nc[1, ])[[1]][[1]]\n\n[[1]]\n           [,1]     [,2]\n [1,] -81.47276 36.23436\n [2,] -81.54084 36.27251\n [3,] -81.56198 36.27359\n [4,] -81.63306 36.34069\n [5,] -81.74107 36.39178\n [6,] -81.69828 36.47178\n [7,] -81.70280 36.51934\n [8,] -81.67000 36.58965\n [9,] -81.34530 36.57286\n[10,] -81.34754 36.53791\n[11,] -81.32478 36.51368\n[12,] -81.31332 36.48070\n[13,] -81.26624 36.43721\n[14,] -81.26284 36.40504\n[15,] -81.24069 36.37942\n[16,] -81.23989 36.36536\n[17,] -81.26424 36.35241\n[18,] -81.32899 36.36350\n[19,] -81.36137 36.35316\n[20,] -81.36569 36.33905\n[21,] -81.35413 36.29972\n[22,] -81.36745 36.27870\n[23,] -81.40639 36.28505\n[24,] -81.41233 36.26729\n[25,] -81.43104 36.26072\n[26,] -81.45289 36.23959\n[27,] -81.47276 36.23436\n\n\nAs you can see, the sfg consists of a number of points (pairs of two numbers). Connecting the points in the order they are stored delineates the Ashe County boundary (Figure 2.2).\n\n\n\n\nCodeplot(sf::st_geometry(nc[1, ]))\n\n\n\n\n\n\nFigure 2.2: Counties in North Carolina\n\n\n\n\nWe will take a closer look at different types of sfg in the next section.\nFinally, the geometry variable is a list of individual sfgs, called simple feature geometry list-column (sfc).\n\ndplyr::select(nc, geometry)\n\nSimple feature collection with 100 features and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\nFirst 10 features:\n                         geometry\n1  MULTIPOLYGON (((-81.47276 3...\n2  MULTIPOLYGON (((-81.23989 3...\n3  MULTIPOLYGON (((-80.45634 3...\n4  MULTIPOLYGON (((-76.00897 3...\n5  MULTIPOLYGON (((-77.21767 3...\n6  MULTIPOLYGON (((-76.74506 3...\n7  MULTIPOLYGON (((-76.00897 3...\n8  MULTIPOLYGON (((-76.56251 3...\n9  MULTIPOLYGON (((-78.30876 3...\n10 MULTIPOLYGON (((-80.02567 3...\n\n\nElements of a geometry list-column are allowed to be different in nature from other elements3. In the nc data, all the elements (sfgs) in geometry column are MULTIPOLYGON. However, you could also have LINESTRING or POINT objects mixed with MULTIPOLYGONS objects in a single sf object if you would like.\n3 This is just like a regular list object that can contain mixed types of elements: numeric, character, etc",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#simple-feature-geometry-simple-feature-geometry-list-column-and-simple-feature",
    "href": "chapters/02-VectorDataBasics.html#simple-feature-geometry-simple-feature-geometry-list-column-and-simple-feature",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.2 Simple feature geometry, simple feature geometry list-column, and simple feature",
    "text": "2.2 Simple feature geometry, simple feature geometry list-column, and simple feature\nHere, we learn how different types of sfg are constructed. We also learn how to create sfc and sf from sfg from scratch.4\n4 Creating spatial objects from scratch may not be a necessary skill for many of us as economists, but it is still useful to understand the underlying structure of spatial data. Occasionally, the need arises. For example, I had to construct spatial objects from scratch when designing on-farm randomized nitrogen trials. In such cases, it is important to understand how different types of sfg objects are constructed, how to create an sfc from a collection of sfgs, and finally, how to build an sf object from an sfc.\n2.2.1 Simple feature geometry (sfg)\nThe sf package uses a class of sfg (simple feature geometry) objects to represent a geometry of a single geometric feature (say, a city as a point, a river as a line, county and school district as polygons). There are different types of sfgs. Here are some example feature types that we commonly encounter as an economist5:\n5 You can see here if you are interested in other types of sfgs.\n\nPOINT: area-less feature that represents a point (e.g., well, city, farmland)\n\nLINESTRING: (e.g., a tributary of a river)\n\nMULTILINESTRING: (e.g., river with more than one tributary)\n\nPOLYGON: geometry with a positive area (e.g., county, state, country)\n\nMULTIPOLYGON: collection of polygons to represent a single object (e.g., countries with islands: U.S., Japan)\n\n\n2.2.1.1 POINT\n\nPOINT is the simplest geometry type and is represented by a vector of two numeric values. An example below shows how a POINT feature can be made from scratch:\n\n#--- create a POINT ---#\na_point &lt;- sf::st_point(c(2, 1))\n\nThe st_point() function creates a POINT object when supplied with a vector of two numeric values. If you check the class of the newly created object,\n\n#--- check the class of the object ---#\nclass(a_point)\n\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\n\nyou can see that it’s indeed a POINT object. But, it’s also an sfg object. So, a_point is an sfg object of type POINT.\n\n2.2.1.2 LINESTRING\n\nA LINESTRING objects are represented by a sequence of points:\n\n#--- collection of points in a matrix form ---#\ns1 &lt;- rbind(c(2, 3), c(3, 4), c(3, 5), c(1, 5))\n\n#--- see what s1 looks like ---#\ns1\n\n     [,1] [,2]\n[1,]    2    3\n[2,]    3    4\n[3,]    3    5\n[4,]    1    5\n\n#--- create a \"LINESTRING\" ---#\na_linestring &lt;- sf::st_linestring(s1)\n\n#--- check the class ---#\nclass(a_linestring)\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\ns1 is a matrix where each row represents a point. By applying st_linestring() function to s1, you create a LINESTRING object. Figure 2.3 shows what the line looks like.\n\nCodeplot(a_linestring)\n\n\n\n\n\n\nFigure 2.3: Lines\n\n\n\n\nAs you can see, each pair of consecutive points in the matrix are connected by a straight line to form a line.\n\n2.2.1.3 POLYGON\n\nA POLYGON is very similar to LINESTRING in the manner it is represented.\n\n#--- collection of points in a matrix form ---#\np1 &lt;- rbind(c(0, 0), c(3, 0), c(3, 2), c(2, 5), c(1, 3), c(0, 0))\n\n#--- see what s1 looks like ---#\np1\n\n     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n[3,]    3    2\n[4,]    2    5\n[5,]    1    3\n[6,]    0    0\n\n#--- create a \"POLYGON\" ---#\na_polygon &lt;- sf::st_polygon(list(p1))\n\n#--- check the class ---#\nclass(a_polygon)\n\n[1] \"XY\"      \"POLYGON\" \"sfg\"    \n\n\nFigure 2.4 shows what the polygon looks like.\n\nCodeplot(a_polygon)\n\n\n\n\n\n\nFigure 2.4: Polygon\n\n\n\n\nJust like the LINESTRING object we created earlier, a POLYGON is represented by a collection of points. The biggest difference between them is that we need to have some positive area enclosed by lines connecting the points. To do that, you have the the same point for the first and last points to close the loop: here, it’s c(0,0). A POLYGON can have holes in it. The first matrix of a list becomes the exterior ring, and all the subsequent matrices will be holes within the exterior ring.\n\n#--- a hole within p1 ---#\np2 &lt;- rbind(c(1, 1), c(1, 2), c(2, 2), c(1, 1))\n\n#--- create a polygon with hole ---#\na_plygon_with_a_hole &lt;- sf::st_polygon(list(p1, p2))\n\n\nCodeplot(a_plygon_with_a_hole)\n\n\n\n\n\n\nFigure 2.5: Polygon with a hole\n\n\n\n\n\n2.2.1.4 MULTIPOLYGON\n\nYou can create a MULTIPOLYGON object in a similar manner. The only difference is that you supply a list of lists of matrices, with each inner list representing a polygon. An example below:\n\n#--- second polygon ---#\np3 &lt;- rbind(c(4, 0), c(5, 0), c(5, 3), c(4, 2), c(4, 0))\n\n#--- create a multipolygon ---#\na_multipolygon &lt;- sf::st_multipolygon(list(list(p1, p2), list(p3)))\n\n\nCodeplot(a_multipolygon)\n\n\n\n\n\n\nFigure 2.6: Multi-polygon with a hole\n\n\n\n\nEach of list(p1,p2) and list(p3) represents a polygon. You supply a list of these lists to the st_multipolygon() function to make a MULTIPOLYGON object.\n\n2.2.2 Create simple feature geometry list-column (sfc) and simple feature (sf) from scratch\nTo make a simple feature geometry list-column (sfc), you can simply supply a list of sfg to the st_sfc() function as follows:\n\n#--- create an sfc ---#\nsfc_ex &lt;- sf::st_sfc(list(a_point, a_linestring, a_polygon, a_multipolygon))\n\nTo create an sf object, you first add an sfc as a column to a data.frame.\n\n#--- create a data.frame ---#\ndf_ex &lt;- data.frame(name = c(\"A\", \"B\", \"C\", \"D\"))\n\n#--- add the sfc as a column ---#\ndf_ex$geometry &lt;- sfc_ex\n\n#--- take a look ---#\ndf_ex\n\n  name                       geometry\n1    A                    POINT (2 1)\n2    B LINESTRING (2 3, 3 4, 3 5, ...\n3    C POLYGON ((0 0, 3 0, 3 2, 2 ...\n4    D MULTIPOLYGON (((0 0, 3 0, 3...\n\n\nAt this point, it is not yet recognized as an sf by R.\n\n#--- see what it looks like (this is not an sf object yet) ---#\nclass(df_ex)\n\n[1] \"data.frame\"\n\n\nYou can register it as an sf object using st_as_sf().\n\n#--- let R recognize the data frame as sf ---#\nsf_ex &lt;- sf::st_as_sf(df_ex)\n\n#--- see what it looks like ---#\nsf_ex\n\nSimple feature collection with 4 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 5 ymax: 5\nCRS:           NA\n  name                       geometry\n1    A                    POINT (2 1)\n2    B LINESTRING (2 3, 3 4, 3 5, ...\n3    C POLYGON ((0 0, 3 0, 3 2, 2 ...\n4    D MULTIPOLYGON (((0 0, 3 0, 3...\n\n\nAs you can see sf_ex is now recognized also as an sf object.\n\n#--- check the class ---#\nclass(sf_ex)\n\n[1] \"sf\"         \"data.frame\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#reading-and-writing-vector-data",
    "href": "chapters/02-VectorDataBasics.html#reading-and-writing-vector-data",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.3 Reading and writing vector data",
    "text": "2.3 Reading and writing vector data\nThe vast majority of people use ArcGIS to handle spatial data, which has its own system of storing spatial data called shapefile. So, chances are that your collaborators use shapefiles. Moreover, there are many GIS data online that are available only as shapefiles. So, it is important to learn how to read and write shapefiles.\n\n2.3.1 Reading a shapefile\nWe can use st_read() to read a shapefile. It reads in a shapefile and then turn the data into an sf object. Let’s take a look at an example.\n\n#--- read a NE county boundary shapefile ---#\nnc_loaded &lt;- sf::st_read(\"Data/nc.shp\")\n\nWhen storing a spatial dataset, the information in the dataset is divided into separate files. All of them have the same prefix, but have different extensions. We typically say we read a shapefile, but we really are importing all these files including the shapefile with the .shp extension.\n\n2.3.2 Writing to a shapefile\nWriting an sf object as a shapefile is just as easy. You use the sf::st_write() function, with the first argument being the name of the sf object you are exporting, and the second being the path to the new shapefile. For example, the code below will export an sf object called nc_loaded as nc2.shp (along with other supporting files).\n\nsf::st_write(\n  nc_loaded,\n  dsn = \"Data/nc2.shp\",\n  driver = \"ESRI Shapefile\",\n  append = FALSE\n)\n\nappend = FALSE forces writing the data when a file already exists with the same name. Without the option, this happens.\n\nsf::st_write(\n  nc_loaded,\n  dsn = \"Data/nc2.shp\",\n  layer = \"nc2\",\n  driver = \"ESRI Shapefile\"\n)\n\nLayer nc2 in dataset Data/nc2.shp already exists:\nuse either append=TRUE to append to layer or append=FALSE to overwrite layer\n\n\nError in eval(expr, envir, enclos): Dataset already exists.\n\n\n\n2.3.3 Better alternatives\nIf your collaborator uses ArcGIS and insists on a shapefile, you can certainly use the command above to generate one. However, there is no real need to rely on the shapefile format. Popular alternatives, such as GeoJSON and GeoPackage, are more efficient. Unlike shapefiles, which generate multiple files, these formats produce a single file with .geojson or .gpkg extensions, respectively6. Both formats are easily readable in ArcGIS, so it might be worth encouraging your collaborators to switch to these more modern formats.\n6 It can be frustrating when a collaborator sends 15 files for just three geographic objects via email, which could have been reduced to three files using GeoJSON or GeoPackage.geojson\n\n#--- write as a gpkg file ---#\nsf::st_write(nc, dsn = \"Data/nc.geojson\", append = FALSE)\n\n#--- read a geojson file ---#\nnc &lt;- sf::st_read(\"Data/nc.geojson\")\n\ngpkg\n\n#--- write as a gpkg file ---#\nsf::st_write(nc, dsn = \"Data/nc.gpkg\", append = FALSE)\n\n#--- read a gpkg file ---#\nnc &lt;- sf::st_read(\"Data/nc.gpkg\")\n\nOr better yet, if your collaborator uses R (or if it is only you who is going to use the data), then just save it as an rds file using saveRDS(), which can be of course read using readRDS().\n\n#--- save as an rds ---#\nsaveRDS(nc, \"Data/nc_county.rds\")\n\n#--- read an rds ---#\nnc &lt;- readRDS(\"Data/nc_county.rds\")\n\nThe use of rds files can be particularly attractive when the dataset is large because rds files are typically more memory efficient than shapefiles, eating up less of your disk memory.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#sec-projection-sf",
    "href": "chapters/02-VectorDataBasics.html#sec-projection-sf",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.4 Projection with a different Coordinate Reference Systems",
    "text": "2.4 Projection with a different Coordinate Reference Systems\nYou often need to reproject an sf using a different coordinate reference system (CRS) because you need it to have the same CRS as an sf object that you are interacting it with (spatial join) or mapping it with. In order to check the current CRS for an sf object, you can use the sf::st_crs() function.\n\nsf::st_crs(nc)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\nwkt stands for Well Known Text7, which is one of many many formats to store CRS information.8 4267 is the SRID (Spatial Reference System Identifier) defined by the European Petroleum Survey Group (EPSG) for the CRS9.\n7 sf versions prior to 0.9 provides CRS information in the form of proj4string. The newer version of sf presents CRS in the form of wtk (see this slide). You can find the reason behind this change in the same slide, starting from here.8 See here for numerous other formats that represent the same CRS.9 You can find the CRS-EPSG number correspondence here.10 Potential pool of CRS is infinite. Only the commonly-used CRS have been assigned EPSG SRID.When you transform your sf using a different CRS, you can use its EPSG number if the CRS has an EPSG number.10 Let’s transform the sf to WGS 84 (another commonly used GCS), whose EPSG number is 4326. We can use the sf::st_transform() function to achieve that, with the first argument being the sf object you are transforming and the second being the EPSG number of the new CRS.\n\n#--- transform ---#\nnc_wgs84 &lt;- sf::st_transform(nc, 4326)\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_wgs84)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nNotice that wkt was also altered accordingly to reflect the change in CRS: datum was changed to WGS 84. Now, let’s transform (reproject) the data using NAD83 / UTM zone 17N CRS. Its EPSG number is \\(26917\\).11 So, the following code does the job.\n11 See here.\n#--- transform ---#\nnc_utm17N &lt;- sf::st_transform(nc_wgs84, 26917)\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_utm17N)\n\nCoordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]\n\n\nAs you can see in its CRS information, the projection system is now UTM zone 17N.\nYou often need to change the CRS of an sf object when you interact (e.g., spatial subsetting, joining, etc) it with another sf object. In such a case, you can extract the CRS of the other sf object using sf::st_crs() and use it for transformation.12 So, you do not need to find the EPSG of the CRS of the sf object you are interacting it with.\n12 In this example, we are using the same data with two different CRS. But, you get the point.\n#--- transform ---#\nnc_utm17N_2 &lt;- sf::st_transform(nc_wgs84, sf::st_crs(nc_utm17N))\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_utm17N_2)\n\nCoordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#quick-and-interactive-view-of-an-sf-object",
    "href": "chapters/02-VectorDataBasics.html#quick-and-interactive-view-of-an-sf-object",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.5 Quick and interactive view of an sf object",
    "text": "2.5 Quick and interactive view of an sf object\n\n2.5.1 Quick view\nThe easiest way to visualize an sf object is to use plot():\n\nplot(nc)\n\n\n\nQuick Visualization of an sf object\n\n\n\nAs you can see, plot() create a map for each variable where the spatial units are color-differentiated based on the values of the variable. For creating more elaborate maps that are of publication-quality, see Chapter 7.\n\n2.5.2 Interactive view\nSometimes it is useful to be able to tell where certain spatial objects are and what values are associated with them on a map. The tmap_leaflet() function from the tmap package can create an interactive map where you can point to a spatial object and the associated information is revealed on the map. Let’s use the North Carolina county map as an example here:\n\n\n\n\n\n\n\nAs you can see, if you put your cursor on a polygon (county) and click on it, then its information pops up.\n\nAlternatively, you could use the tmap package to create interactive maps. You can first create a static map following a syntax like this:\n\n#--- NOT RUN (for polygons) ---#\ntm_shape(sf) +\n  tm_polygons()\n\n#--- NOT RUN (for points) ---#\ntm_shape(sf) +\n  tm_symbols()\n\nThis creates a static map of nc where county boundaries are drawn:\n\n(\n  tm_nc_polygons &lt;- tm_shape(nc) + tm_polygons()\n)\n\n\n\n\n\n\n\nThen, you can apply tmap_leaflet() to the static map to have an interactive view of the map:\n\ntmap_leaflet(tm_nc_polygons)\n\n\n\n\n\n\nYou could also change the view mode of tmap objects to the view mode using tmap_mode(\"view\") and then simply evaluate tm_nc_polygons.\n\n#--- change to the \"view\" mode ---#\ntmap_mode(\"view\")\n\n#--- now you have an interactive biew ---#\ntm_nc_polygons\n\nNote that once you change the view mode to “view”, then the evaluation of all tmap objects become interactive. I prefer the first option, as I need to revert the view mode back to “plot” by tmap_mode(\"plot\") if I don’t want interactive views.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#sec-df-to-sf",
    "href": "chapters/02-VectorDataBasics.html#sec-df-to-sf",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.6 Turning a data.frame of points into an sf\n",
    "text": "2.6 Turning a data.frame of points into an sf\n\nOften times, you have a dataset with geographic coordinates as variables in a csv or other formats, which would not be recognized as a spatial dataset by R immediately when it is read into R. In this case, you need to identify which variables represent the geographic coordinates from the data set, and create an sf yourself. Fortunately, it is easy to do so using the st_as_sf() function. Let’s first read a dataset (irrigation wells in Nebraska):\n\n#--- read irrigation well registration data ---#\n(\n  wells &lt;- readRDS(\"Data/well_registration.rds\")\n)\n\nKey: &lt;wellid&gt;\n        wellid ownerid        nrdname acres  regdate section     longdd\n         &lt;int&gt;   &lt;int&gt;         &lt;char&gt; &lt;num&gt;   &lt;char&gt;   &lt;int&gt;      &lt;num&gt;\n     1:      2  106106 Central Platte   160 12/30/55       3  -99.58401\n     2:      3   14133   South Platte    46  4/29/31       8 -102.62495\n     3:      4   14133   South Platte    46  4/29/31       8 -102.62495\n     4:      5   14133   South Platte    46  4/29/31       8 -102.62495\n     5:      6   15837 Central Platte   160  8/29/32      20  -99.62580\n    ---                                                                \n105818: 244568  135045 Upper Big Blue    NA  8/26/16      30  -97.58872\n105819: 244569  105428    Little Blue    NA  8/26/16      24  -97.60752\n105820: 244570  135045 Upper Big Blue    NA  8/26/16      30  -97.58294\n105821: 244571  135045 Upper Big Blue    NA  8/26/16      25  -97.59775\n105822: 244572  105428    Little Blue    NA  8/26/16      15  -97.64086\n           latdd\n           &lt;num&gt;\n     1: 40.69825\n     2: 41.11699\n     3: 41.11699\n     4: 41.11699\n     5: 40.73268\n    ---         \n105818: 40.89017\n105819: 40.13257\n105820: 40.88722\n105821: 40.89639\n105822: 40.13380\n\n#--- check the class ---#\nclass(wells)\n\n[1] \"data.table\" \"data.frame\"\n\n\nAs you can see the data is not an sf object. In this dataset, longdd and latdd represent longitude and latitude, respectively. We now turn the dataset into an sf object:\n\n#--- recognize it as an sf ---#\nwells_sf &lt;- sf::st_as_sf(wells, coords = c(\"longdd\", \"latdd\"))\n\n#--- take a look at the data ---#\nhead(wells_sf[, 1:5])\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699\nCRS:           NA\n  wellid ownerid        nrdname acres  regdate                   geometry\n1      2  106106 Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2      3   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3      4   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4      5   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5      6   15837 Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6      7   90248 Central Platte   120  2/15/35 POINT (-99.64524 40.73164)\n\n\nNote that the CRS of wells_sf is NA. Obviously, \\(R\\) does not know the reference system without you telling it. We know13 that the geographic coordinates in the wells data is NAD 83 (\\(epsg=4269\\)) for this dataset. So, we can assign the right CRS using either sf::st_set_crs() or sf::st_crs().\n13 Yes, YOU need to know the CRS of your data.\n#--- set CRS ---#\nwells_sf &lt;- sf::st_set_crs(wells_sf, 4269)\n\n#--- or this ---#\nsf::st_crs(wells_sf) &lt;- 4269\n\n#--- see the change ---#\nhead(wells_sf[, 1:5])\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699\nGeodetic CRS:  NAD83\n  wellid ownerid        nrdname acres  regdate                   geometry\n1      2  106106 Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2      3   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3      4   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4      5   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5      6   15837 Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6      7   90248 Central Platte   120  2/15/35 POINT (-99.64524 40.73164)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#sec-conv_sp",
    "href": "chapters/02-VectorDataBasics.html#sec-conv_sp",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.7 Conversion to and from sp objects",
    "text": "2.7 Conversion to and from sp objects\nYou may find instances where sp objects are necessary or desirable. For example, some old spatial packages only accept sp objects. In such a case, it is good to know how to convert an sf object to an sp object, vice versa. You can convert an sf object to its sp counterpart using as(sf_object, \"Spatial\"):\n\n#--- conversion ---#\nwells_sp &lt;- as(wells_sf, \"Spatial\")\n\n#--- check the class ---#\nclass(wells_sp)\n\n[1] \"SpatialPointsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\nAs you can see wells_sp is a class of SpatialPointsDataFrame, points with a data.frame supported by the sp package. The above syntax works for converting an sf of polygons into SpatialPolygonsDataFrame as well14.\n14 The function does not work for an sf object that consists of different geometry types (e.g., POINT and POLYGON). This is because sp objects do not allow different types of geometries in the single sp object. For example, SpatialPointsDataFrame consists only of points data.You can revert wells_sp back to an sf object using the st_as_sf() function, as follows:\n\n#--- revert back to sf ---#\nwells_sf &lt;- sf::st_as_sf(wells_sp)\n\n#--- check the class ---#\nclass(wells_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe do not cover how to use the sp package as the benefit of learning it has become marginal compared to when sf was not yet mature15.\n\n15 For those interested in learning the sp package, this website is a good resource.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#non-spatial-transformation-of-sf",
    "href": "chapters/02-VectorDataBasics.html#non-spatial-transformation-of-sf",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.8 Non-spatial transformation of sf\n",
    "text": "2.8 Non-spatial transformation of sf\n\n\n2.8.1 Using dplyr\n\nAn important feature of an sf object is that it is basically a data.frame with geometric information stored as a variable (column). This means that transforming an sf object works just like transforming a data.frame. Basically, everything you can do to a data.frame, you can do to an sf as well. The code below just provides an example of basic operations including dplyr::select(), dplyr::filter(), and dplyr::mutate() in action with an sf object to just confirm that dplyr operations works with an sf object just like a data.frame.\n\n#--- here is what the data looks like ---#\ndplyr::select(wells_sf, wellid, nrdname, acres, regdate, nrdname)\n\nSimple feature collection with 105822 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid           nrdname acres  regdate                   geometry\n1       2    Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2       3      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3       4      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4       5      South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5       6    Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6       7    Central Platte   120  2/15/35 POINT (-99.64524 40.73164)\n7       8      South Platte   113   8/7/37 POINT (-103.5257 41.24492)\n8      10      South Platte   160   5/4/38 POINT (-103.0284 41.13243)\n9      11 Middle Republican   807   5/6/38  POINT (-101.1193 40.3527)\n10     12 Middle Republican   148 11/29/77 POINT (-101.1146 40.35631)\n\n\nNotice that geometry column will be retained after dplyr::select() even if you did not tell R to keep it above.\nLet’s apply dplyr::select(), dplyr::filter(), and dplyr::mutate() to the dataset.\n\n#--- do some transformations ---#\nwells_sf %&gt;%\n  #--- select variables (geometry will always remain after select) ---#\n  dplyr::select(wellid, nrdname, acres, regdate, nrdname) %&gt;%\n  #--- removes observations with acre &lt; 30  ---#\n  dplyr::filter(acres &gt; 30) %&gt;%\n  #--- hectare instead of acre ---#\n  dplyr::mutate(hectare = acres * 0.404686)\n\nSimple feature collection with 63271 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0529 ymin: 40.00161 xmax: -96.87681 ymax: 41.73599\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid           nrdname acres  regdate                   geometry   hectare\n1       2    Central Platte   160 12/30/55 POINT (-99.58401 40.69825)  64.74976\n2       3      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n3       4      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n4       5      South Platte    46  4/29/31 POINT (-102.6249 41.11699)  18.61556\n5       6    Central Platte   160  8/29/32  POINT (-99.6258 40.73268)  64.74976\n6       7    Central Platte   120  2/15/35 POINT (-99.64524 40.73164)  48.56232\n7       8      South Platte   113   8/7/37 POINT (-103.5257 41.24492)  45.72952\n8      10      South Platte   160   5/4/38 POINT (-103.0284 41.13243)  64.74976\n9      11 Middle Republican   807   5/6/38  POINT (-101.1193 40.3527) 326.58160\n10     12 Middle Republican   148 11/29/77 POINT (-101.1146 40.35631)  59.89353\n\n\nNow, let’s try to get a summary of a variable by group using the group_by() and summarize() functions. Here, we use only the first 100 observations because dplyr::summarize() takes just too long.\n\n#--- summary by group ---#\nwells_by_nrd &lt;-\n  wells_sf[1:100, ] %&gt;%\n  #--- group by nrdname ---#\n  dplyr::group_by(nrdname) %&gt;%\n  #--- summarize ---#\n  dplyr::summarize(tot_acres = sum(acres, na.rm = TRUE))\n\n#--- take a look ---#\nwells_by_nrd\n\nSimple feature collection with 8 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -103.6467 ymin: 40.0475 xmax: -97.89758 ymax: 41.24492\nGeodetic CRS:  NAD83\n# A tibble: 8 × 3\n  nrdname           tot_acres                                           geometry\n  &lt;chr&gt;                 &lt;dbl&gt;                                   &lt;MULTIPOINT [°]&gt;\n1 Central Platte        6690. ((-99.00983 40.7301), (-99.03167 40.71927), (-99.…\n2 Lower Republican       350  ((-99.30403 40.05116), (-99.21946 40.05126), (-99…\n3 Middle Republican     1396  ((-100.9216 40.18866), (-101.1663 40.36351), (-10…\n4 South Platte          1381  ((-103.6467 41.2375), (-103.6128 41.23761), (-103…\n5 Tri-Basin              480  ((-98.94967 40.64285), (-98.94906 40.65061), (-98…\n6 Twin Platte           1098. ((-100.9571 41.18633), (-100.8659 41.14635), (-10…\n7 Upper Big Blue         330        ((-97.89758 40.86156), (-97.89998 40.86336))\n8 Upper Republican       480         ((-101.8858 40.47776), (-101.6501 40.6012))\n\n\nSo, we can summarize an sf by group using dplyr::group_by() and dplyr::summarize(). One interesting change that happened is the geometry variable. Each NRD now has multipoint sfg, which is the combination of all the wells (points) located inside the NRD. Now, it is hard to imagine that you need summarized geometries after group summary. Moreover, it is a very slow operation. If you have lots of free time, try running the above code with wells_sf instead of wells_sf[1:100, ]. I never waited for it to finish as it was running for a long long time. It is advised that you simply drop the geometry and turn the sf object into a data.frame (or tibble, data.table) first and then do group summary.\n\n#--- remove geometry ---#\nwells_no_longer_sf &lt;- sf::st_drop_geometry(wells_sf)\n\n#--- take a look ---#\nhead(wells_no_longer_sf)\n\n  wellid ownerid        nrdname acres  regdate section\n1      2  106106 Central Platte   160 12/30/55       3\n2      3   14133   South Platte    46  4/29/31       8\n3      4   14133   South Platte    46  4/29/31       8\n4      5   14133   South Platte    46  4/29/31       8\n5      6   15837 Central Platte   160  8/29/32      20\n6      7   90248 Central Platte   120  2/15/35      19\n\n\nWe can now do a group summary very quickly:\n\nwells_no_longer_sf %&gt;%\n  #--- group by nrdname ---#\n  dplyr::group_by(nrdname) %&gt;%\n  #--- summarize ---#\n  dplyr::summarize(tot_acres = sum(acres, na.rm = TRUE))\n\n# A tibble: 9 × 2\n  nrdname           tot_acres\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Central Platte     1890918.\n2 Little Blue         995900.\n3 Lower Republican    543079.\n4 Middle Republican   443472.\n5 South Platte        216109.\n6 Tri-Basin           847058.\n7 Twin Platte         452678.\n8 Upper Big Blue     1804782.\n9 Upper Republican    551906.\n\n\n\n2.8.2 Using data.table\n\nThe data.table package provides data wrangling options that are extremely fast (see here for various benchmark results). It particularly shines when datasets are large and is much faster than dplyr. However, it cannot be as naturally integrated into the workflow involving sf objects as dplyr can. Let’s convert an sf object of points into a data.table object using data.table().\n\n#--- convert an sf to data.table ---#\n(\n  wells_dt &lt;- data.table::data.table(wells_sf)\n)\n\n        wellid ownerid        nrdname acres  regdate section\n         &lt;int&gt;   &lt;int&gt;         &lt;char&gt; &lt;num&gt;   &lt;char&gt;   &lt;int&gt;\n     1:      2  106106 Central Platte   160 12/30/55       3\n     2:      3   14133   South Platte    46  4/29/31       8\n     3:      4   14133   South Platte    46  4/29/31       8\n     4:      5   14133   South Platte    46  4/29/31       8\n     5:      6   15837 Central Platte   160  8/29/32      20\n    ---                                                     \n105818: 244568  135045 Upper Big Blue    NA  8/26/16      30\n105819: 244569  105428    Little Blue    NA  8/26/16      24\n105820: 244570  135045 Upper Big Blue    NA  8/26/16      30\n105821: 244571  135045 Upper Big Blue    NA  8/26/16      25\n105822: 244572  105428    Little Blue    NA  8/26/16      15\n                          geometry\n                       &lt;sfc_POINT&gt;\n     1: POINT (-99.58401 40.69825)\n     2: POINT (-102.6249 41.11699)\n     3: POINT (-102.6249 41.11699)\n     4: POINT (-102.6249 41.11699)\n     5:  POINT (-99.6258 40.73268)\n    ---                           \n105818: POINT (-97.58872 40.89017)\n105819: POINT (-97.60752 40.13257)\n105820: POINT (-97.58294 40.88722)\n105821: POINT (-97.59775 40.89639)\n105822:  POINT (-97.64086 40.1338)\n\n#--- check the class ---#\nclass(wells_dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\nYou see that wells_dt is no longer an sf object, but the geometry column still remains in the data.\nIf you attempt to run sf operations on a data.table object that contains a geometry column, you will encounter an error, as shown below:\n\nsf::st_buffer(wells_dt, dist = 2)\n\nError in UseMethod(\"st_buffer\"): no applicable method for 'st_buffer' applied to an object of class \"c('data.table', 'data.frame')\"\n\n\nHowever, you can apply sf spatial operations only on the geometry like this:\n\n#--- work on the first 10 ---#\nwells_dt[1:10, ]$geometry %&gt;%\n  sf::st_buffer(dist = 2) %&gt;%\n  head()\n\nGeometry set for 6 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.625 ymin: 40.69823 xmax: -99.58398 ymax: 41.11701\nGeodetic CRS:  NAD83\nFirst 5 geometries:\n\n\nFinally, it is easy to revert a data.table object back to an sf object again by using the st_as_sf() function.\n\n#--- wells ---#\n(\n  wells_sf_again &lt;- sf::st_as_sf(wells_dt)\n)\n\nSimple feature collection with 105822 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942\nGeodetic CRS:  NAD83\nFirst 10 features:\n   wellid ownerid           nrdname acres  regdate section\n1       2  106106    Central Platte   160 12/30/55       3\n2       3   14133      South Platte    46  4/29/31       8\n3       4   14133      South Platte    46  4/29/31       8\n4       5   14133      South Platte    46  4/29/31       8\n5       6   15837    Central Platte   160  8/29/32      20\n6       7   90248    Central Platte   120  2/15/35      19\n7       8   48113      South Platte   113   8/7/37      28\n8      10   17073      South Platte   160   5/4/38       2\n9      11   98432 Middle Republican   807   5/6/38      36\n10     12   79294 Middle Republican   148 11/29/77      31\n                     geometry\n1  POINT (-99.58401 40.69825)\n2  POINT (-102.6249 41.11699)\n3  POINT (-102.6249 41.11699)\n4  POINT (-102.6249 41.11699)\n5   POINT (-99.6258 40.73268)\n6  POINT (-99.64524 40.73164)\n7  POINT (-103.5257 41.24492)\n8  POINT (-103.0284 41.13243)\n9   POINT (-101.1193 40.3527)\n10 POINT (-101.1146 40.35631)\n\n\nThis means that if you need fast data transformation, you can first convert an sf object to a data.table, perform the necessary transformations using data.table functions, and then convert it back to an sf object if needed.\nThose who know the dtplyr package (it takes advantage of the speed of data.table while you can keep using dplyr syntax and functions) may wonder if it works well with sf objects. Nope:\n\nlibrary(dtplyr)\n\n#--- convert an \"lazy\" data.table ---#\nwells_ldt &lt;- lazy_dt(wells_sf)\n\n#--- try  ---#\nsf::st_buffer(wells_ldt, dist = 2)\n\nError in UseMethod(\"st_buffer\"): no applicable method for 'st_buffer' applied to an object of class \"c('dtplyr_step_first', 'dtplyr_step')\"\n\n\nBy the way, this package is awesome if you really love dplyr, but want the speed of data.table. dtplyr is of course slightly slower than data.table because internal translations of dplyr language to data.table language have to happen first.16\n16 I personally use data.table unless it is necessary to use dplyr like operations when dealing with sf objects. It is more concise than dplyr, which is somewhat verbose (yet expressive because of it). Ultimately, it is your personal preference which to use. You might be interested in reading this discussion about the comparative advantages and disadvantages of the two packages.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#non-interactive-geometrical-operations",
    "href": "chapters/02-VectorDataBasics.html#non-interactive-geometrical-operations",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.9 Non-interactive geometrical operations",
    "text": "2.9 Non-interactive geometrical operations\nThere are various geometrical operations that are particularly useful for economists. Here, some of the most commonly used geometrical operations are introduced17. You can see the practical use of some of these functions in Chapter 1.\n17 For the complete list of available geometrical operations under the sf package, see here.\n2.9.1 sf::st_buffer()\n\nsf::st_buffer() creates a buffer around points, lines, or the border of polygons. Let’s create buffers around points. First, we read well locations data (Figure 2.7).\n\n#--- read wells location data ---#\nurnrd_wells_sf &lt;-\n  readRDS(\"Data/urnrd_wells.rds\") %&gt;%\n  #--- project to UTM 14N WGS 84 ---#\n  sf::st_transform(32614)\n\n\n\n\n\nCodeggplot(urnrd_wells_sf) +\n  geom_sf(size = 0.4, color = \"red\") +\n  theme_void()\n\n\n\n\n\n\nFigure 2.7: Map of the wells\n\n\n\n\nLet’s create buffers around the wells.\n\n#--- create a one-mile buffer around the wells ---#\nwells_buffer &lt;- sf::st_buffer(urnrd_wells_sf, dist = 1600)\n\nAs you can see, there are many circles around wells with the radius of \\(1,600\\) meters (Figure 2.8).\n\nCodeggplot() +\n  geom_sf(data = urnrd_wells_sf, size = 0.6, color = \"red\") +\n  geom_sf(data = wells_buffer, fill = NA) +\n  theme_void()\n\n\n\n\n\n\nFigure 2.8: Buffers around wells\n\n\n\n\nA practical application of buffer creation can be seen in Section 1.1.\n\nWe now create buffers around polygons. First, read county boundary data and select three counties (Chase, Dundy, and Perkins) in Nebraska (Figure 2.9).\n\nNE_counties &lt;-\n  readRDS(\"Data/NE_county_borders.rds\") %&gt;%\n  dplyr::filter(NAME %in% c(\"Perkins\", \"Dundy\", \"Chase\")) %&gt;%\n  sf::st_transform(32614)\n\n\n\n\n\nCodeggplot(NE_counties) +\n  geom_sf(aes(fill = NAME)) +\n  scale_fill_brewer(name = \"County\", palette = \"RdYlGn\") +\n  theme_void()\n\n\n\n\n\n\nFigure 2.9: Map of the three counties\n\n\n\n\nThe following code creates buffers around polygons:\n\nNE_buffer &lt;- sf::st_buffer(NE_counties, dist = 2000)\n\n\nCodeggplot() +\n  geom_sf(data = NE_buffer, fill = \"blue\", alpha = 0.3) +\n  geom_sf(data = NE_counties, aes(fill = NAME)) +\n  scale_fill_brewer(name = \"County\", palette = \"RdYlGn\") +\n  theme_void()\n\n\n\n\n\n\nFigure 2.10: Buffers around the three counties\n\n\n\n\nFor example, this can be useful to identify observations which are close to the border of political boundaries when you want to take advantage of spatial discontinuity of policies across adjacent political boundaries.\n\n2.9.2 sf::st_area()\n\nThe sf::st_area() function calculates the area of polygons.\n\n#--- generate area by polygon ---#\n(\n  NE_counties &lt;- dplyr::mutate(NE_counties, area = st_area(NE_counties))\n)\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry             area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 [m^2]\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 [m^2]\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530 [m^2]\n\n\nNow, as you can see below, the default class of the results of sf::st_area() is units, which does not accept numerical operations.\n\nclass(NE_counties$area)\n\n[1] \"units\"\n\n\nSo, let’s turn it into double.\n\n(\n  NE_counties &lt;- dplyr::mutate(NE_counties, area = as.numeric(area))\n)\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry       area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530\n\n\nsf::st_area() is useful when you want to find area-weighted average of characteristics after spatially joining two polygon layers using the st_intersection() function (See Section 3.4.4).\n\n2.9.3 sf::st_centroid()\n\nThe sf::st_centroid() function finds the centroid of each polygon (Figure 2.11).\n\n#--- create centroids ---#\n(\n  NE_centroids &lt;- sf::st_centroid(NE_counties)\n)\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 271156.7 ymin: 4450826 xmax: 276594.1 ymax: 4525635\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                 geometry       area\n1 2840176 POINT (276594.1 4525635) 2302174854\n2 7978172 POINT (271469.9 4489429) 2316908196\n3 3046331 POINT (271156.7 4450826) 2389890530\n\n\n\nCodeggplot() +\n  geom_sf(data = NE_counties) +\n  geom_sf(data = NE_centroids) +\n  theme_void()\n\n\n\n\n\n\nFigure 2.11: The centroids of the polygons\n\n\n\n\nIt can be useful when creating a map with labels because the centroid of polygons tend to be a good place to place labels (Figure 2.12).18\n18 When creating maps with the ggplot2 package, you can use geom_sf_text() or geom_sf_label(), which automatically finds where to put texts. See some examples here.\nCodeggplot() +\n  geom_sf(data = NE_counties) +\n  geom_sf_text(data = NE_centroids, aes(label = NAME)) +\n  theme_void()\n\n\n\n\n\n\nFigure 2.12: County names placed at the centroids of the counties\n\n\n\n\n\n2.9.4 sf::st_length()\n\nWe can use sf::st_length() to calculate great circle distances19 of LINESTRING and MULTILINESTRING when they are represented in geodetic coordinates. On the other hand, if they are projected and use a Cartesian coordinate system, it will calculate Euclidean distance. We use U.S. railroad data for a demonstration.\n19 Great circle distance is the shortest distance between two points on the surface of a sphere (earth)\n#--- import US railroad data and take only the first 10 of it ---#\n(\n  a_railroad &lt;- rail_roads &lt;- sf::st_read(dsn = \"Data\", layer = \"tl_2015_us_rails\")[1:10, ]\n)\n\nReading layer `tl_2015_us_rails' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 180958 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006\nGeodetic CRS:  NAD83\n\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776\nGeodetic CRS:  NAD83\n      LINEARID                    FULLNAME MTFCC                       geometry\n1  11020239500       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058...\n2  11020239501       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n3  11020239502       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.6682 ...\n4  11020239503       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n5  11020239504       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031...\n6  11020239575      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n7  11020239576      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852...\n8  11020239577      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n9  11020239589    Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736...\n10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848...\n\n#--- check CRS ---#\nsf::st_crs(a_railroad)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nIt uses geodetic coordinate system. Let’s calculate the great circle distance of the lines (Section 1.4 for a practical use case of this function).\n\n(\n  a_railroad &lt;- mutate(a_railroad, length = sf::st_length(a_railroad))\n)\n\nSimple feature collection with 10 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776\nGeodetic CRS:  NAD83\n      LINEARID                    FULLNAME MTFCC                       geometry\n1  11020239500       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058...\n2  11020239501       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n3  11020239502       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.6682 ...\n4  11020239503       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687...\n5  11020239504       Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031...\n6  11020239575      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n7  11020239576      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852...\n8  11020239577      Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695...\n9  11020239589    Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736...\n10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848...\n           length\n1    662.0043 [m]\n2    658.1026 [m]\n3  19953.8571 [m]\n4  13880.5060 [m]\n5   7183.7286 [m]\n6   1062.4649 [m]\n7   7832.3375 [m]\n8  31773.7700 [m]\n9   4544.6795 [m]\n10 17101.8581 [m]\n\n\n\n2.9.5 sf::st_distance()\n\nsf::st_distance calculates distances between sfgs. Its output is the matrix of distances whose \\(i,j\\) element is the distance between the \\(i\\)th sfg of the first sf and \\(j\\)th sfg of the second sf. The following code find the distance between the first 10 wells in urnrd_wells_sf.\n\nst_distance(urnrd_wells_sf[1:10, ], urnrd_wells_sf[1:10, ])\n\nUnits: [m]\n          [,1]       [,2]       [,3]      [,4]     [,5]      [,6]       [,7]\n [1,]     0.00 24224.9674 24007.0480 19678.000 26974.04 25490.735 21710.2051\n [2,] 24224.97     0.0000   386.7858  8137.875 44632.84  2180.034 14285.1996\n [3,] 24007.05   386.7858     0.0000  7754.775 44610.53  2067.106 13901.5230\n [4,] 19678.00  8137.8750  7754.7753     0.000 43774.40  7892.077  6753.1200\n [5,] 26974.04 44632.8443 44610.5290 43774.401     0.00 46610.537 47713.4334\n [6,] 25490.73  2180.0343  2067.1055  7892.077 46610.54     0.000 13362.2041\n [7,] 21710.21 14285.1996 13901.5230  6753.120 47713.43 13362.204     0.0000\n [8,] 22029.35 14345.1319 13962.6364  6909.932 48027.14 13377.109   319.3365\n [9,] 17243.59 18297.5078 17916.8399 10172.159 44061.21 17978.355  6297.5682\n[10,] 25184.43  6052.8916  5747.7464  5709.523 48215.58  4340.971  9505.0583\n            [,8]      [,9]     [,10]\n [1,] 22029.3535 17243.589 25184.434\n [2,] 14345.1319 18297.508  6052.892\n [3,] 13962.6364 17916.840  5747.746\n [4,]  6909.9319 10172.159  5709.523\n [5,] 48027.1382 44061.209 48215.582\n [6,] 13377.1087 17978.355  4340.971\n [7,]   319.3365  6297.568  9505.058\n [8,]     0.0000  6543.339  9465.084\n [9,]  6543.3389     0.000 14832.639\n[10,]  9465.0840 14832.639     0.000\n\n\n\n2.9.6 sf::st_union() and sf::st_combine()\n\nWhile sf::st_combine() combines multiple sfgs in a single sf object into a single sfg without resolving the internal borders, sf::st_union() does so with the internal borders resolved. Let’s take a look at what sf::st_combine() does first using NE_counties.\nNE_counties has three features as you can see below.\n\nNE_counties\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry       area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530\n\n\nAfter applying sf::st_combine(), the resulting object has only a single feature.\n\n(\n  NE_counties_combined &lt;- sf::st_combine(NE_counties)\n)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n\n\n\n\n\n\nCodeggplot(NE_counties_combined) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 2.13\n\n\n\n\nAs you can see in Figure 2.13, the internal borders did not get resolved. It is just that all the three counties are now recognized as a single feature rather than three separate features.\nOn the other hand, sf::st_union() resolve the internal borders, while combining all the features (Figure 2.14). A use case of sf::st_union() can be seen in Section 1.6.\n\n(\n  NE_counties_union &lt;- sf::st_union(NE_counties)\n)\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n\n\n\n\n\n\nCodeggplot(data = NE_counties_union) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 2.14\n\n\n\n\nNote also that the resulting objects of sf::st_combine() and sf::st_union() are no longer sf, rather they are sfc, which do not retain the original variables in NE_counties.\n\nclass(NE_counties_combined)\n\n[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \n\nclass(NE_counties_union)\n\n[1] \"sfc_POLYGON\" \"sfc\"        \n\n\n\n2.9.7 sf::st_simplify() and rmapshaper::ms_simplify()\n\nSometimes, a vector data is excessively detailed for your purpose. For example, when showing the relative location of Japan to the surrounding countries in a map, the spatial object representing Japan does not have to be very accurate in its coastlines, say at the meter level. In such cases, you can simplify the spatial object using sf::st_simplify() so that you can render the map much faster.\nHere, we use IL County borders for illustration (Figure 2.15).\n\nIL_counties &lt;- sf::st_read(\"Data/IL_county_detailed.geojson\")\n\nReading layer `IL_county_detailed' from data source \n  `/Users/tmieno2/Dropbox/TeachingUNL/R-as-GIS-for-Scientists/chapters/Data/IL_county_detailed.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 102 features and 61 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51309 ymin: 36.9703 xmax: -87.4952 ymax: 42.50849\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nCodeggplot(IL_counties) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 2.15: Illinois county border\n\n\n\n\nHere is the object size of IL_counties.\n\nobject.size(IL_counties)\n\n1628016 bytes\n\n\nWhen you focus on a single county, you can see that county borders in this data are fairly detaield. Let’s look at the Cook county (Figure 2.16).\n\nCook &lt;- filter(IL_counties, NAME == \"Cook County\")\n\n\nCode(\n  g_cook_original &lt;-\n    ggplot(Cook) +\n    geom_sf() +\n    theme_void()\n)\n\n\n\n\n\n\nFigure 2.16: Cook County border\n\n\n\n\nYou can see that its border with Lake Michigan has lots of non-linear segments. Let’s simplify this using sf::st_simplify(). We can use the dTolerance option to specify the degree of simplification.\n\nCook_simplify &lt;- sf::st_simplify(Cook, dTolerance = 1000)\n\nFigure 2.17 below compares the original (left) and simplified (right) versions.\n\nCode #| \ng_cook_simplified &lt;-\n  ggplot(Cook_simplify) +\n  geom_sf() +\n  theme_void()\n\ng_cook_original | g_cook_simplified\n\n\n\n\n\n\nFigure 2.17: Before and after simplifying Cook County border\n\n\n\n\nLet’s now simplify IL_counties using sf::st_simplify() to make its map light-weight (Figure 2.18).\n\nIL_counties_simplified &lt;- sf::st_simplify(IL_counties, dTolerance = 1000)\n\n\nCodeggplot(IL_counties_simplified) +\n  geom_sf(fill = \"blue\", alpha = 0.5) +\n  theme_void()\n\n\n\n\n\n\nFigure 2.18: Simpligied Illinois county borders\n\n\n\n\nYou probably notice we now have some gaps between some counties. This is because sf::st_simplify() does not respect the internal borders. To simplify only the outer borders, you can use rmapshaper::ms_simplify(). The keep option controls the degree of simplification (the lower, the more simplified).\n\nIL_counties_mssimplified &lt;- rmapshaper::ms_simplify(IL_counties, keep = 0.01)\n\n\nCodeggplot(IL_counties_mssimplified) +\n  geom_sf(fill = \"blue\", alpha = 0.5) +\n  theme_void()\n\n\n\n\n\n\nFigure 2.19: Illinois county border simplified with ms_simplify()\n\n\n\n\nYou can see that internal borders are simplified in a way that does not create holes (see Section 1.6 for an application that makes use of rmapshaper::ms_simplify()).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/02-VectorDataBasics.html#sec-sf-basics-exercises",
    "href": "chapters/02-VectorDataBasics.html#sec-sf-basics-exercises",
    "title": "2  Vector Data Handling with sf",
    "section": "\n2.10 Exercises",
    "text": "2.10 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nBefore working on exercises, please run the following codes by hitting the Run Code button.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nTips for working with an webr session\n\n\n\n\nHit the Run Code button to run all the codes in the code area.\nTo run only parts of the code, highlight the section you want to execute with your cursor and then press Cmd + Return (Mac) or Ctrl + Enter (Windows).\nAll the code blocks are interconnected and R objects defined in one R code block is available in another R code block.\n\n\n\n\n2.10.1 Exercise 1: Projection\nAfter running data(fairway_grid), check the CRS of fairway_grid and then transform it using WGS84 (look up the EPSG code for WGS84).20\n20 Hint: see Section 2.4\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodedata(fairway_grid)\nsf::st_crs(fairway_grid)\nsf::st_transform(fairway_grid, 4326)\n\n\n\n\n\n\n\n2.10.2 Exercise 2: Turn a data.frame of points into an sf\n\nUsing the LAT (latitude) and LNG (longitude) columns in mower_sensor, turn the tibble into an sf, and then assign the CRS of WGS 84 using its EPSG code.21\n21 Hint: see Section 2.6\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodest_as_sf(\n  mower_sensor,\n  coords = c(\"LNG\", \"LAT\"),\n  crs = 4326\n)\n\n\n\n\n\n\n2.10.3 Exercise 3: Centroid and distance\nFind the centroid of the polygons in fairway_grid and then calculate the distance between the centroids. Which polygon is the closest to the first polygon recorded in fairway_grid?22\n22 Hint: see Section 2.9.3 and Section 2.9.5\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodedata(fairway_grid)\ncentroids &lt;- sf::st_centroid(fairway_grid)\ndist_mat &lt;- sf::st_distance(centroids)\n\nwhich.min(dist_mat[1, ][-1]) + 1",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vector Data Handling with `sf`</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html",
    "href": "chapters/03-SpatialInteractionVectorVector.html",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "",
    "text": "Before you start\nIn this chapter, we explore the spatial interactions between two sf objects. We begin by examining topological relations—how two spatial objects relate to each other—focusing on sf::st_intersects() and sf::st_is_within_distance(). sf::st_intersects() is particularly important as it is the most commonly used topological relation and serves as the default for spatial subsetting and joining in sf.\nNext, we cover spatial subsetting, which involves filtering spatial data based on the geographic features of another dataset. Finally, we will discuss spatial joining, where attribute values from one spatial dataset are assigned to another based on their topological relationship1.\nTo test your knowledge, you can work on coding exercises at Section 3.7.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#before-you-start",
    "href": "chapters/03-SpatialInteractionVectorVector.html#before-you-start",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "",
    "text": "1 For those familiar with the sp package, these operations are similar to sp::over().\nDirection for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nPackages\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  ggplot2, # for map creation\n  tmap # for map creation\n)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#sec-topo",
    "href": "chapters/03-SpatialInteractionVectorVector.html#sec-topo",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.1 Topological relations",
    "text": "3.1 Topological relations\nBefore diving into spatial subsetting and joining, we first examine topological relations. These refer to how multiple spatial objects are related to each other in space. The sf package provides various functions to identify these spatial relationships, with our primary focus being on intersections, which can be identified using sf::st_intersects()2. We will also briefly explore sf::st_is_within_distance(). If you’re interested in other topological relations, you can check the documentation with ?geos_binary_pred.\n2 Other topological relations like st_within() or st_touches() are rarely used.\n3.1.1 Data preparation\nWe will use three sf objects: points, lines, and polygons.\nPOINTS\n\n#--- create points ---#\npoint_1 &lt;- sf::st_point(c(2, 2))\npoint_2 &lt;- sf::st_point(c(1, 1))\npoint_3 &lt;- sf::st_point(c(1, 3))\n\n#--- combine the points to make a single  sf of points ---#\n(\npoints &lt;- \n  list(point_1, point_2, point_3) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(point_name = c(\"point 1\", \"point 2\", \"point 3\"))\n)\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 2 ymax: 3\nCRS:           NA\n            x point_name\n1 POINT (2 2)    point 1\n2 POINT (1 1)    point 2\n3 POINT (1 3)    point 3\n\n\nLINES\n\n#--- create points ---#\nline_1 &lt;- sf::st_linestring(rbind(c(0, 0), c(2.5, 0.5)))\nline_2 &lt;- sf::st_linestring(rbind(c(1.5, 0.5), c(2.5, 2)))\n\n#--- combine the points to make a single  sf of points ---#\n(\nlines &lt;- \n  list(line_1, line_2) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(line_name = c(\"line 1\", \"line 2\"))\n)\n\nSimple feature collection with 2 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 2\nCRS:           NA\n                            x line_name\n1   LINESTRING (0 0, 2.5 0.5)    line 1\n2 LINESTRING (1.5 0.5, 2.5 2)    line 2\n\n\nPOLYGONS\n\n#--- create polygons ---#\npolygon_1 &lt;- sf::st_polygon(list(\n  rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)) \n))\n\npolygon_2 &lt;- sf::st_polygon(list(\n  rbind(c(0.5, 1.5), c(0.5, 3.5), c(2.5, 3.5), c(2.5, 1.5), c(0.5, 1.5)) \n))\n\npolygon_3 &lt;- sf::st_polygon(list(\n  rbind(c(0.5, 2.5), c(0.5, 3.2), c(2.3, 3.2), c(2, 2), c(0.5, 2.5)) \n))\n\n#--- combine the polygons to make an sf of polygons ---#\n(\npolygons &lt;- \n  list(polygon_1, polygon_2, polygon_3) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(polygon_name = c(\"polygon 1\", \"polygon 2\", \"polygon 3\"))\n)\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 3.5\nCRS:           NA\n                               x polygon_name\n1 POLYGON ((0 0, 2 0, 2 2, 0 ...    polygon 1\n2 POLYGON ((0.5 1.5, 0.5 3.5,...    polygon 2\n3 POLYGON ((0.5 2.5, 0.5 3.2,...    polygon 3\n\n\n\nFigure 3.1 shows how they look together:\n\nCodeggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\") + \n  geom_sf(data = points, aes(shape = point_name), size = 4) +\n  scale_shape_discrete(name = \"Points\")  \n\n\n\n\n\n\nFigure 3.1: Visualization of the points, lines, and polygons\n\n\n\n\n\n3.1.2 sf::st_intersects()\n\nsf::st_intersects() identifies which sfg object in an sf (or sfc) intersects with sfg object(s) in another sf. For example, you can use the function to identify which well is located within which county. sf::st_intersects() is the most commonly used topological relation. It is important to understand what it does as it is the default topological relation used when performing spatial subsetting and joining, which we will cover later.\n\npoints and polygons (Figure 3.2)\n\n\n\n\nCodeggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = points, aes(shape = point_name), size = 4) +\n  scale_shape_discrete(name = \"Points\")\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\nsf::st_intersects(points, polygons)\n\nSparse geometry binary predicate list of length 3, where the predicate\nwas `intersects'\n 1: 1, 2, 3\n 2: 1\n 3: 2, 3\n\n\nAs you can see, the output is a list of which polygon(s) each of the points intersect with. The numbers 1, 2, and 3 in the first row mean that 1st (polygon 1), 2nd (polygon 2), and 3rd (polygon 3) objects of the polygons intersect with the first point (point 1) of the points object. The fact that point 1 is considered to be intersecting with polygon 2 means that the area inside the border is considered a part of the polygon (of course).\nIf you would like the results of sf::st_intersects() in a matrix form with boolean values filling the matrix, you can add sparse = FALSE option.\n\nsf::st_intersects(points, polygons, sparse = FALSE)\n\n      [,1]  [,2]  [,3]\n[1,]  TRUE  TRUE  TRUE\n[2,]  TRUE FALSE FALSE\n[3,] FALSE  TRUE  TRUE\n\n\n\nlines and polygons (Figure 3.3)\n\n\n\n\nCodeggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\")\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\nsf::st_intersects(lines, polygons)\n\nSparse geometry binary predicate list of length 2, where the predicate\nwas `intersects'\n 1: 1\n 2: 1, 2\n\n\nThe output is a list of which polygon(s) each of the lines intersect with. For example, line intersects with the 1st and 2nd elements of polygons, which are polygon 1 and polygon 2.\n\npolygons and polygons (Figure 3.4)\n\n\n\n\nCodeggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\")\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\nFor polygons vs polygons interaction, sf::st_intersects() identifies any polygons that either touches (even at a point like polygons 1 and 3) or share some area.\n\nsf::st_intersects(polygons, polygons)\n\nSparse geometry binary predicate list of length 3, where the predicate\nwas `intersects'\n 1: 1, 2, 3\n 2: 1, 2, 3\n 3: 1, 2, 3\n\n\nClearly, all of them interact with all the polygons (including themselves) in this case.\n\n3.1.3 sf::st_is_within_distance()\n\nThis function identifies whether two spatial objects are within the distance you specify3.\n3 For example, this function can be useful to identify neighbors. For example, you may want to find irrigation wells located around well \\(i\\) to label them as well \\(i\\)’s neighbor.We will use two sets of points data.\n\nCodeset.seed(38424738)\n\npoints_set_1 &lt;-\n  lapply(1:5, function(x) sf::st_point(runif(2))) %&gt;% \n  sf::st_sfc() %&gt;% sf::st_as_sf() %&gt;% \n  dplyr::mutate(id = 1:nrow(.))\n\npoints_set_2 &lt;-\n  lapply(1:5, function(x) sf::st_point(runif(2))) %&gt;% \n  sf::st_sfc() %&gt;% sf::st_as_sf() %&gt;% \n  dplyr::mutate(id = 1:nrow(.))\n\n\n\npoints_set_1\n\nSimple feature collection with 5 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.2641607 ymin: 0.2094549 xmax: 0.789738 ymax: 0.9198815\nCRS:           NA\n                            x id\n1 POINT (0.2809702 0.7930091)  1\n2 POINT (0.4907635 0.9198815)  2\n3  POINT (0.789738 0.2094549)  3\n4 POINT (0.2641607 0.3031891)  4\n5 POINT (0.4544901 0.6499282)  5\n\npoints_set_2\n\nSimple feature collection with 5 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.01389647 ymin: 0.2871567 xmax: 0.7799449 ymax: 0.8535346\nCRS:           NA\n                             x id\n1  POINT (0.2641116 0.8535346)  1\n2 POINT (0.01389647 0.2871567)  2\n3  POINT (0.5088085 0.6107668)  3\n4   POINT (0.248774 0.5386513)  4\n5  POINT (0.7799449 0.6599416)  5\n\n\nHere is how they are spatially distributed (Figure 3.5). Instead of circles of points, their corresponding id (or equivalently row number here) values are displayed.\n\nCodeggplot() +\n  geom_sf_text(data = points_set_1, aes(label = id), color = \"red\") +\n  geom_sf_text(data = points_set_2, aes(label = id), color = \"blue\") \n\n\n\n\n\n\nFigure 3.5: The locations of the set of points\n\n\n\n\nWe want to know which of the blue points (points_set_2) are located within 0.2 from each of the red points (points_set_1). Figure 3.6 gives us the answer visually.\n\nCode#--- create 0.2 buffers around points in points_set_1 ---#\nbuffer_1 &lt;- sf::st_buffer(points_set_1, dist = 0.2)\n\nggplot() +\n  geom_sf(data = buffer_1, color = \"red\", fill = NA) +\n  geom_sf_text(data = points_set_1, aes(label = id), color = \"red\") +\n  geom_sf_text(data = points_set_2, aes(label = id), color = \"blue\") \n\n\n\n\n\n\nFigure 3.6: The blue points within 0.2 radius of the red points\n\n\n\n\nConfirm your visual inspection results with the outcome of the following code using sf::st_is_within_distance() function.\n\nsf::st_is_within_distance(points_set_1, points_set_2, dist = 0.2)\n\nSparse geometry binary predicate list of length 5, where the predicate\nwas `is_within_distance'\n 1: 1\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 3",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#spatial-cropping",
    "href": "chapters/03-SpatialInteractionVectorVector.html#spatial-cropping",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.2 Spatial cropping",
    "text": "3.2 Spatial cropping\n\n3.2.1 Data preparation\nLet’s import the following files:\n\n#--- Kansas county borders ---#\nKS_counties &lt;-\n  readRDS(\"Data/KS_county_borders.rds\") %&gt;%\n  sf::st_transform(32614)\n\n#--- High-Plains Aquifer boundary ---#\nhpa &lt;- \n  sf::st_read(\"Data/hp_bound2010.shp\") %&gt;%\n  .[1, ] %&gt;%\n  sf::st_transform(st_crs(KS_counties))\n\n#--- all the irrigation wells in KS ---#\nKS_wells &lt;- \n  readRDS(\"Data/Kansas_wells.rds\") %&gt;%\n  sf::st_transform(st_crs(KS_counties))\n\n#--- US railroads in the Mid West region ---#\nrail_roads_mw &lt;- sf::st_read(\"Data/mw_railroads.geojson\")\n\n\n3.2.2 Bounding box\nWe can use st_crop() to crop a spatial object to a spatial bounding box (extent) of another spatial object. The bounding box of an sf is a rectangle represented by the minimum and maximum of x and y that encompass/contain all the spatial objects in the sf. You can use st_bbox() to find the bounding box of an sf object. Let’s get the bounding box of irrigation wells in Kansas (KS_wells).\n\n#--- get the bounding box of KS_wells ---#\n(\nbbox_KS_wells &lt;- sf::st_bbox(KS_wells)  \n)\n\n     xmin      ymin      xmax      ymax \n 230332.0 4095052.5  887329.7 4433596.9 \n\n#--- check the class ---#\nclass(bbox_KS_wells)\n\n[1] \"bbox\"\n\n\nAs you can see, sf::st_bbox() returns an object of class bbox. You cannot directly create a map using a bbox. So, let’s convert it to an sfc using sf::st_as_sfc():\n\nKS_wells_bbox_sfc &lt;- sf::st_as_sfc(bbox_KS_wells) \n\nFigure 3.7 shows the bounding box (red rectangle).\n\nCodeggplot() +\n  geom_sf(\n    data = KS_wells_bbox_sfc, \n    fill = NA,\n    color = \"red\",\n    linewidth = 1.2\n   ) +\n  geom_sf(\n    data = KS_wells,\n    size = 0.4\n  ) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.7: The bounding box of the irrigation wells in Kansas that overlie HPA\n\n\n\n\n\n3.2.3 Spatial cropping\nLet’s now crop High-Plains aquifer boundary (hpa) to the bounding box of Kansas wells (Figure 3.8).\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_wells, size = 0.4) +\n  geom_sf(data = KS_wells_bbox_sfc, fill = NA, color = \"red\", linewidth = 1) +\n  geom_sf(data = hpa, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.8: HPA and irrigation wells in Kansas\n\n\n\n\n\nhpa_cropped_to_KS &lt;- sf::st_crop(hpa, KS_wells)\n\nHere is what the cropped version of hpa looks like (Figure 3.9):\n\nCodeggplot() +\n  geom_sf(data = KS_wells, size = 0.4) +\n  geom_sf(data = KS_wells_bbox_sfc, color = \"red\", linewidth = 1.2, fill = NA) +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.9: The bounding box of the irrigation wells in Kansas that overlie HPA\n\n\n\n\nAs you can see, the st_crop() operation cut the parts of hpa that are not intersecting with the bounding box of KS_wells.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that we did not explicilty crop hpa to the bouding box of KS_wells (bbox_KS_wells) above. That is, we just run hpa_cropped_to_KS &lt;- sf::st_crop(hpa, KS_wells) instead of hpa_cropped_to_KS &lt;- sf::st_crop(hpa, bbox_KS_wells). When sf::st_crop() is used, it automatically finds the bounding box of the sf as second argument and implements cropping.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#sec-sf-subset",
    "href": "chapters/03-SpatialInteractionVectorVector.html#sec-sf-subset",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.3 Spatial subsetting",
    "text": "3.3 Spatial subsetting\nSpatial subsetting refers to operations that narrow down the geographic scope of a spatial object (source data) based on another spatial object (target data). We illustrate spatial subsetting using Kansas county borders (KS_counties), the boundary of the Kansas portion of the High-Plains Aquifer (hpa_cropped_to_KS), and agricultural irrigation wells in Kansas (KS_wells).\n\n3.3.1 polygons (source) vs polygons (target)\nFigure 3.10 shows the Kansas portion of the HPA and Kansas counties.\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_counties) +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.10: Kansas portion of High-Plains Aquifer and Kansas counties\n\n\n\n\nThe goal is to select only the counties that intersect with the HPA boundary. Spatial subsetting of sf objects follows this syntax:\n\n#--- this does not run ---#\nsf_1[sf_2, ]\n\nwhere you are subsetting sf_1 based on sf_2. Instead of row numbers, you provide another sf object in place. The following code spatially subsets Kansas counties based on the HPA boundary.\n\ncounties_intersecting_hpa &lt;- KS_counties[hpa_cropped_to_KS, ]\n\nFigure 3.11 presents counties in counties_intersecting_hpa and the cropped HPA boundary.\n\nCodeggplot() +\n  #--- add US counties layer ---#\n  geom_sf(data = counties_intersecting_hpa) +\n  #--- add High-Plains Aquifer layer ---#\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.11: The results of spatially subsetting Kansas counties based on HPA boundary\n\n\n\n\nYou can see that only the counties intersecting with the HPA boundary remain. This is because, when using the syntax sf_1[sf_2, ], the default topological relation is sf::st_intersects(). If objects in sf_1 intersect with any of the objects in sf_2, even slightly, they will remain after subsetting.\nSometimes, instead of removing non-intersecting observations, you may just want to flag whether two spatial objects intersect. To do this, you can first retrieve the IDs of the intersecting counties from counties_intersecting_hpa and then create a variable in the original dataset (KS_counties) that indicates whether an intersection occurs.\n\n#--- check the intersections of HPA and counties  ---#\nintersecting_counties_list &lt;- counties_intersecting_hpa$COUNTYFP\n\n#--- assign true/false ---#\nKS_counties &lt;- dplyr::mutate(KS_counties, intersects_hpa = ifelse(COUNTYFP %in% intersecting_counties_list, TRUE, FALSE))\n\n#--- take a look ---#\ndplyr::select(KS_counties, COUNTYFP, intersects_hpa)\n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   COUNTYFP intersects_hpa                       geometry\n1       133          FALSE MULTIPOLYGON (((806203.1 41...\n2       075           TRUE MULTIPOLYGON (((233615.1 42...\n3       123          FALSE MULTIPOLYGON (((544031.7 43...\n4       189           TRUE MULTIPOLYGON (((273665.9 41...\n5       155           TRUE MULTIPOLYGON (((546178.6 42...\n6       129           TRUE MULTIPOLYGON (((229431.1 41...\n7       073          FALSE MULTIPOLYGON (((717254.1 42...\n8       023           TRUE MULTIPOLYGON (((239494.1 44...\n9       089           TRUE MULTIPOLYGON (((542298.2 44...\n10      059          FALSE MULTIPOLYGON (((804792.9 42...\n\n\n\nOther topological relations\nAs mentioned above, the default topological relation used in sf_1[sf_2, ] is sf::st_intersects(). However, you can specify other topological relations using the following syntax:\n\n#--- this does not run ---#\nsf_1[sf_2, op = topological_relation_type] \n\nFor example, if you only want counties that are completely within the HPA boundary, you can use the following approach:\n\ncounties_within_hpa &lt;- KS_counties[hpa_cropped_to_KS, op = st_within]\n\nFigure 3.12 shows the resulting set of counties.\n\nCodeggplot() +\n  geom_sf(data = counties_within_hpa) +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.12: Kansas counties that are completely within the HPA boundary\n\n\n\n\n\n\n3.3.2 points (source) vs polygons (target)\nThe following map (Figure 3.13) shows the Kansas portion of the HPA and all the irrigation wells in Kansas. We can select only wells that reside within the HPA boundary using the same syntax as the above example.\n\nCodeggplot() +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  geom_sf(data = KS_wells, size = 0.1) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.13: A map of Kansas irrigation wells and HPA\n\n\n\n\n\nKS_wells_in_hpa &lt;- KS_wells[hpa_cropped_to_KS, ]\n\nAs you can see in Figure 3.14 below, only the wells that are inside (or intersect with) the HPA remained because the default topological relation is sf::st_intersects().\n\nCodeggplot() +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  geom_sf(data = KS_wells_in_hpa, size = 0.1) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.14: A map of Kansas irrigation wells and HPA\n\n\n\n\nIf you want to flag wells that intersect with HPA, use the following approach:\n\nsite_list &lt;- KS_wells_in_hpa$site\n\n(\n  KS_wells &lt;- dplyr::mutate(KS_wells, in_hpa = ifelse(site %in% site_list, TRUE, FALSE))\n)\n\nSimple feature collection with 37647 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 230332 ymin: 4095052 xmax: 887329.7 ymax: 4433597\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   site    af_used                 geometry in_hpa\n1     1 232.099948   POINT (372548 4153588)   TRUE\n2     3  13.183940 POINT (353698.6 4419755)   TRUE\n3     5  99.187052 POINT (486771.7 4260027)   TRUE\n4     7   0.000000 POINT (248127.1 4296442)   TRUE\n5     8 145.520499 POINT (349813.2 4215310)   TRUE\n6     9   3.614535 POINT (612277.6 4322077)  FALSE\n7    11 188.423543 POINT (267030.7 4381364)   TRUE\n8    12  77.335960 POINT (761774.6 4339039)  FALSE\n9    15   0.000000 POINT (560570.3 4235763)   TRUE\n10   17 167.819034 POINT (387315.1 4175007)   TRUE\n\n\n\n\n3.3.3 lines (source) vs polygons (target)\nFigure 3.15 shows the Kansas counties (in red) and U.S. railroads in the Mid West region(in blue).\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_counties, fill = NA, color = \"red\") +\n  geom_sf(data = rail_roads_mw, color = \"blue\", linewidth = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.15: U.S. railroads and Kansas county boundaries\n\n\n\n\nWe can select only the railroads that intersect with Kansas.\n\nrailroads_KS &lt;- rail_roads_mw[KS_counties, ]\n\nAs you can see in Figure 3.16 below, only the railroads that intersect with Kansas were selected. Note the lines that go beyond the Kansas boundary are also selected. Remember, the default is sf::st_intersect().\n\nCodetm_shape(railroads_KS) +\n  tm_lines(col = \"blue\") +\ntm_shape(KS_counties) +\n  tm_polygons(alpha = 0)  +\n  tm_layout(frame = FALSE) \n\n\n\n\n\n\nFigure 3.16: Railroads that intersect Kansas county boundaries\n\n\n\n\nIf you would like the lines beyond the state boundary to be cut out but the intersecting parts of those lines to remain, use sf::st_intersection(), which is explained in Section 3.6.1.\nIf you want to flag railroads that intersect with Kansas counties in railroads, use the following approach:\n\n#--- get the list of lines ---#\nlines_list &lt;- railroads_KS$LINEARID\n\n#--- railroads ---#\nrail_roads_mw &lt;- dplyr::mutate(rail_roads_mw, intersect_ks = ifelse(LINEARID %in% lines_list, TRUE, FALSE))\n\n#--- take a look ---#\ndplyr::select(rail_roads_mw, LINEARID, intersect_ks)\n\nSimple feature collection with 100661 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -414024 ymin: 3692172 xmax: 2094886 ymax: 5446589\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n      LINEARID intersect_ks                       geometry\n1  11050905483        FALSE MULTILINESTRING ((1250147 3...\n2  11050905484        FALSE MULTILINESTRING ((1250147 3...\n3  11050905485        FALSE MULTILINESTRING ((1250429 3...\n4  11050905486        FALSE MULTILINESTRING ((1248249 3...\n5  11050905487        FALSE MULTILINESTRING ((1248249 3...\n6  11050905488        FALSE MULTILINESTRING ((1249095 3...\n7  11050905489        FALSE MULTILINESTRING ((1250429 3...\n8  11050905490        FALSE MULTILINESTRING ((1250383 3...\n9  11050905492        FALSE MULTILINESTRING ((1250329 3...\n10 11050905494        FALSE MULTILINESTRING ((1250033 3...\n\n\n\n3.3.4 polygons (source) vs points (target)\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_counties, fill = NA) +\n  geom_sf(data = KS_wells_in_hpa, fill = NA, size = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.17: Kansas county boundaries and wells that overlie the HPA\n\n\n\n\nFigure 3.17 shows the Kansas counties and irrigation wells in Kansas that overlie HPA. We can select only the counties that intersect with at least one well.\n\nKS_counties_intersected &lt;- KS_counties[KS_wells_in_hpa, ]  \n\nAs you can see in Figure 3.18 below, only the counties that intersect with at least one well remained.\n\nCodeggplot() +\n  geom_sf(data = KS_counties, fill = NA) +\n  geom_sf(data = KS_counties_intersected, fill = \"blue\", alpha = 0.4) +\n  geom_sf(data = KS_wells_in_hpa, fill = NA, size = 0.3) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.18: Counties that have at least one well",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#sec-sp-join",
    "href": "chapters/03-SpatialInteractionVectorVector.html#sec-sp-join",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.4 Spatial join",
    "text": "3.4 Spatial join\nA spatial join refers to spatial operations that involve the following steps:\n\nOverlay one spatial layer (target layer) onto another (source layer).\nFor each observation in the target layer:\nIdentify which objects in the source layer it geographically intersects with (or has another specified topological relationship).\nExtract values associated with the intersecting objects in the source layer, summarizing if necessary.\nAssign the extracted values to the corresponding objects in the target layer.\n\nWe can classify spatial join into four categories by the type of the underlying spatial objects:\n\n\nvector-vector: vector data (target) against vector data (source)\n\nvector-raster: vector data (target) against raster data (source)\n\nraster-vector: raster data (target) against vector data (source)\n\nraster-raster: raster data (target) against raster data (source)\n\nAmong these four types, our focus here is on the first case, vector-vector. The second case will be discussed in Chapter 5. The third and fourth cases are not covered in this book, as the target data is almost always vector data (e.g., cities or farm fields as points, political boundaries as polygons, etc.).\nThe vector-vector category can be further divided into subcategories depending on the type of spatial object: point, line, or polygon. In this context, we will exclude spatial joins involving lines. This is because line objects are rarely used as observation units in subsequent analyses or as the source data for extracting values.4\n4 For example, in Section 1.4, we did not extract any attribute values from the railroads. Instead, we calculated the travel length of the railroads, meaning the geometry of the railroads was of interest rather than the values associated with them.Below is a list of the types of spatial joins we will cover:\n\npoints (target) against polygons (source)\npolygons (target) against points (source)\npolygons (target) against polygons (source)\n\n\n3.4.1 Syntax\nTo perform spatial join, we can use the sf::st_join() function, with the following syntax:\n\n#--- this does not run ---#\nsf::st_join(target_sf, source_sf)\n\nSimilar to spatial subsetting, the default topological relation is sf::st_intersects().\n\n3.4.2 Case 1: points (target) vs polygons (source)\nIn this case, for each observation (point) in the target data, it identifies which polygon in the source data it intersects with and then assigns the value associated with the polygon to the point.5\n5 A practical example of this case is shown in Demonstration 1 of Chapter 1.We use the Kansas irrigation well data (points) and Kansas county boundary data (polygons) for a demonstration. Our goal is to assign the county-level corn price information from the Kansas county data to wells. First let me create and add a fake county-level corn price variable to the Kansas county data.\n\nKS_corn_price &lt;-\n  KS_counties %&gt;%\n  dplyr::mutate(corn_price = seq(3.2, 3.9, length = nrow(.))) %&gt;%\n  dplyr::select(COUNTYFP, corn_price)\n\nFigure 3.19 presents Kansas counties color-differentiated by the fake corn price.\n\n\n\n\nCodeggplot(KS_corn_price) +\n  geom_sf(aes(fill = corn_price)) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.19: Map of county-level fake corn price\n\n\n\n\nFor this particular context, the following code will do the job:\n\n#--- spatial join ---#\n(\nKS_wells_County &lt;- sf::st_join(KS_wells, KS_corn_price)\n)\n\nSimple feature collection with 37647 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 230332 ymin: 4095052 xmax: 887329.7 ymax: 4433597\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   site    af_used in_hpa COUNTYFP corn_price                 geometry\n1     1 232.099948   TRUE      069   3.556731   POINT (372548 4153588)\n2     3  13.183940   TRUE      039   3.449038 POINT (353698.6 4419755)\n3     5  99.187052   TRUE      165   3.287500 POINT (486771.7 4260027)\n4     7   0.000000   TRUE      199   3.644231 POINT (248127.1 4296442)\n5     8 145.520499   TRUE      055   3.832692 POINT (349813.2 4215310)\n6     9   3.614535  FALSE      143   3.799038 POINT (612277.6 4322077)\n7    11 188.423543   TRUE      181   3.590385 POINT (267030.7 4381364)\n8    12  77.335960  FALSE      177   3.550000 POINT (761774.6 4339039)\n9    15   0.000000   TRUE      159   3.610577 POINT (560570.3 4235763)\n10   17 167.819034   TRUE      069   3.556731 POINT (387315.1 4175007)\n\n\nYou can see from Figure 3.20 that all the wells inside the same county have the same corn price value.\n\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_counties) +\n  geom_sf(data = KS_wells_County, aes(color = corn_price)) +\n  scale_color_viridis_c() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.20: Map of wells color-differentiated by corn price\n\n\n\n\nWe can also confirm that the corn_price value associated with all the wells in a single county is identical with the corn_price value associated with that county in the original corn price data (KS_corn_price).\n\nKS_wells_County %&gt;%\n  dplyr::filter(COUNTYFP == \"069\") %&gt;%\n  dplyr::pull(corn_price) %&gt;%\n  unique()\n\n[1] 3.556731\n\nKS_corn_price %&gt;%\n  dplyr::filter(COUNTYFP == \"069\") %&gt;%\n  dplyr::pull(corn_price)\n\n[1] 3.556731\n\n\n\n3.4.3 Case 2: polygons (target) vs points (source)\nIn this case, for each observation (polygon) in the target data, it identifies which observations (points) in the source data intersect with it, and then assigns the values associated with those points to the polygon (A practical example of this case is shown in Demonstration 2 of Chapter 1.).\n\n\n\n\nCodeggplot() +\n  geom_sf(data = KS_counties) +\n  geom_sf(data = KS_wells, aes(color = af_used), size = 0.2) +\n  scale_color_viridis_c(name = \"Groundwater Pumping (acre-feet)\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 3.21: Map of wells color-differentiated by corn price\n\n\n\n\nFor instance, if you are conducting a county-level analysis and want to calculate the total groundwater pumping for each county, the target data would be KS_counties, and the source data would be KS_wells (Figure 3.21).\n\n#--- spatial join ---#\nKS_County_wells &lt;- sf::st_join(KS_counties, KS_wells)\n\n#--- take a look ---#\ndplyr::select(KS_County_wells, COUNTYFP, site, af_used)\n\nSimple feature collection with 37652 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n    COUNTYFP  site   af_used                       geometry\n1        133 53861  17.01790 MULTIPOLYGON (((806203.1 41...\n1.1      133 70592   0.00000 MULTIPOLYGON (((806203.1 41...\n2        075   328 394.04513 MULTIPOLYGON (((233615.1 42...\n2.1      075   336  80.65036 MULTIPOLYGON (((233615.1 42...\n2.2      075   436 568.25359 MULTIPOLYGON (((233615.1 42...\n2.3      075  1007 215.80416 MULTIPOLYGON (((233615.1 42...\n2.4      075  1170   0.00000 MULTIPOLYGON (((233615.1 42...\n2.5      075  1192  77.39120 MULTIPOLYGON (((233615.1 42...\n2.6      075  1249   0.00000 MULTIPOLYGON (((233615.1 42...\n2.7      075  1300 320.22612 MULTIPOLYGON (((233615.1 42...\n\n\nAs seen in the resulting dataset, each unique polygon-point intersection forms an observation. For each polygon, there will be as many observations as there are wells intersecting with that polygon. After joining the two layers, you can calculate statistics for each polygon (in this case, each county). Since the goal is to calculate groundwater extraction by county, the following code will accomplish this:\n\nKS_County_wells %&gt;% \n  dplyr::group_by(COUNTYFP) %&gt;% \n  dplyr::summarize(af_used = sum(af_used, na.rm = TRUE)) \n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\n# A tibble: 105 × 3\n   COUNTYFP af_used                                                     geometry\n   &lt;fct&gt;      &lt;dbl&gt;                                           &lt;MULTIPOLYGON [m]&gt;\n 1 001           0  (((806387 4191583, 805512 4215779, 844240.9 4217265, 845100…\n 2 003           0  (((804971.3 4254894, 843634.8 4256415, 844240.9 4217265, 80…\n 3 005         771. (((794798 4394875, 814054.6 4395649, 833328.7 4396412, 8396…\n 4 007        4972. (((498886 4147060, 547336.9 4147260, 547367.8 4137622, 5575…\n 5 009       61083. (((497132.8 4283127, 544688.2 4283265, 545236.6 4281565, 54…\n 6 011           0  (((844767.7 4183342, 845100.4 4193068, 844240.9 4217265, 88…\n 7 013         480. (((774189.2 4432752, 774491.2 4432762, 812462.5 4434180, 81…\n 8 015         343. (((662406.7 4197742, 661982.8 4217157, 689364.6 4217517, 71…\n 9 017           0  (((688956.3 4246711, 690085.5 4266038, 730694.2 4267016, 73…\n10 019           0  (((719369.2 4131329, 769067.5 4132390, 770145.1 4099095, 76…\n# ℹ 95 more rows\n\n\nIt is just as easy to get other types of statistics by simply modifying the dplyr::summarize() part.\n\n3.4.4 Case 3: polygons (target) vs polygons (source)\nIn this case, sf::st_join(target_sf, source_sf) returns all the unique polygon-polygon intersections, with the attributes from source_sf attached to the intersecting polygons.\nWe will use county-level corn acres data in Iowa for 2018 from USDA NASS6 and Hydrologic Units. Our objective is to estimate corn acres by HUC units based on county-level corn acres data.7\n6 See Section 8.1 for instructions on downloading Quick Stats data from within R.7 There will be substantial measurement errors because the source polygons (corn acres by county) are large compared to the target polygons (HUC units). However, this serves as a useful illustration of a polygon-polygon join.We first import the Iowa corn acre data (Figure 3.22 is the map of Iowa counties color-differentiated by corn acres):\n\n#--- IA boundary ---#\nIA_corn &lt;- readRDS(\"Data/IA_corn.rds\")\n\n#--- take a look ---#\nIA_corn\n\nSimple feature collection with 93 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n   county_code year  acres                       geometry\n1          083 2018 183500 MULTIPOLYGON (((458997 4711...\n2          141 2018 167000 MULTIPOLYGON (((267700.8 47...\n3          081 2018 184500 MULTIPOLYGON (((421231.2 47...\n4          019 2018 189500 MULTIPOLYGON (((575285.6 47...\n5          023 2018 165500 MULTIPOLYGON (((497947.5 47...\n6          195 2018 111500 MULTIPOLYGON (((459791.6 48...\n7          063 2018 110500 MULTIPOLYGON (((345214.3 48...\n8          027 2018 183000 MULTIPOLYGON (((327408.5 46...\n9          121 2018  70000 MULTIPOLYGON (((396378.1 45...\n10         077 2018 107000 MULTIPOLYGON (((355180.1 46...\n\n\n\n\n\n\nCodeggplot(IA_corn) +\n  geom_sf(aes(fill = acres)) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.22: Map of Iowa counties color-differentiated by corn planted acreage\n\n\n\n\nNow import the HUC units data (Figure 3.23 presents the map of HUC units):\n\n#--- import HUC units ---#\nHUC_IA &lt;- \n  sf::st_read(\"Data/huc250k.shp\") %&gt;% \n  dplyr::select(HUC_CODE) %&gt;% \n  #--- reproject to the CRS of IA ---#\n  sf::st_transform(st_crs(IA_corn)) %&gt;% \n  #--- select HUC units that overlaps with IA ---#\n  .[IA_corn, ]\n\n\n\n\n\nCodeggplot(HUC_IA) + \n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.23: Map of HUC units that intersect with Iowa state boundary\n\n\n\n\nFigure 3.24 presents a map of Iowa counties superimposed on the HUC units:\n\nCodeggplot() +\n  geom_sf(data = HUC_IA) +\n  geom_sf(data = IA_corn, aes(fill = acres), alpha = 0.4) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.24: Map of HUC units superimposed on the counties in Iowas\n\n\n\n\nSpatial join using sf::st_join() will produce the following:\n\n(\nHUC_joined &lt;- sf::st_join(HUC_IA, IA_corn)\n)\n\nSimple feature collection with 349 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 154941.7 ymin: 4346327 xmax: 773299.8 ymax: 4907735\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n      HUC_CODE county_code year  acres                       geometry\n608   10170203         149 2018 226500 POLYGON ((235551.4 4907513,...\n608.1 10170203         167 2018 249000 POLYGON ((235551.4 4907513,...\n608.2 10170203         193 2018 201000 POLYGON ((235551.4 4907513,...\n608.3 10170203         119 2018 184500 POLYGON ((235551.4 4907513,...\n621   07020009         063 2018 110500 POLYGON ((408580.7 4880798,...\n621.1 07020009         109 2018 304000 POLYGON ((408580.7 4880798,...\n621.2 07020009         189 2018 120000 POLYGON ((408580.7 4880798,...\n627   10170204         141 2018 167000 POLYGON ((248115.2 4891652,...\n627.1 10170204         143 2018 116000 POLYGON ((248115.2 4891652,...\n627.2 10170204         167 2018 249000 POLYGON ((248115.2 4891652,...\n\n\nEach intersecting HUC-county combination becomes a separate observation, with the resulting geometry being the same as that of the HUC unit. To illustrate this, let’s take a closer look at one of the HUC units. The HUC unit with HUC_CODE ==10170203 intersects with four County.\n\n#--- get the HUC unit with `HUC_CODE ==10170203`  ---#\n(\ntemp_HUC_county &lt;- filter(HUC_joined, HUC_CODE == 10170203)\n)\n\nSimple feature collection with 4 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 154941.7 ymin: 4709628 xmax: 248115.2 ymax: 4907735\nProjected CRS: NAD83 / UTM zone 15N\n      HUC_CODE county_code year  acres                       geometry\n608   10170203         149 2018 226500 POLYGON ((235551.4 4907513,...\n608.1 10170203         167 2018 249000 POLYGON ((235551.4 4907513,...\n608.2 10170203         193 2018 201000 POLYGON ((235551.4 4907513,...\n608.3 10170203         119 2018 184500 POLYGON ((235551.4 4907513,...\n\n\nAll of the four observations in temp_HUC_county has the identical geometry (Figure 3.25 shows this visually.).\n\nlength(unique(temp_HUC_county$geometry))\n\n[1] 1\n\n\n\n\n\n\nCodetemp_HUC_county %&gt;%\n  dplyr::mutate(county_text = paste0(\"County Code: \", county_code)) %&gt;%\n  ggplot(.) +\n  geom_sf() +\n  facet_wrap(county_text ~ ., nrow = 2) +\n  theme_void()\n\n\n\n\n\n\nFigure 3.25: Geoemetry of the four observations in the joined object\n\n\n\n\nMoreover, they are identical with the geometry of the intersecting HUC unit (the HUC unit with HUC_CODE ==10170203):\n\nidentical(\n  #--- geometry of the one of the observations from the joined object ---#\n  temp_HUC_county[1, ]$geometry,\n  #--- original HUC geometry ---#\n  dplyr::filter(HUC_IA, HUC_CODE == 10170203) %&gt;% pull(geometry)\n)\n\n[1] TRUE\n\n\nSo, all four observations have identical geometry, which corresponds to the geometry of the HUC unit. This indicates that sf::st_join() does not retain information about the extent of the intersection between the HUC unit and the four counties. Keep in mind that the default behavior uses sf::st_intersects(), which only checks whether the spatial objects intersect, without providing further details.\nIf you are calculating a simple average of corn acres and are not concerned with the degree of spatial overlap, this method works just fine. However, if you want to compute an area-weighted average, the information provided is insufficient. You will learn how to calculate the area-weighted average in the next subsection.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#spatial-join-and-summary-in-one-step",
    "href": "chapters/03-SpatialInteractionVectorVector.html#spatial-join-and-summary-in-one-step",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.5 Spatial join and summary in one step",
    "text": "3.5 Spatial join and summary in one step\nIn Section 3.4.3, we first joined KS_counties and KS_wells uusing sf::st_join() and then applied dplyr::summarize() to find total groundwater use per county. This two-step procedure can be done in one step using aggregate() with the summarizing function specified for the FUN option.\n\naggregate(source layer, target layer, FUN = function)\n\nSince we are trying to find total groundwater use for each county, the target layer is KS_counties, the source layer is KS_wells, which has af_used, and the function to summarize is sum(). So, the following code will replicate what we just did above:\n\n#--- sum ---#\naggregate(KS_wells, KS_counties, FUN = sum)\n\nSimple feature collection with 105 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n       site      af_used in_hpa                       geometry\n1    124453 1.701790e+01      0 MULTIPOLYGON (((806203.1 41...\n2  12514550 6.261704e+04    159 MULTIPOLYGON (((233615.1 42...\n3   1964254 1.737791e+03      0 MULTIPOLYGON (((544031.7 43...\n4  42442400 3.023589e+05   1056 MULTIPOLYGON (((273665.9 41...\n5  68068173 6.110576e+04   1294 MULTIPOLYGON (((546178.6 42...\n6  15756801 9.664610e+04    477 MULTIPOLYGON (((229431.1 41...\n7    149202 0.000000e+00      0 MULTIPOLYGON (((717254.1 42...\n8  17167377 5.750807e+04    592 MULTIPOLYGON (((239494.1 44...\n9   1809003 2.201696e+03     15 MULTIPOLYGON (((542298.2 44...\n10   160064 4.571102e+00      0 MULTIPOLYGON (((804792.9 42...\n\n\nNotice that the sum() function was applied to all the columns in KS_wells, including site id number. So, you might want to select variables you want to join before you apply the aggregate() function like this:\n\naggregate(dplyr::select(KS_wells, af_used), KS_counties, FUN = sum)\n\nSimple feature collection with 105 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n        af_used                       geometry\n1  1.701790e+01 MULTIPOLYGON (((806203.1 41...\n2  6.261704e+04 MULTIPOLYGON (((233615.1 42...\n3  1.737791e+03 MULTIPOLYGON (((544031.7 43...\n4  3.023589e+05 MULTIPOLYGON (((273665.9 41...\n5  6.110576e+04 MULTIPOLYGON (((546178.6 42...\n6  9.664610e+04 MULTIPOLYGON (((229431.1 41...\n7  0.000000e+00 MULTIPOLYGON (((717254.1 42...\n8  5.750807e+04 MULTIPOLYGON (((239494.1 44...\n9  2.201696e+03 MULTIPOLYGON (((542298.2 44...\n10 4.571102e+00 MULTIPOLYGON (((804792.9 42...",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#spatial-intersection-cropping-join",
    "href": "chapters/03-SpatialInteractionVectorVector.html#spatial-intersection-cropping-join",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.6 Spatial intersection (cropping join)",
    "text": "3.6 Spatial intersection (cropping join)\nSometimes, you may need to crop spatial objects by polygon boundaries. For instance, in the demonstration in Section 1.4, we calculated the total length of railroads within each county by trimming the parts of the railroads that extended beyond county boundaries. Similarly, we cannot find area-weighted averages using sf::st_join() alone because it does not provide information about how much of each HUC unit intersects with a county. However, if we can obtain the geometry of the intersecting portions of the HUC unit and the county, we can calculate their areas, allowing us to compute area-weighted averages of the joined attributes.\nFor these purposes, we can use sf::st_intersection(). Below, we illustrate how sf::st_intersection() works for line-polygon and polygon-polygon intersections (using data generated in Section 3.1). Intersections involving points with sf::st_intersection() are equivalent to using sf::st_join() since points have neither length nor area (nothing to crop). Therefore, they are not discussed here.\n\n3.6.1 sf::st_intersection()\n\nWhile sf::st_intersects() returns the indices of intersecting objects, sf::st_intersection() returns the actual intersecting spatial objects, with the non-intersecting parts of the sf objects removed. Additionally, the attribute values from the source sf are merged with the intersecting geometries (sfg) in the target sf.\n\n\n\n\nCodeggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\") \n\n\n\n\n\n\nFigure 3.26: Visualization of the points, lines, and polygons\n\n\n\n\nWe will now explore how this works for both line-polygon and polygon-polygon intersections, using the same toy examples we used to demonstrate how sf::st_intersects() functions (Figure 3.26 shows the lines and polygons).\n\nlines and polygons\nThe following code finds the intersection of the lines and the polygons.\n\n(\nintersections_lp &lt;- \n  sf::st_intersection(lines, polygons) %&gt;% \n  dplyr::mutate(int_name = paste0(line_name, \"-\", polygon_name))\n)\n\nSimple feature collection with 3 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 2\nCRS:           NA\n    line_name polygon_name                              x         int_name\n1      line 1    polygon 1        LINESTRING (0 0, 2 0.4) line 1-polygon 1\n2      line 2    polygon 1   LINESTRING (1.5 0.5, 2 1.25) line 2-polygon 1\n2.1    line 2    polygon 2 LINESTRING (2.166667 1.5, 2... line 2-polygon 2\n\n\nAs shown in the output, each intersection between the lines and polygons becomes an observation (e.g., line 1-polygon 1, line 2-polygon 1, and line 2-polygon 2). Any part of the lines that does not intersect with a polygon is removed and does not appear in the returned sf object. To confirm this, see Figure 3.27 below:\n\nCodeggplot() +\n  #--- here are all the original polygons  ---#\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  #--- here is what is returned after st_intersection ---#\n  geom_sf(data = intersections_lp, aes(color = int_name), size = 1.5)\n\n\n\n\n\n\nFigure 3.27: The outcome of the intersections of the lines and polygons\n\n\n\n\nThis approach also allows us to calculate the length of the line segments that are fully contained within the polygons, similar to what we did in Section 1.4. Additionally, the attributes (e.g., polygon_name) from the source sf (the polygons) are merged with their intersecting lines. As a result, sf::st_intersection() not only transforms the original geometries but also joins the attributes, which is why I refer to this as a “cropping join.”\n\npolygons and polygons\nThe following code finds the intersection of polygon 1 and polygon 3 with polygon 2.\n\n(\nintersections_pp &lt;- \n  sf::st_intersection(polygons[c(1,3), ], polygons[2, ]) %&gt;% \n  dplyr::mutate(int_name = paste0(polygon_name, \"-\", polygon_name.1))\n)\n\nSimple feature collection with 2 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0.5 ymin: 1.5 xmax: 2.3 ymax: 3.2\nCRS:           NA\n  polygon_name polygon_name.1                              x\n1    polygon 1      polygon 2 POLYGON ((2 2, 2 1.5, 0.5 1...\n3    polygon 3      polygon 2 POLYGON ((0.5 3.2, 2.3 3.2,...\n             int_name\n1 polygon 1-polygon 2\n3 polygon 3-polygon 2\n\n\nAs shown in Figure 3.28, each intersection between polygons 1 and 3 with polygon 2 becomes a separate observation (e.g., polygon 1-polygon 2 and polygon 3-polygon 2). Similar to the lines-polygons case, the non-intersecting parts of polygons 1 and 3 are removed and do not appear in the returned sf. Later, we will explore how sf::st_intersection() can be used to calculate area-weighted values from the intersecting polygons with the help of sf::st_area().\n\nCodeggplot() +\n  #--- here are all the original polygons  ---#\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  #--- here is what is returned after st_intersection ---#\n  geom_sf(data = intersections_pp, aes(fill = int_name))\n\n\n\n\n\n\nFigure 3.28: The outcome of the intersections of polygon 2 and polygons 1 and 3\n\n\n\n\n\n3.6.2 Area-weighted average\nLet’s now return to the example of HUC units and county-level corn acres data from Section 3.4. This time, we aim to calculate the area-weighted average of corn acres, rather than just the simple average.\nUsing sf::st_intersection(), we can divide each HUC polygon into parts based on the boundaries of the intersecting counties. For each HUC polygon, the intersecting counties are identified, and the polygon is split according to these boundaries.\n\n(\nHUC_intersections &lt;- \n  sf::st_intersection(HUC_IA, IA_corn) %&gt;% \n  dplyr::mutate(huc_county = paste0(HUC_CODE, \"-\", county_code))\n)\n\nSimple feature collection with 349 features and 5 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\nFirst 10 features:\n      HUC_CODE county_code year  acres                       geometry\n732   07080207         083 2018 183500 POLYGON ((482923.9 4711643,...\n803   07080205         083 2018 183500 POLYGON ((499725.6 4696873,...\n826   07080105         083 2018 183500 POLYGON ((461853.2 4682925,...\n627   10170204         141 2018 167000 POLYGON ((269364.9 4793311,...\n687   10230003         141 2018 167000 POLYGON ((271504.3 4754685,...\n731   10230002         141 2018 167000 POLYGON ((267684.6 4790972,...\n683   07100003         081 2018 184500 POLYGON ((435951.2 4789348,...\n686   07080203         081 2018 184500 MULTIPOLYGON (((459303.2 47...\n732.1 07080207         081 2018 184500 POLYGON ((429573.1 4779788,...\n747   07100005         081 2018 184500 POLYGON ((421044.8 4772268,...\n        huc_county\n732   07080207-083\n803   07080205-083\n826   07080105-083\n627   10170204-141\n687   10230003-141\n731   10230002-141\n683   07100003-081\n686   07080203-081\n732.1 07080207-081\n747   07100005-081\n\n\nThe key difference from the sf::st_join() example is that each observation in the returned data represents a unique HUC-county intersection. Figure 3.29 below shows a map of all the intersections between the HUC unit with HUC_CODE == 10170203 and its four intersecting counties.\n\nCodeggplot() +\n  geom_sf(\n    data = dplyr::filter(\n      HUC_intersections,\n      HUC_CODE == \"10170203\"\n    ),\n    aes(fill = huc_county)\n  ) +\n  scale_fill_viridis_d() +\n  theme_void()\n\n\n\n\n\n\nFigure 3.29: Intersections of a HUC unit and Iowa counties\n\n\n\n\nNote that the attributes from the county data, such as acres, are joined to the resulting intersections, as seen in the output above. As mentioned earlier, sf::st_intersection() performs a spetial type of join where the resulting observations are the intersections between the target and source sf objects.\nTo calculate the area-weighted average of corn acres, you can first use sf::st_area() to determine the area of each intersection. Then, compute the area-weighted average as follows:\n\n(\nHUC_aw_acres &lt;- \n  HUC_intersections %&gt;% \n  #--- get area ---#\n  dplyr::mutate(area = as.numeric(st_area(.))) %&gt;% \n  #--- get area-weight by HUC unit ---#\n  dplyr::group_by(HUC_CODE) %&gt;% \n  dplyr::mutate(weight = area / sum(area)) %&gt;% \n  #--- calculate area-weighted corn acreage by HUC unit ---#\n  dplyr::summarize(aw_acres = sum(weight * acres))\n)\n\nSimple feature collection with 55 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687\nProjected CRS: NAD83 / UTM zone 15N\n# A tibble: 55 × 3\n   HUC_CODE aw_acres                                                    geometry\n   &lt;chr&gt;       &lt;dbl&gt;                                              &lt;GEOMETRY [m]&gt;\n 1 07020009  251185. POLYGON ((421060.3 4797550, 420940.3 4797420, 420860.3 479…\n 2 07040008  165000  POLYGON ((602922.4 4817166, 602862.4 4816996, 602772.4 481…\n 3 07060001  105234. MULTIPOLYGON (((649333.8 4761761, 648933.7 4761621, 648793…\n 4 07060002  140201. MULTIPOLYGON (((593780.7 4816946, 594000.7 4816865, 594380…\n 5 07060003  149000  MULTIPOLYGON (((692011.6 4712966, 691981.6 4712956, 691881…\n 6 07060004  162123. POLYGON ((652046.5 4718813, 651916.4 4718783, 651786.4 471…\n 7 07060005  142428. POLYGON ((734140.4 4642126, 733620.4 4641926, 733520.3 464…\n 8 07060006  159635. POLYGON ((673976.9 4651911, 673950.7 4651910, 673887.8 465…\n 9 07080101  115572. POLYGON ((666760.8 4558401, 666630.9 4558362, 666510.8 455…\n10 07080102  160008. POLYGON ((576151.1 4706650, 575891 4706810, 575741 4706890…\n# ℹ 45 more rows",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/03-SpatialInteractionVectorVector.html#sec-exercises-int-vv",
    "href": "chapters/03-SpatialInteractionVectorVector.html#sec-exercises-int-vv",
    "title": "3  Spatial Interactions of Vector Data: Subsetting and Joining",
    "section": "\n3.7 Exercises",
    "text": "3.7 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nBefore working on exercises, please run the following codes by hitting the Run Code button.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nTips for working with an webr session\n\n\n\n\nHit the Run Code button to run all the codes in the code area.\nTo run only parts of the code, highlight the section you want to execute with your cursor and then press Cmd + Return (Mac) or Ctrl + Enter (Windows).\nAll the code blocks are interconnected and R objects defined in one R code block is available in another R code block.\n\n\n\n\n3.7.1 Exercise 1: Spatially intersecting objects\nFind the points in mower_sensor_sf that are intersecting with any of the polygons in fairway_grid.8\n8 Hint: see Section 3.3\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodeintersecting_points &lt;- mower_sensor_sf[fairway_grid, ]\n\n\n\n\n\n\n3.7.2 Exercise 2: Spatial subset and cropping\nRun the following codes first and get the sense of their spatial positions:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nProblem 1\nProblem 2\nProblem 3\n\n\n\nFind the counties in Colorado that are intersecting with the High-Plains aquifer (hp_boundary) and name the resulting R object subset_co_counties. If you know how to create a map from sf objects using ggplot2 (Chapter 7), then create a map of the intersecting Colorado counties.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodesubset_co_counties &lt;- co_counties[hp_boundary, ]\n\nggplot(subset_co_counties) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\n\nCrop the Colorado counties to hpa_boundary and name it cropped_co_counties If you know how to create a map from sf objects using ggplot2 (Chapter 7), then create a map of the cropped Colorado counties (with aes(fill = \"blue, alpha = 0.3)) on top of all the Colorado counties.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodecropped_co_counties &lt;- sf::st_crop(co_counties, hp_boundary)\n\nggplot() +\n  geom_sf(data = co_counties) +\n  geom_sf(data = cropped_co_counties, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\n\nFind the area of counties in co_counties (name the column area) and cropped_co_counties (name the column area_cropped). Then, join them together using left_join and filter the ones that satisfy area_cropped &lt; area to find out which counties were chopped off.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodesubset_co_area &lt;- \n  co_counties %&gt;%\n  dplyr::mutate(area = sf::st_area(geometry) %&gt;% as.numeric()) %&gt;%\n  #--- make it non-sf so it can be merged with left_join later  ---#\n  sf::st_drop_geometry() %&gt;%\n  dplyr::select(area, COUNTYFP)\n\ncropped_co_area &lt;- \n  cropped_co_counties %&gt;%\n  dplyr::mutate(area_cro = sf::st_area(geometry) %&gt;% as.numeric()) %&gt;%\n  #--- make it non-sf so it can be merged with left_join later  ---#\n  sf::st_drop_geometry() %&gt;%\n  dplyr::select(area_cro, COUNTYFP)\n\ncut_off_counties &lt;- \n  dplyr::left_join(subset_co_area, cropped_co_area, by = \"COUNTYFP\") %&gt;%\n  dplyr::filter(area &lt; area_cro)\n\n\n\n\n\n\n\n\n\n\n3.7.3 Exercise 3: Spatial join\nIn this exercise, we will use Nebraska county boundaries (ne_counties) and irrigation wells in Nebraska (wells_ne_sf).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nProblem 1\nProblem 2\n\n\n\nSpatially join ne_counties with wells_ne_sf and then find the total amount of groundwater extracted (gw_extracted) per county.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodesf::st_join(ne_counties, wells_ne_sf) %&gt;%\n  #--- geometry no longer neded (makes summary faster later) ---#\n  st_drop_geometry() %&gt;%\n  dplyr::group_by(countyfp) %&gt;%\n  dplyr::summarize(tot_gw = sum(gw_extracted))\n\n\n\n\n\n\n\nUse aggregate() instead to do the process implemented in Problem 1 in one step.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodeaggregate(wells_ne_sf, ne_counties, FUN = sum)\n\n\n\n\n\n\n\n\n\n\n3.7.4 Exercise 3: Spatial join\nIn this exercise, we use soybean yield (soy_yield) and as-applied seed rate (as_applied_s_rate) within a production field.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor each of the yield points, find all the seed rate points within 10 meter. Then, find the average of the neighboring seed rates by yield point.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodesoy_seed &lt;-\n  st_join(\n    soy_yield,\n    as_applied_s_rate,\n    join = \\(x, y) st_is_within_distance(x, y, dist = 10)\n  )\n\nsoy_seed %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(yield_id) %&gt;%\n  summarize(avg_seed_rate = mean(seed_rate))\n\n\n\n\n\n\n3.7.5 Exercise 4: Spatial intersection\nIn this exercise, you will work with Colorado county borders co_counties and the High-Plains aquifer boundary (hp_boundary).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn your analysis, you want to retain only the counties that have at least 80% of their area overlapping with the aquifer. First, project them to WGS84 UTM Zone 13 (EPSG: 32613) using sf::st_transform(), identify these counties using st_intersection(), and then create a variable in co_counties that indicates whether the 80% overlapping condition is satisfied or not.\n\n\nWork here\nAnswer\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCodeco_counties_utm13 &lt;- st_transform(co_counties, 32613)\nhp_boundary_utm13 &lt;- st_transform(hp_boundary, 32613)\n\nco_intersecting &lt;-\n  sf::st_intersection(co_counties_utm13, hp_boundary_utm13) %&gt;%\n  dplyr::mutate(area_int = st_area(geometry) %&gt;% as.numeric()) %&gt;%\n  sf::st_drop_geometry()\n\nco_counties &lt;- \n  co_counties %&gt;%\n  dplyr::mutate(area = st_area(geometry) %&gt;% as.numeric()) %&gt;%\n  left_join(., co_intersecting, by = \"name\") %&gt;%\n  dplyr::mutate(more_than_80 = ifelse(area_int &gt; area * 0.8, TRUE, FALSE))",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spatial Interactions of Vector Data: Subsetting and Joining</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html",
    "href": "chapters/04-RasterDataBasics.html",
    "title": "4  Raster Data Handling",
    "section": "",
    "text": "Before you start\nIn this chapter, we will explore how to handle raster data using the raster and terra packages. The raster package has long been the standard for raster data handling, but the terra package has now superseded the raster package. terra typically offers faster performance for many raster operations compared to raster. However, we will still cover raster object classes and how to convert between raster and terra objects. This is because many of the existing spatial packages still rely on raster object classes and have not yet transitioned to terra. Both packages share many function names, and key differences will be clarified as we proceed.\nFor many scientists, one of the most common and time-consuming raster data task is extracting values for vector data. As such, we will focus on the essential knowledge needed for this process, which will be thoroughly discussed in Chapter 5.\nFinally, if you frequently work with raster data that includes temporal dimensions (e.g., PRISM, Daymet), you may find the stars package useful (covered in Chapter 6). It offers a data model tailored for time-based raster data and allows the use of dplyr verbs for data manipulation.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#before-you-start",
    "href": "chapters/04-RasterDataBasics.html#before-you-start",
    "title": "4  Raster Data Handling",
    "section": "",
    "text": "Direction for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nPackages\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  terra, # handle raster data\n  raster, # handle raster data\n  mapview, # create interactive maps\n  dplyr, # data wrangling\n  sf, # vector data handling\n  lubridate # date handling\n)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#raster-data-object-classes",
    "href": "chapters/04-RasterDataBasics.html#raster-data-object-classes",
    "title": "4  Raster Data Handling",
    "section": "\n4.1 Raster data object classes",
    "text": "4.1 Raster data object classes\n\n4.1.1 raster package: RasterLayer, RasterStack, and RasterBrick\n\nLet’s start with taking a look at raster data. We will use the CDL data for Iowa in 2015. We can use raster::raster() to read a raster data file.\n\n(\n  IA_cdl_2015 &lt;- raster::raster(\"Data/IA_cdl_2015.tif\")\n)\n\nclass      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : Layer_1 \nvalues     : 0, 229  (min, max)\n\n\nEvaluating an imported raster object provides key information about the raster data, such as its dimensions (number of cells, rows, and columns), spatial resolution (e.g., 30 meters by 30 meters for this dataset), extent, coordinate reference system (CRS), and the minimum and maximum values recorded. The downloaded data is of the class RasterLayer, which is defined by the raster package. A RasterLayer contains only one layer, meaning that the raster cells hold the value of a single variable (in this case, the land use category code as an integer)\n\nYou can stack multiple raster layers of the same spatial resolution and extent to create a RasterStack using raster::stack() or RasterBrick using raster::brick(). Often times, processing a multi-layer object has computational advantages over processing multiple single-layer one by one1.\n1 You will see this in Chapter 5 where we learn how to extract values from a raster layer for a vector data.To create a RasterStack and RasterBrick, let’s load the CDL data for IA in 2016 and stack it with the 2015 data.\n\nIA_cdl_2016 &lt;- raster::raster(\"Data/IA_cdl_2016.tif\")\n\n#--- stack the two ---#\n(\n  IA_cdl_stack &lt;- raster::stack(IA_cdl_2015, IA_cdl_2016)\n)\n\nclass      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nnames      : Layer_1, IA_cdl_2016 \nmin values :       0,           0 \nmax values :     229,         241 \n\n\nIA_cdl_stack is of class RasterStack, and it has two layers of variables: CDL for 2015 and 2016. You can make it a RasterBrick using raster::brick():\n\n#--- stack the two ---#\nIA_cdl_brick &lt;- brick(IA_cdl_stack)\n\n#--- or this works as well ---#\n# IA_cdl_brick &lt;- brick(IA_cdl_2015, IA_cdl_2016)\n\n#--- take a look ---#\nIA_cdl_brick\n\n\n\nclass      : RasterBrick \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : Error in if (is.na(x)) { : argument is of length zero\n \nsource     : r_tmp_2023-03-07_111005_60204_86270.grd \nnames      : IA_cdl_2015, IA_cdl_2016 \nmin values :           0,           0 \nmax values :         229,         241 \n\n\nYou probably noticed that it took some time to create the RasterBrick object2. While spatial operations on RasterBrick are supposedly faster than RasterStack, the time to create a RasterBrick object itself is often long enough to kill the speed advantage entirely. Often, the three raster object types are collectively referred to as Raster\\(^*\\) objects for shorthand in the documentation of the raster and other related packages.\n2 Read here for the difference between RasterStack and RasterBrick\n4.1.2 terra package: SpatRaster\n\nterra package has only one object class for raster data, SpatRaster and no distinctions between one-layer and multi-layer rasters is necessary. Let’s first convert a RasterLayer to a SpatRaster using terra::rast() function.\n\n#--- convert to a SpatRaster ---#\nIA_cdl_2015_sr &lt;- terra::rast(IA_cdl_2015)\n\n#--- take a look ---#\nIA_cdl_2015_sr\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : IA_cdl_2015.tif \ncolor table : 1 \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n\n\nYou can see that the number of layers (nlyr in dimensions) is \\(1\\) because the original object is a RasterLayer, which by definition has only one layer. Now, let’s convert a RasterStack to a SpatRaster using terra::rast().\n\n#--- convert to a SpatRaster ---#\nIA_cdl_stack_sr &lt;- terra::rast(IA_cdl_stack)\n\n#--- take a look ---#\nIA_cdl_stack_sr\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \ncolor table : 1, 2 \nnames       : Layer_1, IA_cdl_2016 \nmin values  :       0,           0 \nmax values  :     229,         241 \n\n\nAgain, it is a SpatRaster, and you now see that the number of layers is 2. We just confirmed that terra has only one class for raster data whether it is single-layer or multiple-layer ones.\nIn order to make multi-layer SpatRaster from multiple single-layer SpatRaster you can just use c() like below:\n\n#--- create a single-layer SpatRaster ---#\nIA_cdl_2016_sr &lt;- terra::rast(IA_cdl_2016)\n\n#--- concatenate ---#\n(\n  IA_cdl_ml_sr &lt;- c(IA_cdl_2015_sr, IA_cdl_2016_sr)\n)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \ncolor table : 1, 2 \nnames       : Layer_1, IA_cdl_2016 \nmin values  :       0,           0 \nmax values  :     229,         241 \n\n\n\n4.1.3 Converting a SpatRaster object to a Raster\\(^*\\) object.\nYou can convert a SpatRaster object to a Raster\\(^*\\) object using raster::raster(), raster::stack(), and raster::brick(). Keep in mind that if you use raster::rater() even though SpatRaster has multiple layers, the resulting RasterLayer object has only the first of the multiple layers.\n\n#--- RasterLayer (only 1st layer) ---#\nIA_cdl_stack_sr %&gt;% raster::raster()\n\nclass      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : Layer_1 \nvalues     : 0, 229  (min, max)\n\n#--- RasterLayer ---#\nIA_cdl_stack_sr %&gt;% raster::stack()\n\nclass      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nnames      : Layer_1, IA_cdl_2016 \nmin values :       0,           0 \nmax values :     229,         241 \n\n#--- RasterLayer (this takes some time) ---#\nIA_cdl_stack_sr %&gt;% raster::brick()\n\nclass      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nnames      : Layer_1, IA_cdl_2016 \nmin values :       0,           0 \nmax values :     229,         241 \n\n\nInstead of these functions, you can simply use as(SpatRast, \"Raster\") like below:\n\nas(IA_cdl_stack_sr, \"Raster\")\n\nclass      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nnames      : Layer_1, IA_cdl_2016 \nmin values :       0,           0 \nmax values :     229,         241 \n\n\nThis works for any Raster\\(^*\\) object and you do not have to pick the right function like above.\n\n4.1.4 Vector data in the terra package\nterra package has its own class for vector data, called SpatVector. While we do not use any of the vector data functionality provided by the terra package, we learn how to convert an sf object to SpatVector because some of the terra functions do not support sf as of now (this will likely be resolved very soon). We will see some use cases of this conversion in Chapter 5 when we learn raster value extractions for vector data using terra::extract().\nAs an example, let’s use Illinois county border data.\n\n#--- Illinois county boundary ---#\n(\n  IL_county &lt;- \n    tigris::counties(\n      state = \"Illinois\", \n      progress_bar = FALSE\n    ) %&gt;%\n    dplyr::select(STATEFP, COUNTYFP)\n)\n\nSimple feature collection with 102 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.01994 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n    STATEFP COUNTYFP                       geometry\n86       17      067 MULTIPOLYGON (((-90.90609 4...\n92       17      025 MULTIPOLYGON (((-88.69516 3...\n131      17      185 MULTIPOLYGON (((-87.89243 3...\n148      17      113 MULTIPOLYGON (((-88.91954 4...\n158      17      005 MULTIPOLYGON (((-89.37207 3...\n159      17      009 MULTIPOLYGON (((-90.53674 3...\n213      17      083 MULTIPOLYGON (((-90.1459 39...\n254      17      147 MULTIPOLYGON (((-88.46335 4...\n266      17      151 MULTIPOLYGON (((-88.48289 3...\n303      17      011 MULTIPOLYGON (((-89.16654 4...\n\n\nYou can convert an sf object to SpatVector object using terra::vect().\n\n(\n  IL_county_sv &lt;- terra::vect(IL_county)\n)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 102, 2  (geometries, attributes)\n extent      : -91.51308, -87.01994, 36.9703, 42.50848  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat NAD83 (EPSG:4269) \n names       : STATEFP COUNTYFP\n type        :   &lt;chr&gt;    &lt;chr&gt;\n values      :      17      067\n                    17      025\n                    17      185",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#read-and-write-a-raster-data-file",
    "href": "chapters/04-RasterDataBasics.html#read-and-write-a-raster-data-file",
    "title": "4  Raster Data Handling",
    "section": "\n4.2 Read and write a raster data file",
    "text": "4.2 Read and write a raster data file\nRaster data files can come in numerous different formats. For example, PRPISM comes in the Band Interleaved by Line (BIL) format, some of the Daymet data comes in netCDF format. Other popular formats include GeoTiff, SAGA, ENVI, and many others.\n\n4.2.1 Read raster file(s)\nYou can use terra::rast() to read raster data in many common formats, and in most cases, this function will work for your raster data. In this example, we read a GeoTIFF file (with a .tif extension).\n\n(\n  IA_cdl_2015_sr &lt;- terra::rast(\"Data/IA_cdl_2015.tif\")\n)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : IA_cdl_2015.tif \ncolor table : 1 \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n\n\nYou can read multiple single-layer raster datasets of the same spatial extent and resolution at the same time to have a multi-layer SpatRaster object. Here, we import two single-layer raster datasets (IA_cdl_2015.tif and IA_cdl_2016.tif) to create a two-layer SpatRaster object.\n\n#--- the list of path to the files ---#\nfiles_list &lt;- c(\"Data/IA_cdl_2015.tif\", \"Data/IA_cdl_2016.tif\")\n\n#--- read the two at the same time ---#\n(\n  multi_layer_sr &lt;- terra::rast(files_list)\n)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_cdl_2016.tif  \ncolor table : 1, 2 \nnames       : Layer_1, IA_cdl_2016 \nmin values  :       0,           0 \nmax values  :     229,         241 \n\n\nOf course, this only works because the two datasets have the identical spatial extent and resolution. There are, however, no restrictions on what variable each of the raster layers represent. For example, you can combine PRISM temperature and precipitation raster layers if you want.\n\n4.2.2 Write raster files\nYou can write a SpatRaster object using terra::writeRaster().\n\nterra::writeRaster(IA_cdl_2015_sr, \"Data/IA_cdl_stack.tif\", filetype = \"GTiff\", overwrite = TRUE)\n\nThe above code saves IA_cdl_2015_sr (a SpatRaster object) as a GeoTiff file.3 The filetype option can be dropped as writeRaster() infers the filetype from the extension of the file name. The overwrite = TRUE option is necessary if a file with the same name already exists and you are overwriting it. This is one of the many areas terra is better than raster. raster::writeRaster() can be frustratingly slow for a large Raster\\(^*\\) object. terra::writeRaster() is much faster.\n3 There are many other alternative formats (see here)You can also save a multi-layer SpatRaster object just like you save a single-layer SpatRaster object.\n\nterra::writeRaster(IA_cdl_stack_sr, \"Data/IA_cdl_stack.tif\", filetype = \"GTiff\", overwrite = TRUE)\n\nThe saved file is a multi-band raster datasets. So, if you have many raster files of the same spatial extent and resolution, you can “stack” them on R and then export it to a single multi-band raster datasets, which cleans up your data folder.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#extract-information-from-raster-data-object",
    "href": "chapters/04-RasterDataBasics.html#extract-information-from-raster-data-object",
    "title": "4  Raster Data Handling",
    "section": "\n4.3 Extract information from raster data object",
    "text": "4.3 Extract information from raster data object\n\n4.3.1 Get CRS\nYou often need to extract the CRS of a raster object before you interact it with vector data (e.g., extracting values from a raster layer to vector data, or cropping a raster layer to the spatial extent of vector data), which can be done using terra::crs():\n\nterra::crs(IA_cdl_2015_sr)\n\n[1] \"PROJCRS[\\\"unknown\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\n\n4.3.2 Subset\nYou can access specific layers in a multi-layer raster object by indexing:\n\n#--- index ---#\nIA_cdl_stack_sr[[2]] # (originally IA_cdl_2016.tif)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : IA_cdl_2016.tif \ncolor table : 1 \nname        : IA_cdl_2016 \nmin value   :           0 \nmax value   :         241 \n\n\n\n4.3.3 Get cell values\nYou can access the values stored in a SpatRaster object using the terra::values() function:\n\n#--- terra::values ---#\nvalues_from_rs &lt;- terra::values(IA_cdl_stack_sr)\n\n#--- take a look ---#\nhead(values_from_rs)\n\n\n\n     Layer_1 Layer_1\n[1,]       0       0\n[2,]       0       0\n[3,]       0       0\n[4,]       0       0\n[5,]       0       0\n[6,]       0       0\n\n\nThe returned values come in a matrix form of two columns because we are getting values from a two-layer SpatRaster object (one column for each layer). In general, terra::values() returns a \\(X\\) by \\(n\\) matrix, where \\(X\\) is the number of cells and \\(n\\) is the number of layers.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#turning-a-raster-object-into-a-data.frame-not-necessary",
    "href": "chapters/04-RasterDataBasics.html#turning-a-raster-object-into-a-data.frame-not-necessary",
    "title": "4  Raster Data Handling",
    "section": "\n4.4 Turning a raster object into a data.frame (not necessary)",
    "text": "4.4 Turning a raster object into a data.frame (not necessary)\nYou can use the as.data.frame() function with xy = TRUE option to construct a data.frame where each row represents a single cell that has cell values for each layer and its coordinates (the center of the cell).\n\n#--- converting to a data.frame ---#\nIA_cdl_df &lt;- as.data.frame(IA_cdl_stack_sr, xy = TRUE) # this works with Raster* objects as well\n\n#--- take a look ---#\nhead(IA_cdl_df)\n\n\n\n       x       y Layer_1 Layer_1.1\n1 -52080 2288280       0         0\n2 -52050 2288280       0         0\n3 -52020 2288280       0         0\n4 -51990 2288280       0         0\n5 -51960 2288280       0         0\n6 -51930 2288280       0         0\n\n\n\n\n\n\n\n\nCaveat\n\n\n\nI have seen cases where raster objects are converted to a data.frame and then to an sf object for interacting with polygons using sf::st_join() to extract and assign cell values to polygons (see Chapter 3 for this type of operation). However, this approach is generally not recommended for two main reasons.\nFirst, it is significantly slower than using functions designed to work directly with raster objects and polygons, such as terra::extract() or exactextractr::exact_extract(), which are introduced in Chapter 5. The primary reason is that converting a raster to a data.frame is a time-consuming process.\nSecond, once a raster object is converted to point sf data, it becomes impossible to weight cell values based on their degree of overlap with the target polygons. While working with a data.frame may be appealing due to their familiarity, the conversion is often unnecessary and inefficient4.\n\n\n4 If you know of cases where converting to a data.frame is beneficial, please let me know, and I will include them here.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#quick-visualization",
    "href": "chapters/04-RasterDataBasics.html#quick-visualization",
    "title": "4  Raster Data Handling",
    "section": "\n4.5 Quick visualization",
    "text": "4.5 Quick visualization\nTo have a quick visualization of the data values of SpatRaster objects, you can simply use plot():\n\nplot(IA_cdl_2015_sr)\n\n\n\n\n\n\n\nFor a more elaborate map using raster data, see Section 7.2.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/04-RasterDataBasics.html#sec-work-with-netcdf",
    "href": "chapters/04-RasterDataBasics.html#sec-work-with-netcdf",
    "title": "4  Raster Data Handling",
    "section": "\n4.6 Working with netCDFs",
    "text": "4.6 Working with netCDFs\nA netCDF file contains data with a specific structure: a two-dimensional spatial grid (e.g., longitude and latitude) and a third dimension which is usually date or time. This structure is convenient for weather data measured on a consistent grid over time. One such dataset is called gridMET which maintains a gridded dataset of weather variables at 4km resolution. Let’s download the daily precipitation data for 2018 using downloader::download()5. We set the destination file name (what to call the file and where we want it to be), and the mode to wb for a binary download.\n5 gridMET data is also available in the Google Earth Engine Data Catalog, which can be accessed with the R library rgee\n#--- download gridMET precipitation 2018 ---#\ndownloader::download(\n  url = str_c(\"http://www.northwestknowledge.net/metdata/data/pr_2018.nc\"),\n  destfile = \"Data/pr_2018.nc\",\n  mode = \"wb\"\n)\n\nThis code should have stored the data as pr_2018.nc in the Data folder. You can read a netCDF file using terra::rast().\n\n(\n  pr_2018_gm &lt;- terra::rast(\"Data/pr_2018.nc\")\n)\n\nclass       : SpatRaster \ndimensions  : 585, 1386, 365  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : pr_2018.nc \nvarname     : precipitation_amount (pr) \nnames       : preci~43099, preci~43100, preci~43101, preci~43102, preci~43103, preci~43104, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \n\n\nYou can see that it has 365 layers: one layer per day in 2018. Let’s now look at layer names:\n\nhead(names(pr_2018_gm))\n\n[1] \"precipitation_amount_day=43099\" \"precipitation_amount_day=43100\"\n[3] \"precipitation_amount_day=43101\" \"precipitation_amount_day=43102\"\n[5] \"precipitation_amount_day=43103\" \"precipitation_amount_day=43104\"\n\n\nSince we have 365 layers and the number at the end of the layer names increase by 1, you would think that nth layer represents nth day of 2018. In this case, you are correct. However, it is always a good practice to confirm what each layer represents without assuming anything. Now, let’s use the ncdf4 package, which is built specifically to handle netCDF4 objects.\n\n(\n  pr_2018_nc &lt;- ncdf4::nc_open(\"Data/pr_2018.nc\")\n)\n\nFile Data/pr_2018.nc (NC_FORMAT_NETCDF4):\n\n     1 variables (excluding dimension variables):\n        unsigned short precipitation_amount[lon,lat,day]   (Chunking: [231,98,61])  (Compression: level 9)\n            _FillValue: 32767\n            units: mm\n            description: Daily Accumulated Precipitation\n            long_name: pr\n            standard_name: pr\n            missing_value: 32767\n            dimensions: lon lat time\n            grid_mapping: crs\n            coordinate_system: WGS84,EPSG:4326\n            scale_factor: 0.1\n            add_offset: 0\n            coordinates: lon lat\n            _Unsigned: true\n\n     4 dimensions:\n        lon  Size:1386 \n            units: degrees_east\n            description: longitude\n            long_name: longitude\n            standard_name: longitude\n            axis: X\n        lat  Size:585 \n            units: degrees_north\n            description: latitude\n            long_name: latitude\n            standard_name: latitude\n            axis: Y\n        day  Size:365 \n            description: days since 1900-01-01\n            units: days since 1900-01-01 00:00:00\n            long_name: time\n            standard_name: time\n            calendar: gregorian\n        crs  Size:1 \n            grid_mapping_name: latitude_longitude\n            longitude_of_prime_meridian: 0\n            semi_major_axis: 6378137\n            long_name: WGS 84\n            inverse_flattening: 298.257223563\n            GeoTransform: -124.7666666333333 0.041666666666666 0  49.400000000000000 -0.041666666666666\n            spatial_ref: GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n    19 global attributes:\n        geospatial_bounds_crs: EPSG:4326\n        Conventions: CF-1.6\n        geospatial_bounds: POLYGON((-124.7666666333333 49.400000000000000, -124.7666666333333 25.066666666666666, -67.058333300000015 25.066666666666666, -67.058333300000015 49.400000000000000, -124.7666666333333 49.400000000000000))\n        geospatial_lat_min: 25.066666666666666\n        geospatial_lat_max: 49.40000000000000\n        geospatial_lon_min: -124.7666666333333\n        geospatial_lon_max: -67.058333300000015\n        geospatial_lon_resolution: 0.041666666666666\n        geospatial_lat_resolution: 0.041666666666666\n        geospatial_lat_units: decimal_degrees north\n        geospatial_lon_units: decimal_degrees east\n        coordinate_system: EPSG:4326\n        author: John Abatzoglou - University of Idaho, jabatzoglou@uidaho.edu\n        date: 03 May 2021\n        note1: The projection information for this file is: GCS WGS 1984.\n        note2: Citation: Abatzoglou, J.T., 2013, Development of gridded surface meteorological data for ecological applications and modeling, International Journal of Climatology, DOI: 10.1002/joc.3413\n        note3: Data in slices after last_permanent_slice (1-based) are considered provisional and subject to change with subsequent updates\n        note4: Data in slices after last_provisional_slice (1-based) are considered early and subject to change with subsequent updates\n        note5: Days correspond approximately to calendar days ending at midnight, Mountain Standard Time (7 UTC the next calendar day)\n\n\nAs you can see from the output, there is tons of information that we did not see when we read the data using rast(), which includes the explanation of the third dimension (day) of this raster object. It turned out that the numerical values at the end of layer names in the SpatRaster object are days since 1900-01-01. So, the first layer (named precipitation_amount_day=43099) represents:\n\nlubridate::ymd(\"1900-01-01\") + 43099\n\n[1] \"2018-01-01\"\n\n\nActually, if we use raster::brick(), instead of terra::rast(), then we can see the naming convention of the layers:\n\n(\n  pr_2018_b &lt;- raster::brick(\"Data/pr_2018.nc\")\n)\n\nclass      : RasterBrick \ndimensions : 585, 1386, 810810, 365  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : pr_2018.nc \nnames      : X43099, X43100, X43101, X43102, X43103, X43104, X43105, X43106, X43107, X43108, X43109, X43110, X43111, X43112, X43113, ... \nday (days since 1900-01-01 00:00:00): 43099, 43463 (min, max)\nvarname    : precipitation_amount \n\n\nSpatRaster or RasterBrick objects are easier to work with as many useful functions accept them as inputs, but not the ncdf4 object. Personally, I first scrutinize a netCDFs file using nc_open() and then import it as a SpatRaster or RasterBrick object6. Recovering the dates for the layers is particularly important as we often wrangle the resulting data based on date (e.g., subset the data so that you have only April to September). An example of date recovery can be seen in Section 8.5.\n6 Even though RasterBrick provides the description of how layers are named, I think it is a good practice to see the full description in the ncdf4 object.For those who are interested in more detailed descriptions of how to work with ncdf4 object is provided here.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Raster Data Handling</span>"
    ]
  },
  {
    "objectID": "chapters/05-SpatialInteractionVectorRaster.html",
    "href": "chapters/05-SpatialInteractionVectorRaster.html",
    "title": "5  Spatial Interactions of Vector and Raster Data",
    "section": "",
    "text": "Before you start\nIn this chapter we learn the spatial interactions of a vector and raster dataset. We first look at how to crop (spatially subset) a raster dataset based on the geographic extent of a vector dataset. We then cover how to extract values from raster data for points and polygons. To be precise, here is what we mean by raster data extraction and what it does for points and polygons data:\nWe will show how we can use terra::extract() for both cases. But, we will also see that for polygons, exactextractr::exact_extract() from the exactextractr package is often considerably faster than terra::extract().",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Interactions of Vector and Raster Data</span>"
    ]
  },
  {
    "objectID": "chapters/05-SpatialInteractionVectorRaster.html#before-you-start",
    "href": "chapters/05-SpatialInteractionVectorRaster.html#before-you-start",
    "title": "5  Spatial Interactions of Vector and Raster Data",
    "section": "",
    "text": "Points: For each of the points, find which raster cell it is located within, and assign the value of the cell to the point.\nPolygons: For each of the polygons, identify all the raster cells that intersect with the polygon, and assign a vector of the cell values to the polygon\n\n\nDirection for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nPackages\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  terra, # handle raster data\n  tidyterra, # handle and visualize raster data\n  raster, # handle raster data\n  exactextractr, # fast extractions\n  sf, # vector data operations\n  dplyr, # data wrangling\n  tidyr, # data wrangling\n  data.table, # data wrangling\n  prism, # download PRISM data\n  tictoc, # timing codes\n  tigris, # to get county sf\n  tmap # for mapping\n)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Interactions of Vector and Raster Data</span>"
    ]
  },
  {
    "objectID": "chapters/05-SpatialInteractionVectorRaster.html#sec-raster-crop",
    "href": "chapters/05-SpatialInteractionVectorRaster.html#sec-raster-crop",
    "title": "5  Spatial Interactions of Vector and Raster Data",
    "section": "\n5.1 Cropping to the Area of Interest",
    "text": "5.1 Cropping to the Area of Interest\nHere we use PRISM maximum temperature (tmax) data as a raster dataset and Kansas county boundaries as a vector dataset.\nLet’s download the tmax data for July 1, 2018 (Figure 5.1).\n\n#--- set the path to the folder to which you save the downloaded PRISM data ---#\n# This code sets the current working directory as the designated folder\noptions(prism.path = \"Data\")\n\n#--- download PRISM precipitation data ---#\nprism::get_prism_dailys(\n  type = \"tmax\",\n  date = \"2018-07-01\",\n  keepZip = FALSE\n)\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file &lt;- \"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\"\n\n#--- read in the prism data ---#\nprism_tmax_0701_sr &lt;- terra::rast(prism_file)\n\n\n\n\n\nCodeggplot() +\n  geom_spatraster(data = prism_tmax_0701_sr) +\n  scale_fill_whitebox_c(\n    name = \"tmax\",\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"º\"),\n    n.breaks = 12,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  theme_void()\n\n\n\n\n\n\nFigure 5.1: Map of PRISM tmax data on July 1, 2018\n\n\n\n\nWe now get Kansas county border data from the tigris package (Figure 5.2) as sf.\n\n#--- Kansas boundary (sf) ---#\nKS_county_sf &lt;-\n  #--- get Kansas county boundary ---\n  tigris::counties(state = \"Kansas\", cb = TRUE) %&gt;%\n  #--- sp to sf ---#\n  sf::st_as_sf() %&gt;%\n  #--- transform using the CRS of the PRISM tmax data  ---#\n  sf::st_transform(terra::crs(prism_tmax_0701_sr))\n\n\n\n\n\nCode#--- gen map ---#\nggplot() +\n  geom_sf(data = KS_county_sf, fill = NA, color = \"blue\") +\n  theme_void()\n\n\n\n\n\n\nFigure 5.2: Kansas county boundaries\n\n\n\n\n\nCropping a raster layer to the area of interest can be useful, as it eliminates unnecessary data and reduces processing time. A smaller raster also makes value extraction faster. You can crop a raster layer using terra::crop(), as shown below:\n\n#--- syntax (this does not run) ---#\nterra::crop(SpatRaster, sf)\n\nSo, in this case, this does the job.\n\n#--- crop the entire PRISM to its KS portion---#\nprism_tmax_0701_KS_sr &lt;-\n  terra::crop(\n    prism_tmax_0701_sr,\n    KS_county_sf\n  )\n\nFigure 5.3 shows the PRISM tmax raster data cropped to the geographic extent of Kansas. Notice that the cropped raster layer extends beyond the outer boundary of Kansas state boundary (it is a bit hard to see, but look at the upper right corner).\n\nCodeggplot() +\n  geom_spatraster(data = prism_tmax_0701_KS_sr) +\n  geom_sf(data = KS_county_sf, fill = NA, color = \"blue\") +\n  scale_fill_whitebox_c(\n    name = \"tmax\",\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"º\"),\n    n.breaks = 12,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  theme_void()\n\n\n\n\n\n\nFigure 5.3: PRISM tmax raster data cropped to the geographic extent of Kansas",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Interactions of Vector and Raster Data</span>"
    ]
  },
  {
    "objectID": "chapters/05-SpatialInteractionVectorRaster.html#sec-extract-raster-to-vector",
    "href": "chapters/05-SpatialInteractionVectorRaster.html#sec-extract-raster-to-vector",
    "title": "5  Spatial Interactions of Vector and Raster Data",
    "section": "\n5.2 Extracting Values from Raster Layers for Vector Data",
    "text": "5.2 Extracting Values from Raster Layers for Vector Data\nIn this section, we will learn how to extract information from raster layers for spatial units represented as vector data (points and polygons). For the demonstrations in this section, we use the following datasets:\n\nRaster: PRISM tmax data cropped to Kansas state border for 07/01/2018 (obtained in Section 5.1) and 07/02/2018 (downloaded below)\nPolygons: Kansas county boundaries (obtained in Section 5.1)\nPoints: Irrigation wells in Kansas (imported below)\n\n\n5.2.1 Simple visual illustrations of raster data extraction\nExtracting to Points\nFigure 5.4 shows visually what we mean by “extract raster values to points.”\n\nCodeset.seed(378533)\n\n#--- create polygons ---#\npolygon &lt;-\n  sf::st_polygon(list(\n    rbind(c(0, 0), c(8, 0), c(8, 8), c(0, 8), c(0, 0))\n  ))\n\nraster_like_cells &lt;-\n  sf::st_make_grid(polygon, n = c(8, 8)) %&gt;%\n  sf::st_as_sf() %&gt;%\n  mutate(value = sample(1:64, 64))\n\nstars_cells &lt;-\n  stars::st_rasterize(raster_like_cells, nx = 8, ny = 8)\n\ncell_centroids &lt;-\n  sf::st_centroid(raster_like_cells) %&gt;%\n  sf::st_as_sf()\n\n#--------------------------\n# Create points for which values are extracted\n#--------------------------\n#--- points ---#\npoint_1 &lt;- sf::st_point(c(2.4, 2.2))\npoint_2 &lt;- sf::st_point(c(6.7, 1.8))\npoint_3 &lt;- sf::st_point(c(4.2, 7.1))\n\n#--- combine the points to make a single  sf of points ---#\npoints &lt;- list(point_1, point_2, point_3) %&gt;%\n  sf::st_sfc() %&gt;%\n  sf::st_as_sf() %&gt;%\n  dplyr::mutate(point_name = c(\"Point 1\", \"Point 2\", \"Point 3\"))\n\n#--------------------------\n# Create maps\n#--------------------------\nggplot() +\n  geom_stars(data = stars_cells, alpha = 0.5) +\n  scale_fill_distiller(name = \"Value\", palette = \"Spectral\") +\n  geom_sf_text(data = raster_like_cells, aes(label = value)) +\n  geom_sf(data = points, aes(shape = point_name), size = 2) +\n  scale_shape(name = \"Points\") +\n  theme_void() +\n  theme_for_map\n\n\n\n\n\n\nFigure 5.4: Visual illustration of raster data extraction for points data\n\n\n\n\nIn the figure, we have grids (cells) and each grid holds a value (presented at the center). There are three points. We will find which grid the points fall inside and get the associated values and assign them to the points. In this example, Points 1, 2, and 3 will have 50, 4, 54, respectively,\nExtracting to Polygons\nFigure 5.5 shows visually what we mean by “extract raster values to polygons.”\n\nCode#--------------------------\n# Create a polygon for which values are extracted\n#--------------------------\npolygon_extract &lt;-\n  sf::st_polygon(list(\n    rbind(c(1.5, 2), c(6, 2.3), c(7, 6.5), c(2, 5), c(1.5, 2))\n  ))\n\npolygons_extract_viz &lt;-\n  ggplot() +\n  geom_stars(data = stars_cells, alpha = 0.5) +\n  scale_fill_distiller(name = \"Value\", palette = \"Spectral\") +\n  geom_sf(data = polygon_extract, fill = \"gray\", alpha = 0.5) +\n  geom_sf(data = cell_centroids, color = \"black\", size = 0.8) +\n  geom_sf_text(\n    data = raster_like_cells, \n    aes(label = value),\n    nudge_x = -0.25,\n    nudge_y = 0.25\n  ) +\n  theme_void() +\n  theme_for_map\n\npolygons_extract_viz\n\n\n\n\n\n\nFigure 5.5: Visual illustration of raster data extraction for polygons data\n\n\n\n\nThere is a polygon overlaid on top of the cells along with their centroids represented by black dots. Extracting raster values to a polygon means finding raster cells that intersect with the polygons and get the value of all those cells and assigns them to the polygon. As you can see some cells are completely inside the polygon, while others are only partially overlapping with the polygon. Depending on the function you use and its options, you regard different cells as being spatially related to the polygons. For example, by default, terra::extract() will extract only the cells whose centroid is inside the polygon. But, you can add an option to include the cells that are only partially intersected with the polygon. In such a case, you can also get the fraction of the cell what is overlapped with the polygon, which enables us to find area-weighted values later. We will discuss the details these below.\nPRISM tmax data for 07/02/2018\n\n#--- download PRISM precipitation data ---#\nprism::get_prism_dailys(\n  type = \"tmax\",\n  date = \"2018-07-02\",\n  keepZip = FALSE\n)\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file &lt;- \"Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil\"\n\n#--- read in the prism data and crop it to Kansas state border ---#\nprism_tmax_0702_KS_sr &lt;-\n  terra::rast(prism_file) %&gt;%\n  terra::crop(KS_county_sf)\n\nIrrigation wells in Kansas:\n\n#--- read in the KS points data ---#\n(\n  KS_wells &lt;- readRDS(\"Data/Chap_5_wells_KS.rds\")\n)\n\nSimple feature collection with 37647 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id                   geometry\n1        1 POINT (-100.4423 37.52046)\n2        3 POINT (-100.7118 39.91526)\n3        5 POINT (-99.15168 38.48849)\n4        7 POINT (-101.8995 38.78077)\n5        8  POINT (-100.7122 38.0731)\n6        9 POINT (-97.70265 39.04055)\n7       11 POINT (-101.7114 39.55035)\n8       12 POINT (-95.97031 39.16121)\n9       15 POINT (-98.30759 38.26787)\n10      17 POINT (-100.2785 37.71539)\n\n\n\nHere is how the wells are spatially distributed over the PRISM grids and Kansas county borders (Figure 6.9):\n\nCodetm_shape(KS_county_sf) +\n  tm_polygons(alpha = 0) +\n  tm_shape(KS_wells) +\n  tm_symbols(size = 0.02) +\n  tm_layout(\n    frame = FALSE,\n    legend.outside = TRUE,\n    legend.outside.position = \"bottom\"\n  )\n\n\n\n\n\n\nFigure 5.6: Map of Kansas county borders, irrigation wells, and PRISM tmax\n\n\n\n\n\n5.2.2 Extracting to Points\nYou can extract values from raster layers to points using terra::extract(). terra::extract() finds which raster cell each of the points is located within and assigns the value of the cell to the point.\n\n#--- syntax (this does not run) ---#\nterra::extract(raster, points)\n\nThe points objects can be either sf or SpatVect1.\n1 terra::extract used to accept only SpatVect and the sf objects had to be converted to a SpatVect using terra::vect()Let’s extract tmax values from the PRISM tmax layer (prism_tmax_0701_KS_sr) to the irrigation wells (KS_wells as sf):\n\ntmax_from_prism &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_wells)\n\nhead(tmax_from_prism)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.241\n2  2                               29.288\n3  3                               32.585\n4  4                               30.104\n5  5                               34.232\n6  6                               35.168\n\n\nThe resulting object is a data.frame, where the ID variable represents the order of the observations in the points data and the second column represents that values extracted for the points from the raster cells. So, you can assign the extracted values as a new variable of the points data as follows:\n\nKS_wells$tmax_07_01 &lt;- tmax_from_prism[, -1]\n\nExtracting values from a multi-layer SpatRaster works the same way. Here, we combine prism_tmax_0701_KS_sr and prism_tmax_0702_KS_sr to create a multi-layer SpatRaster and then extract values from them.\n\n#--- create a multi-layer SpatRaster ---#\nprism_tmax_stack &lt;- c(prism_tmax_0701_KS_sr, prism_tmax_0702_KS_sr)\n\n#--- extract tmax values ---#\ntmax_from_prism_stack &lt;- terra::extract(prism_tmax_stack, KS_wells)\n\n#--- take a look ---#\nhead(tmax_from_prism_stack)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.241                               30.544\n2  2                               29.288                               29.569\n3  3                               32.585                               29.866\n4  4                               30.104                               29.819\n5  5                               34.232                               30.481\n6  6                               35.168                               30.640\n\n\nWe now have two columns that hold values extracted from the raster cells: the 2nd column for the 1st raster layer and the 3rd column for the 2nd raster layer in prism_tmax_stack.\n\n5.2.3 Extracting to Polygons (terra way)\nYou can use terra::extract() for extracting raster cell values for polygons as well. For each of the polygons, it will identify all the raster cells whose center lies inside the polygon and assign the vector of values of the cells to the polygon.\n\n5.2.3.1 extract from a single-layer SpatRaster\n\nLet’s extract tmax values from prism_tmax_0701_KS_sr for each of the KS counties.\n\n#--- extract values from the raster for each county ---#\ntmax_by_county &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_county_sf)\n\n\n#--- check the class ---#\nclass(tmax_by_county)\n\n[1] \"data.frame\"\n\n#--- take a look ---#\nhead(tmax_by_county)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.228\n2  1                               34.222\n3  1                               34.256\n4  1                               34.268\n5  1                               34.262\n6  1                               34.477\n\n#--- take a look ---#\ntail(tmax_by_county)\n\n       ID PRISM_tmax_stable_4kmD2_20180701_bil\n12840 105                               34.185\n12841 105                               34.180\n12842 105                               34.241\n12843 105                               34.381\n12844 105                               34.295\n12845 105                               34.267\n\n\nSo, terra::extract() returns a data.frame, where ID values represent the corresponding row number in the polygons data. For example, the observations with ID == n are for the nth polygon. Using this information, you can easily merge the extraction results to the polygons data. Suppose you are interested in the mean of the tmax values of the intersecting cells for each of the polygons, then you can do the following:\n\n#--- get mean tmax ---#\nmean_tmax &lt;-\n  tmax_by_county %&gt;%\n  group_by(ID) %&gt;%\n  summarize(tmax = mean(PRISM_tmax_stable_4kmD2_20180701_bil))\n\n(\n  KS_county_sf &lt;-\n    #--- back to sf ---#\n    KS_county_sf %&gt;%\n    #--- define ID ---#\n    mutate(ID := seq_len(nrow(.))) %&gt;%\n    #--- merge by ID ---#\n    left_join(., mean_tmax, by = \"ID\")\n)\n\nSimple feature collection with 105 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n1       20      175 00485050 0500000US20175 20175       Seward\n2       20      027 00484983 0500000US20027 20027         Clay\n3       20      171 00485048 0500000US20171 20171        Scott\n4       20      047 00484993 0500000US20047 20047      Edwards\n5       20      147 00485037 0500000US20147 20147     Phillips\n6       20      149 00485038 0500000US20149 20149 Pottawatomie\n7       20      055 00485326 0500000US20055 20055       Finney\n8       20      167 00485046 0500000US20167 20167      Russell\n9       20      135 00485031 0500000US20135 20135         Ness\n10      20      093 00485011 0500000US20093 20093       Kearny\n              NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER ID     tmax\n1        Seward County     KS     Kansas   06 1656693304  1961444  1 34.21421\n2          Clay County     KS     Kansas   06 1671314413 26701337  2 32.58354\n3         Scott County     KS     Kansas   06 1858536838   306079  3 34.63195\n4       Edwards County     KS     Kansas   06 1610699245   206413  4 33.40829\n5      Phillips County     KS     Kansas   06 2294395636 22493383  5 34.01768\n6  Pottawatomie County     KS     Kansas   06 2177493041 54149843  6 35.54426\n7        Finney County     KS     Kansas   06 3372157854  1716371  7 34.47948\n8       Russell County     KS     Kansas   06 2295402858 34126776  8 33.55797\n9          Ness County     KS     Kansas   06 2783562234   667491  9 34.77049\n10       Kearny County     KS     Kansas   06 2254696661  1133601 10 33.56413\n                         geometry\n1  MULTIPOLYGON (((-101.0681 3...\n2  MULTIPOLYGON (((-97.3707 39...\n3  MULTIPOLYGON (((-101.1284 3...\n4  MULTIPOLYGON (((-99.56988 3...\n5  MULTIPOLYGON (((-99.62821 3...\n6  MULTIPOLYGON (((-96.72774 3...\n7  MULTIPOLYGON (((-101.103 37...\n8  MULTIPOLYGON (((-99.04234 3...\n9  MULTIPOLYGON (((-100.2477 3...\n10 MULTIPOLYGON (((-101.5419 3...\n\n\n\n5.2.3.2 extract and summarize in one step\nInstead of finding the mean after applying terra::extract() as done above, you can do that within terra::extract() using the fun option.\n\n#--- extract values from the raster for each county ---#\ntmax_by_county &lt;-\n  terra::extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sf,\n    fun = mean\n  )\n\n\n#--- take a look ---#\nhead(tmax_by_county)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                             34.21421\n2  2                             32.58354\n3  3                             34.63195\n4  4                             33.40829\n5  5                             34.01768\n6  6                             35.54426\n\n\nYou can apply other summary functions like min(), max(), sum().\n\n5.2.3.3 extract from multi-layer SpatRaster\n\nExtracting values from a multi-layer raster data works exactly the same way except that data processing after the value extraction is slightly more complicated.\n\n#--- extract from a multi-layer raster object ---#\ntmax_by_county_from_stack &lt;-\n  terra::extract(\n    prism_tmax_stack,\n    KS_county_sf\n  )\n\n#--- take a look ---#\nhead(tmax_by_county_from_stack)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.181                               30.414\n2  1                               34.180                               30.314\n3  1                               34.210                               30.238\n4  1                               34.190                               30.163\n5  1                               34.292                               30.207\n6  1                               34.258                               30.222\n\n\nSimilar to the single-layer case, the resulting object is a data.frame and you have two columns corresponding to each of the layers in the two-layer raster object.\n\n5.2.3.4 exact = TRUE option\nSometimes, you would like to have how much of the raster cells are intersecting with the intersecting polygon to find area-weighted summary later. In that case, you can add exact = TRUE option to terra::extract().\n\n#--- extract from a multi-layer raster object ---#\ntmax_by_county_from_stack &lt;-\n  terra::extract(\n    prism_tmax_stack,\n    KS_county_sf,\n    exact = TRUE\n  )\n\n#--- take a look ---#\nhead(tmax_by_county_from_stack)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.222                               30.489\n2  1                               34.181                               30.414\n3  1                               34.180                               30.314\n4  1                               34.210                               30.238\n5  1                               34.190                               30.163\n6  1                               34.292                               30.207\n   fraction\n1 0.1074196\n2 0.8066955\n3 0.8067041\n4 0.8066761\n5 0.8061875\n6 0.8054752\n\n\nAs you can see, now you have fraction column to the resulting data.frame and you can find area-weighted summary of the extracted values like this:\n\ntmax_by_county_from_stack %&gt;%\n  group_by(ID) %&gt;%\n  summarize(\n    tmax_0701 = sum(fraction * PRISM_tmax_stable_4kmD2_20180701_bil) / sum(fraction),\n    tmax_0702 = sum(fraction * PRISM_tmax_stable_4kmD2_20180702_bil) / sum(fraction)\n  )\n\n# A tibble: 105 × 3\n      ID tmax_0701 tmax_0702\n   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1     1      34.5      30.5\n 2     2      34.5      29.8\n 3     3      33.6      31.2\n 4     4      32.3      29.5\n 5     5      32.6      28.7\n 6     6      35.7      28.8\n 7     7      34.0      30.6\n 8     8      33.4      29.9\n 9     9      32.9      30.4\n10    10      33.4      30.6\n# ℹ 95 more rows\n\n\n\n5.2.4 Extracting to Polygons (exactextractr way)\nexactextractr::exact_extract() function from the exactextractr package is a faster alternative than terra::extract() for a large raster data as we confirm later (exactextractr::exact_extract() does not work with points data at the moment).2 exactextractr::exact_extract() also provides a coverage fraction value for each of the cell-polygon intersections. The syntax of exactextractr::exact_extract() is very much similar to terra::extract().\n2 See here for how it does extraction tasks differently from other major GIS software.\n#--- syntax (this does not run) ---#\nexactextractr::exact_extract(raster, polygons sf, include_cols = list of vars)\n\nA notable difference and advantage of exactextractr::exact_extract() is that you can specify a list of variables from the polygons sf to be inherited to the output. This means that we do not have to merge the output with the polygons sf using the rule that the nth element of the output corresponds to the nth row of polygons sf unlike the terra::extract() way. exactextractr::exact_extract() can accept both SpatRaster and Raster\\(^*\\) objects as the raster object. However, while it accepts sf as the polygons object, but it does not accept SpatVect.\n\n5.2.4.1 extract from a single-layer SpatRaster\n\nLet’s get tmax values from the PRISM raster layer for Kansas county polygons, the following does the job:\n\n#--- extract values from the raster for each county ---#\ntmax_by_county &lt;-\n  exactextractr::exact_extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sf,\n    #--- inherit COUNTYFP from KS_county_sf ---#\n    include_cols = \"COUNTYFP\",\n    #--- this is for not displaying progress bar ---#\n    progress = FALSE\n  )\n\nThe resulting object is a list of data.frames.\n\n#--- take a look at the first 6 rows of the first two list elements ---#\ntmax_by_county[1:2] %&gt;% lapply(function(x) head(x))\n\n[[1]]\n  COUNTYFP  value coverage_fraction\n1      175 34.222         0.1074141\n2      175 34.181         0.8066747\n3      175 34.180         0.8066615\n4      175 34.210         0.8066448\n5      175 34.190         0.8061629\n6      175 34.292         0.8054324\n\n[[2]]\n  COUNTYFP  value coverage_fraction\n1      027 33.847        0.03732148\n2      027 33.897        0.10592906\n3      027 34.010        0.10249268\n4      027 34.186        0.10018899\n5      027 34.293        0.10074520\n6      027 34.220        0.09701102\n\n\nFor each element of the list, you see value and coverage_fraction. value is the tmax value of the intersecting raster cells, and coverage_fraction is the fraction of the intersecting area relative to the full raster grid, which can help find coverage-weighted summary of the extracted values (like fraction variable when you use terra::extract() with exact = TRUE).\nWe can take advantage of dplyr::bind_rows() to combine the list of the datasets into a single data.frame.\n\n(\n  #--- combine ---#\n  tmax_combined &lt;- \n    tmax_by_county %&gt;%\n    dplyr::bind_rows() %&gt;%\n    tibble::as_tibble()\n)\n\n# A tibble: 15,149 × 3\n   COUNTYFP value coverage_fraction\n   &lt;chr&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n 1 175       34.2             0.107\n 2 175       34.2             0.807\n 3 175       34.2             0.807\n 4 175       34.2             0.807\n 5 175       34.2             0.806\n 6 175       34.3             0.805\n 7 175       34.3             0.805\n 8 175       34.2             0.805\n 9 175       34.2             0.803\n10 175       34.2             0.803\n# ℹ 15,139 more rows\n\n\n\n\n\ndata.table users can use data.table::rbindlist() instead.\n\ndata.table::rbindlist(tmax_by_county, idcol = \"id\")\n\n          id COUNTYFP  value coverage_fraction\n       &lt;int&gt;   &lt;char&gt;  &lt;num&gt;             &lt;num&gt;\n    1:     1      175 34.222         0.1074141\n    2:     1      175 34.181         0.8066747\n    3:     1      175 34.180         0.8066615\n    4:     1      175 34.210         0.8066448\n    5:     1      175 34.190         0.8061629\n   ---                                        \n15145:   105      197 34.847         0.7625142\n15146:   105      197 34.840         0.7627296\n15147:   105      197 34.625         0.7640904\n15148:   105      197 34.755         0.7634417\n15149:   105      197 34.953         0.6051113\n\n\nLet’s summarize the data by COUNTYFP and then merge it back to the polygons sf data. Here, we calculate coverage-weighted mean of tmax.\n\n#--- weighted mean ---#\n(\n  tmax_by_county &lt;-\n    tmax_combined %&gt;%\n    #--- group summary ---#\n    dplyr::group_by(COUNTYFP) %&gt;%\n    dplyr::summarise(tmax_aw = sum(value * coverage_fraction) / sum(coverage_fraction))\n)\n\n# A tibble: 105 × 2\n   COUNTYFP tmax_aw\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 001         34.3\n 2 003         34.9\n 3 005         35.3\n 4 007         34.1\n 5 009         32.9\n 6 011         34.8\n 7 013         34.9\n 8 015         34.3\n 9 017         35.8\n10 019         33.4\n# ℹ 95 more rows\n\n#--- merge ---#\ndplyr::left_join(KS_county_sf, tmax_by_county, by = \"COUNTYFP\") %&gt;%\n  dplyr::select(COUNTYFP, tmax_aw)\n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   COUNTYFP  tmax_aw                       geometry\n1       175 34.46388 MULTIPOLYGON (((-101.0681 3...\n2       027 34.53894 MULTIPOLYGON (((-97.3707 39...\n3       171 33.56887 MULTIPOLYGON (((-101.1284 3...\n4       047 32.26198 MULTIPOLYGON (((-99.56988 3...\n5       147 32.62552 MULTIPOLYGON (((-99.62821 3...\n6       149 35.67416 MULTIPOLYGON (((-96.72774 3...\n7       055 33.96688 MULTIPOLYGON (((-101.103 37...\n8       167 33.35979 MULTIPOLYGON (((-99.04234 3...\n9       135 32.86975 MULTIPOLYGON (((-100.2477 3...\n10      093 33.41405 MULTIPOLYGON (((-101.5419 3...\n\n\n\n5.2.4.2 extract from multi-layer SpatRaster\n\nExtracting values from a multi-layer SpatRaster object works in exactly the same manner as a single-layer SpatRaster object.\n\ntmax_by_county_stack &lt;-\n  exactextractr::exact_extract(\n    prism_tmax_stack,\n    KS_county_sf,\n    include_cols = \"COUNTYFP\",\n    progress = FALSE\n  )\n\n#--- take a look at the first 6 lines of the first element---#\ntmax_by_county_stack[[1]] %&gt;% head()\n\n  COUNTYFP PRISM_tmax_stable_4kmD2_20180701_bil\n1      175                               34.222\n2      175                               34.181\n3      175                               34.180\n4      175                               34.210\n5      175                               34.190\n6      175                               34.292\n  PRISM_tmax_stable_4kmD2_20180702_bil coverage_fraction\n1                               30.489         0.1074141\n2                               30.414         0.8066747\n3                               30.314         0.8066615\n4                               30.238         0.8066448\n5                               30.163         0.8061629\n6                               30.207         0.8054324\n\n\nAs you can see above, exactextractr::exact_extract() appends additional columns for additional layers.\n\n#--- combine them ---#\ntmax_all_combined &lt;- bind_rows(tmax_by_county_stack)\n\n#--- take a look ---#\nhead(tmax_all_combined)\n\n  COUNTYFP PRISM_tmax_stable_4kmD2_20180701_bil\n1      175                               34.222\n2      175                               34.181\n3      175                               34.180\n4      175                               34.210\n5      175                               34.190\n6      175                               34.292\n  PRISM_tmax_stable_4kmD2_20180702_bil coverage_fraction\n1                               30.489         0.1074141\n2                               30.414         0.8066747\n3                               30.314         0.8066615\n4                               30.238         0.8066448\n5                               30.163         0.8061629\n6                               30.207         0.8054324\n\n\nIn order to find the coverage-weighted tmax by date, you can first pivot it to a long format using tidyr::pivot_longer().\n\n#--- pivot to a longer format ---#\n(\n  tmax_long &lt;-\n    tidyr::pivot_longer(\n      tmax_all_combined,\n      -c(COUNTYFP, coverage_fraction),\n      names_to = \"date\",\n      values_to = \"tmax\"\n    )\n)\n\n# A tibble: 30,298 × 4\n   COUNTYFP coverage_fraction date                                  tmax\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;\n 1 175                  0.107 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 2 175                  0.107 PRISM_tmax_stable_4kmD2_20180702_bil  30.5\n 3 175                  0.807 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 4 175                  0.807 PRISM_tmax_stable_4kmD2_20180702_bil  30.4\n 5 175                  0.807 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 6 175                  0.807 PRISM_tmax_stable_4kmD2_20180702_bil  30.3\n 7 175                  0.807 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n 8 175                  0.807 PRISM_tmax_stable_4kmD2_20180702_bil  30.2\n 9 175                  0.806 PRISM_tmax_stable_4kmD2_20180701_bil  34.2\n10 175                  0.806 PRISM_tmax_stable_4kmD2_20180702_bil  30.2\n# ℹ 30,288 more rows\n\n\nAnd then find coverage-weighted tmax by date:\n\n(\n  tmax_long %&gt;%\n    dplyr::group_by(COUNTYFP, date) %&gt;%\n    dplyr::summarize(tmax = sum(tmax * coverage_fraction) / sum(coverage_fraction))\n)\n\n# A tibble: 210 × 3\n# Groups:   COUNTYFP [105]\n   COUNTYFP date                                  tmax\n   &lt;chr&gt;    &lt;chr&gt;                                &lt;dbl&gt;\n 1 001      PRISM_tmax_stable_4kmD2_20180701_bil  34.3\n 2 001      PRISM_tmax_stable_4kmD2_20180702_bil  29.4\n 3 003      PRISM_tmax_stable_4kmD2_20180701_bil  34.9\n 4 003      PRISM_tmax_stable_4kmD2_20180702_bil  29.4\n 5 005      PRISM_tmax_stable_4kmD2_20180701_bil  35.3\n 6 005      PRISM_tmax_stable_4kmD2_20180702_bil  28.9\n 7 007      PRISM_tmax_stable_4kmD2_20180701_bil  34.1\n 8 007      PRISM_tmax_stable_4kmD2_20180702_bil  29.7\n 9 009      PRISM_tmax_stable_4kmD2_20180701_bil  32.9\n10 009      PRISM_tmax_stable_4kmD2_20180702_bil  29.8\n# ℹ 200 more rows\n\n\n\n\n\nFor data.table users, this does the same:\n\n(\n  tmax_all_combined %&gt;%\n    data.table() %&gt;%\n    melt(id.var = c(\"COUNTYFP\", \"coverage_fraction\")) %&gt;%\n    .[, .(tmax = sum(value * coverage_fraction) / sum(coverage_fraction)), by = .(COUNTYFP, variable)]\n)\n\n     COUNTYFP                             variable     tmax\n       &lt;char&gt;                               &lt;fctr&gt;    &lt;num&gt;\n  1:      175 PRISM_tmax_stable_4kmD2_20180701_bil 34.46388\n  2:      027 PRISM_tmax_stable_4kmD2_20180701_bil 34.53894\n  3:      171 PRISM_tmax_stable_4kmD2_20180701_bil 33.56887\n  4:      047 PRISM_tmax_stable_4kmD2_20180701_bil 32.26198\n  5:      147 PRISM_tmax_stable_4kmD2_20180701_bil 32.62552\n ---                                                       \n206:      169 PRISM_tmax_stable_4kmD2_20180702_bil 30.54567\n207:      073 PRISM_tmax_stable_4kmD2_20180702_bil 29.64180\n208:      071 PRISM_tmax_stable_4kmD2_20180702_bil 30.13041\n209:      001 PRISM_tmax_stable_4kmD2_20180702_bil 29.43459\n210:      197 PRISM_tmax_stable_4kmD2_20180702_bil 29.97933\n\n\n\n5.2.4.3 extract and summarize in one step\nInstead of returning the value from all the intersecting cells, exactextractr::exact_extract() can summarize the extracted values by polygon and then return the summarized numbers. This is much like how terra::extract() with FUN option works. There are multiple default options you can choose from. All you need to do is to add the desired summary function name as the third argument of exactextractr::exact_extract(). For example, the following will get us the mean of the extracted values weighted by coverage_fraction.\n\nextacted_mean &lt;- \n  exactextractr::exact_extract(\n    prism_tmax_stack, \n    KS_county_sf, \n    \"mean\", \n    append_cols = \"COUNTYFP\", \n    progress = FALSE\n  )\n\nhead(extacted_mean)\n\nNotice that you use append_cols instead of include_cols when you summarize within exactextractr::exact_extract().\n\n(\nmean_tmax_long &lt;- \n  extacted_mean %&gt;% \n  #--- wide to long ---#\n  pivot_longer(-COUNTYFP, names_to = \"date\", values_to = \"tmax\") %&gt;%\n  #--- recover date ---#\n  dplyr::mutate(date = gsub(\"mean.date\", \"\", date) %&gt;% lubridate::ymd())\n)\n\n# A tibble: 210 × 3\n   COUNTYFP date    tmax\n   &lt;chr&gt;    &lt;date&gt; &lt;dbl&gt;\n 1 175      NA      34.5\n 2 175      NA      30.5\n 3 027      NA      34.5\n 4 027      NA      29.8\n 5 171      NA      33.6\n 6 171      NA      31.2\n 7 047      NA      32.3\n 8 047      NA      29.5\n 9 147      NA      32.6\n10 147      NA      28.7\n# ℹ 200 more rows\n\n\nThere are other summary function options that may be of interest, such as “max”, “min.” You can see all the default options at the package website.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Interactions of Vector and Raster Data</span>"
    ]
  },
  {
    "objectID": "chapters/05-SpatialInteractionVectorRaster.html#sec-extract-speed",
    "href": "chapters/05-SpatialInteractionVectorRaster.html#sec-extract-speed",
    "title": "5  Spatial Interactions of Vector and Raster Data",
    "section": "\n5.3 Extraction speed comparison",
    "text": "5.3 Extraction speed comparison\nHere we compare the extraction speed of raster::extract(), terra::extract(), and exactextractr::exact_extract(). A more comprehensive discussion on the speed of raster extraction can be found in Chapter 9.\n\n5.3.1 Data preparation\nWe first create two new R objects here:\n\n\nSpatVectore version of KS_wells, which is currently an sf object.\n\nSpatVectore version of KS_county_sf, which is currently an sf object.\n\n\nKS_wells_sv &lt;- terra::vect(KS_wells)\nKS_county_sv &lt;- terra::vect(KS_county_sf)\n\nWe also create a version of prism_tmax_0701_KS_sr that is disaggregated by a factor of 10 to create a much larger raster data.\n\n#--- disaggregate ---#\n(\n  prism_tmax_0701_KS_sr_10 &lt;- terra::disagg(prism_tmax_0701_KS_sr, fact = 10)\n)\n\nclass       : SpatRaster \ndimensions  : 730, 1790, 1  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : -102.0625, -94.60417, 36.97917, 40.02083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 \nsource(s)   : memory\nvarname     : PRISM_tmax_stable_4kmD2_20180701_bil \nname        : PRISM_tmax_stable_4kmD2_20180701_bil \nmin value   :                               27.711 \nmax value   :                               37.656 \n\n\nThe disaggregated PRISM data now has 10 times more rows and columns.\n\n#--- original ---#\ndim(prism_tmax_0701_KS_sr)\n\n[1]  73 179   1\n\n#--- disaggregated ---#\ndim(prism_tmax_0701_KS_sr_10)\n\n[1]  730 1790    1\n\n\n\n5.3.2 Points: terra::extract()\n\nExtraction speed of terra::extract() depends on the class of the raster and vector (points) objects3. Whether the points data is sf or SpatVector makes a difference as you can see below with SpatVector being faster.\n3 Until recently, raster::extract() was much much slower compared to terra::extract(). However, recent updates to the package made significant improvement in extraction speed and raster::extract() is comparable to terra::extract() in speed.\n#--- SpatRaster and sf ---#\ntic()\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_wells)\ntoc()\n\n0.221 sec elapsed\n\n#--- SpatRaster and SpatVector ---#\ntic()\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_wells_sv)\ntoc()\n\n0.024 sec elapsed\n\n\nAlthough the time difference may seem small, it can accumulate significantly when extracting data from numerous raster layers to the same set of points, resulting in a notable increase in overall processing time. In such cases, it is recommended to first convert the points from sf to SpatVector before performing the extractions.\n\n5.3.3 Polygons: exactextractr::exact_extract() and terra::extract()\n\nterra::extract() is faster than exactextractr::exact_extract() for a relatively small raster data. Let’s use prism_tmax_0701_KS_sr and time them to see the difference.\n\n#--- terra::extract ---#\ntic()\nterra_extract_temp &lt;-\n  terra::extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sv\n  )\ntoc()\n\n0.031 sec elapsed\n\n#--- exact_extract ---#\ntic()\nexact_extract_temp &lt;-\n  exactextractr::exact_extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sf,\n    progress = FALSE\n  )\ntoc()\n\n0.069 sec elapsed\n\n\nAs you can see, terra::extract() is faster than exactextractr::exact_extract(). However, once the raster data becomes larger (or spatially finer), then exact_extact() starts to shine.\n\nNow, let’s compare terra::extrct() and exact_extrct() using the disaggregated data.\n\n#--- terra extract ---#\ntic()\nterra_extract_temp &lt;-\n  terra::extract(\n    prism_tmax_0701_KS_sr_10,\n    KS_county_sv\n  )\ntoc()\n\n1.864 sec elapsed\n\n#--- exact extract ---#\ntic()\nexact_extract_temp &lt;-\n  exact_extract(\n    prism_tmax_0701_KS_sr_10,\n    KS_county_sf,\n    progress = FALSE\n  )\ntoc()\n\n0.132 sec elapsed\n\n\nAs you can see, exactextractr::exact_extract() is considerably faster. The difference in time becomes even more pronounced as the size of the raster data becomes larger and the number of polygons are greater. The time difference of several seconds seem nothing, but imagine processing Cropland Data Layer (CDL) files for the entire US over 10 years, then you would appreciate the speed of exactextractr::exact_extract(). See Chapter 9 for more comprehensive treatment of raster value extraction speed.\n\n5.3.4 Single-layer vs multi-layer\nPretend that you have five dates of PRISM tmax data (here we repeat the same file five times) and would like to extract values from all of them. Extracting values from a multi-layer raster objects (RasterStack for raster package) takes less time than extracting values from the individual layers one at a time. This can be observed below.\n\nterra::extract()\n\n#--- extract from 5 layers one at a time ---#\ntic()\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntemp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv)\ntoc()\n\n10.505 sec elapsed\n\n#--- extract from a 5-layer SpatRaster ---#\ntic()\nprism_tmax_ml_5 &lt;-\n  c(\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10\n  )\ntemp &lt;- terra::extract(prism_tmax_ml_5, KS_county_sv)\ntoc()\n\n3.094 sec elapsed\n\n\n\nexactextractr::exact_extract()\n\n#--- extract from 5 layers one at a time ---#\ntic()\ntemp &lt;- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp &lt;- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp &lt;- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp &lt;- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntemp &lt;- exact_extract(prism_tmax_0701_KS_sr_10, KS_county_sf, progress = FALSE)\ntoc()\n\n0.691 sec elapsed\n\n#--- extract from from a 5-layer SpatRaster ---#\ntic()\nprism_tmax_stack_5 &lt;-\n  c(\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10,\n    prism_tmax_0701_KS_sr_10\n  )\n\n#--- set layer names ---#\n# exact_extract() does not like multi-layer raster with the same layer name\nset.names(prism_tmax_stack_5, paste0(\"layer_\", 1:5))\n\ntemp &lt;-\n  exactextractr::exact_extract(\n    prism_tmax_stack_5,\n    KS_county_sf,\n    progress = FALSE\n  )\ntoc()\n\n0.384 sec elapsed\n\n\nThe reduction in computation time for both methods makes sense. Since both layers have exactly the same geographic extent and resolution, finding the polygons-cells correspondence is done once and then it can be used repeatedly across the layers for the multi-layer SparRaster and RasterStack. This clearly suggests that when you are processing many layers of the same spatial resolution and extent, you should first stack them and then extract values at the same time instead of processing them one by one as long as your memory allows you to do so.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Interactions of Vector and Raster Data</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html",
    "href": "chapters/06-stars.html",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "",
    "text": "Before you start\nIn this chapter, we introduce the stars package (Pebesma 2020) for handling raster data. It is especially useful for those working with spatiotemporal raster datasets, such as daily PRISM and Daymet data, as it offers a consistent framework for managing raster data with temporal dimensions. Specifically, stars objects can include a time dimension in addition to the usual spatial 2D dimensions (longitude and latitude), with the time dimension accepting Date values. This feature proves to be particularly handy for tasks like filtering data by date, as you will see below.\nAnother advantage of the stars package is its compatibility with sf objects as the lead developer of the two packages is the same person. Therefore, unlike the terra package approach, we do not need any tedious conversions between sf and SpatVector. The stars package also allows dplyr-like data operations using functions like dplyr::filter(), dplyr::mutate() (see Section 6.6).\nIn Chapters Chapter 4 and Chapter 5, we used the raster and terra packages to handle raster data and interact raster data with vector data. If you do not feel any inconvenience with the approach, you do not need to read on. Also, note that the stars package was not written to replace either raster or terra packages. Here is a good summary of how raster functions map to stars functions. As you can see, there are many functions that are available in the terra packages that cannot be implemented by the stars package. However, I must say the functionality of the stars package is rather complete at least for most users, and it is definitely possible to use just the stars package for all the raster data work in most cases.\nFinally, this book does not cover the use of stars_proxy for big data that does not fit in your memory, which may be useful for some of you. This provides an introduction to stars_proxy for those interested. This book also does not cover irregular raster cells (e.g., curvelinear grids). Interested readers are referred to here.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#before-you-start",
    "href": "chapters/06-stars.html#before-you-start",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "",
    "text": "Direction for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here and put them in the “Data” folder\n\nPackages\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  sf, # vector data handling\n  tidyverse, # data wrangling\n  cubelyr, # handle raster data\n  tmap, # make maps\n  mapview, # make maps\n  exactextractr, # fast raster data extraction\n  lubridate, # handle dates\n  prism # download PRISM data\n)\n\n\nRun the following code to define the theme for map:\n\n\ntheme_set(theme_bw())\n\ntheme_for_map &lt;- theme(\n  axis.ticks = element_blank(),\n  axis.text = element_blank(),\n  axis.line = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(color = \"transparent\"),\n  panel.grid.minor = element_line(color = \"transparent\"),\n  panel.background = element_blank(),\n  plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#sec-stars-structure",
    "href": "chapters/06-stars.html#sec-stars-structure",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.1 Understanding the structure of a stars object",
    "text": "6.1 Understanding the structure of a stars object\nLet’s import a stars object of daily PRISM precipitation and tmax saved as an R dataset.\n\n#--- read PRISM prcp and tmax data  ---#\n(\n  prcp_tmax_PRISM_m8_y09 &lt;- readRDS(\"Data/prcp_tmax_PRISM_m8_y09_small.rds\")\n)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nThis stars object has two attributes: ppt (precipitation) and tmax (maximum temperature). They are like variables in a regular data.frame. They hold information we are interested in using for our analysis.\nThis stars object has three dimensions: x (longitude), y (latitude), and date. Each dimension has from, to, offset, delta, refsys, point, and values. Here are their definitions (except point1):\n1 It is not clear what it really means. I have never had to pay attention to this parameter. So, its definition is not explained here. If you would like to investigate what it is, the best resource is probably this\n\nfrom: beginning index\n\nto: ending index\n\noffset: starting value\n\ndelta: step value\n\nrefsys: GCS or CRS for x and y (and can be Date for a date dimension)\n\nvalues: values of the dimension when objects are not regular\n\nIn order to understand the dimensions of stars objects, let’s first take a look at the figure below (Figure 6.1), which visualizes the tmax values on the 2D x-y surfaces of the stars objects at 10th date value (the spatial distribution of tmax at a particular date).\n\n\n\n\n\n\n\n\nFigure 6.1: Map of tmax values on August 10, 2009: 20 by 20 matrix of cells\n\n\n\n\n\nYou can consider the 2D x-y surface as a matrix, where the location of a cell is defined by the row number and column number. Since the from for x is 1 and to for x is 20, we have 20 columns. Similarly, since the from for y is 1 and to for y is 20, we have 20 rows. The offset value of the x and y dimensions is the longitude and latitude of the upper-left corner point of the upper left cell of the 2D x-y surface, respectively (the red circle i@fig-two-d-map).2 As refsys indicates, they are in NAD83 GCS. The longitude of the upper-left corner point of all the cells in the \\(j\\)th column (from the left) of the 2D x-y surface is -121.7291667 + \\((j-1)\\times\\) 0.0416667, where -121.7291667 is offset for x and 0.0416667 is delta for x. Similarly, the latitude of the upper-left corner point of all the cells in the \\(i\\)th row (from the top) of the 2D x-y surface is 46.6458333 +\\((i-1)\\times\\) -0.0416667, where 46.6458333 is offset for y and -0.0416667 is delta for y.\n2 This changes depending on the delta of x and y. If both of the delta for x and y are positive, then the offset value of the x and y dimensions are the longitude and latitude of the upper-left corner point of the lower-left cell of the 2D x-y surface.The dimension characteristics of x and y are shared by all the layers across the date dimension, and a particular combination of x and y indexes refers to exactly the same location on the earth in all the layers across dates (of course). In the date dimension, we have 10 date values since the from for date is 1 and to for date is 10. The refsys of the date dimension is Date. Since the offset is 2009-08-11 and delta is 1, \\(k\\)th layer represents tmax values for August \\(11+k-1\\), 2009.\nPutting all this information together, we have 20 by 20 x-y surfaces stacked over the date dimension (10 layers), thus making up a 20 by 20 by 10 three-dimensional array (or cube) as shown in the figure below (Figure 6.2).\n\n\n\n\n\n\n\nFigure 6.2: Visual illustration of stars data structure\n\n\n\n\nRemember that prcp_tmax_PRISM_m8_y09 also has another attribute (ppt) which is structured exactly the same way as tmax. So, prcp_tmax_PRISM_m8_y09 basically has four dimensions: attribute, x, y, and date.\nIt is not guaranteed that all the dimensions are regularly spaced or timed. For an irregular dimension, dimension values themselves are stored in values instead of using indexes, offset, and delta to find dimension values. For example, if you observe satellite data with 6-day gaps sometimes and 7-day gaps other times, then the date dimension would be irregular. We will see a made-up example of irregular time dimension in Section 6.5.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#some-basic-operations-on-stars-objects",
    "href": "chapters/06-stars.html#some-basic-operations-on-stars-objects",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.2 Some basic operations on stars objects",
    "text": "6.2 Some basic operations on stars objects\n\n6.2.1 Create a new attribute and drop a attribute\nYou can add a new attribute to a stars object just like you add a variable to a data.frame.\n\n#--- define a new variable which is tmax - 20 ---#\nprcp_tmax_PRISM_m8_y09$tmax_less_20 &lt;- prcp_tmax_PRISM_m8_y09$tmax - 20\n\nAs you can see below, prcp_tmax_PRISM_m8_y09 now has tmax_less_20 as an attribute.\n\nprcp_tmax_PRISM_m8_y09\n\nstars object with 3 dimensions and 3 attributes\nattribute(s):\n                 Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt             0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax            1.833 17.55575 21.483 22.035435 26.54275 39.707\ntmax_less_20  -18.167 -2.44425  1.483  2.035435  6.54275 19.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nYou can drop an attribute by assining NULL to the attribute you want to drop.\n\nprcp_tmax_PRISM_m8_y09$tmax_less_20 &lt;- NULL\n\nprcp_tmax_PRISM_m8_y09\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\n\n6.2.2 Subset a stars object by index\nIn order to access the value of an attribute (say ppt) at a particular location at a particular time from the stars object (prcp_tmax_PRISM_m8_y09), you need to tell R that you are interested in the ppt attribute and specify the corresponding index of x, y, and date. Here, is how you get the ppt value of (3, 4) cell at date = 10.\n\nprcp_tmax_PRISM_m8_y09[\"ppt\", 3, 4, 10]\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median Mean 3rd Qu. Max.\nppt     0       0      0    0       0    0\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       3  3     -121.7  0.04167  NAD83 FALSE [x]\ny       4  4      46.65 -0.04167  NAD83 FALSE [y]\ndate   10 10 2009-08-11   1 days   Date    NA    \n\n\nThis is very much similar to accessing a value from a matrix except that we have more dimensions. The first argument selects the attribute of interest, 2nd x, 3rd y, and 4th date.\nOf course, you can subset a stars object to access the value of multiple cells like this:\n\nprcp_tmax_PRISM_m8_y09[\"ppt\", 3:6, 3:4, 5:10]\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.  Max.\nppt     0       0      0 0.043125       0 0.546\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       3  6     -121.7  0.04167  NAD83 FALSE [x]\ny       3  4      46.65 -0.04167  NAD83 FALSE [y]\ndate    5 10 2009-08-11   1 days   Date    NA    \n\n\n\n6.2.3 Set attribute names\nWhen you read a raster dataset from a GeoTIFF file, the attribute name is the file name by default. So, you will often encounter cases where you want to change the attribute name. You can set the name of attributes using setNames().\n\nprcp_tmax_PRISM_m8_y09_dif_names &lt;- setNames(prcp_tmax_PRISM_m8_y09, c(\"precipitation\", \"maximum_temp\"))\n\nNote that the attribute names of prcp_tmax_PRISM_m8_y09 (original datar) have not changed after this operation (just like dplyr::mutate() or other dplyr functions do):\n\nprcp_tmax_PRISM_m8_y09\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nIf you want to reflect changes in the variable names while keeping the same object name, you need to assign the output of setNames() to the object as follows:\n\n# the codes in the rest of this chapter use \"precipitation\" and \"tmax\" as the variables names, not these ones\nprcp_tmax_PRISM_m8_y09 &lt;- setNames(prcp_tmax_PRISM_m8_y09, c(\"precipitation\", \"maximum_temp\"))\n\nWe will be using this function in Section 6.7.2.\n\n6.2.4 Get the coordinate reference system\nYou can get the CRS of a stars object using sf::st_crs(), which is actually the same function name that we used to extract the CRS of an sf object.\n\nsf::st_crs(prcp_tmax_PRISM_m8_y09)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nYou will use this to change the projection of vector datasets when you interact them:\n\ncrop the stars raster dataset to the spatial extent of sf objects\nextract values from the stars raster dataset for sf objects\n\nas we will see in Chapter 5. Notice also that we used exactly the same function name (sf::st_crs()) to get the CRS of sf objects (see Chapter 2).\n\n6.2.5 Get the dimension characteristics and values\nYou can access these dimension values using st_dimensions().\n\n#--- get dimension characteristics ---#\n(\n  dim_prcp_tmin &lt;- stars::st_dimensions(prcp_tmax_PRISM_m8_y09)\n)\n\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nThe following shows what we can get from this object.\n\nstr(dim_prcp_tmin)\n\nList of 3\n $ x   :List of 7\n  ..$ from  : num 1\n  ..$ to    : num 20\n  ..$ offset: num -122\n  ..$ delta : num 0.0417\n  ..$ refsys:List of 2\n  .. ..$ input: chr \"NAD83\"\n  .. ..$ wkt  : chr \"GEOGCRS[\\\"NAD83\\\",\\n    DATUM[\\\"North American Datum 1983\\\",\\n        ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222\"| __truncated__\n  .. ..- attr(*, \"class\")= chr \"crs\"\n  ..$ point : logi FALSE\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n $ y   :List of 7\n  ..$ from  : num 1\n  ..$ to    : num 20\n  ..$ offset: num 46.6\n  ..$ delta : num -0.0417\n  ..$ refsys:List of 2\n  .. ..$ input: chr \"NAD83\"\n  .. ..$ wkt  : chr \"GEOGCRS[\\\"NAD83\\\",\\n    DATUM[\\\"North American Datum 1983\\\",\\n        ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222\"| __truncated__\n  .. ..- attr(*, \"class\")= chr \"crs\"\n  ..$ point : logi FALSE\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n $ date:List of 7\n  ..$ from  : num 1\n  ..$ to    : int 10\n  ..$ offset: Date[1:1], format: \"2009-08-11\"\n  ..$ delta : 'difftime' num 1\n  .. ..- attr(*, \"units\")= chr \"days\"\n  ..$ refsys: chr \"Date\"\n  ..$ point : logi NA\n  ..$ values: NULL\n  ..- attr(*, \"class\")= chr \"dimension\"\n - attr(*, \"raster\")=List of 4\n  ..$ affine     : num [1:2] 0 0\n  ..$ dimensions : chr [1:2] \"x\" \"y\"\n  ..$ curvilinear: logi FALSE\n  ..$ blocksizes : int [1:10, 1:2] 20 20 20 20 20 20 20 20 20 20 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : NULL\n  .. .. ..$ : chr [1:2] \"x\" \"y\"\n  ..- attr(*, \"class\")= chr \"stars_raster\"\n - attr(*, \"class\")= chr \"dimensions\"\n\n\nFor example, you can get offset of x as follows:\n\ndim_prcp_tmin$x$offset\n\n[1] -121.7292\n\n\nYou can extract dimension values using stars::st_get_dimension_values(). For example, to get values of date,\n\n#--- get date values ---#\nstars::st_get_dimension_values(prcp_tmax_PRISM_m8_y09, \"date\")\n\n [1] \"2009-08-11\" \"2009-08-12\" \"2009-08-13\" \"2009-08-14\" \"2009-08-15\"\n [6] \"2009-08-16\" \"2009-08-17\" \"2009-08-18\" \"2009-08-19\" \"2009-08-20\"\n\n\nThis can be handy as you will see in Section 8.4.2. Later in Section 6.5, we will learn how to set dimensions using stars::st_set_dimensions().\n\n6.2.6 Attributes to dimensions, and vice versa\nYou can make attributes to dimensions using base::merge().\n\n(\n  prcp_tmax_four &lt;- base::merge(prcp_tmax_PRISM_m8_y09)\n)\n\nstars object with 4 dimensions and 1 attribute\nattribute(s):\n          Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nppt.tmax     0       0 11.799 11.66388  21.535 39.707\ndimension(s):\n           from to     offset    delta refsys point     values x/y\nx             1 20     -121.7  0.04167  NAD83 FALSE       NULL [x]\ny             1 20      46.65 -0.04167  NAD83 FALSE       NULL [y]\ndate          1 10 2009-08-11   1 days   Date    NA       NULL    \nattributes    1  2         NA       NA     NA    NA ppt , tmax    \n\n\nAs you can see, the new stars object has an additional dimension called attributes, which represents attributes and has two dimension values: the first for ppt and second for tmax. Now, if you want to access ppt, you can do the following:\n\nprcp_tmax_four[, , , , \"ppt\"]\n\nstars object with 4 dimensions and 1 attribute\nattribute(s):\n          Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nppt.tmax     0       0      0 1.292334   0.011 30.851\ndimension(s):\n           from to     offset    delta refsys point values x/y\nx             1 20     -121.7  0.04167  NAD83 FALSE   NULL [x]\ny             1 20      46.65 -0.04167  NAD83 FALSE   NULL [y]\ndate          1 10 2009-08-11   1 days   Date    NA   NULL    \nattributes    1  1         NA       NA     NA    NA    ppt    \n\n\nWe can do this because the merge kept the attribute names as dimension values as you can see in values.\nYou can revert it back to the original state using base::split(). Since we want the fourth dimension to dissolve,\n\nbase::split(prcp_tmax_four, 4)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median      Mean  3rd Qu.   Max.\nppt   0.000  0.00000  0.000  1.292334  0.01100 30.851\ntmax  1.833 17.55575 21.483 22.035435 26.54275 39.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#quick-visualization-for-exploration",
    "href": "chapters/06-stars.html#quick-visualization-for-exploration",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.3 Quick visualization for exploration",
    "text": "6.3 Quick visualization for exploration\nYou can use plot() to have a quick static map and tmap::mapview() or the tmap package for interactive views.\n\nplot(prcp_tmax_PRISM_m8_y09[\"tmax\", , , ])\n\n\n\n\n\n\n\nIt only plots the first attribute if the stars object has multiple attributes:\n\nplot(prcp_tmax_PRISM_m8_y09)\n\n\n\n\n\n\n\n, which is identical with this:\n\nplot(prcp_tmax_PRISM_m8_y09[\"ppt\", , , ])",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#sec-read-write-stars",
    "href": "chapters/06-stars.html#sec-read-write-stars",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.4 Reading and writing raster data",
    "text": "6.4 Reading and writing raster data\nThere are so many formats in which raster data is stored. Some of the common ones include GeoTIFF, netCDF, GRIB. All the available GDAL drivers for reading and writing raster data can be found by the following code:\n\nsf::st_drivers(what = \"raster\") %&gt;% DT::datatable()\n\n\n6.4.1 Reading raster data\nYou can use stars::read_stars() to read a raster data file. It is very unlikely that the raster file you are trying to read is not in one of the supported formats.\nFor example, you can read a GeoTIFF file as follows:\n\n(\n  ppt_m1_y09_stars &lt;- stars::read_stars(\"Data/PRISM_ppt_y2009_m1.tif\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to offset    delta refsys point\nx       1 1405   -125  0.04167  NAD83 FALSE\ny       1  621  49.94 -0.04167  NAD83 FALSE\nband    1   31     NA       NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090101_bil,...,PRISM_ppt_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n\n\nThis one imports a raw PRISM data stored as a BIL file.\n\n(\n  prism_tmax_20180701 &lt;- stars::read_stars(\"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\")\n)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n                                 Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_201...  3.623  25.318 31.596 29.50749  33.735 48.008\n                                  NA's\nPRISM_tmax_stable_4kmD2_201...  390874\ndimension(s):\n  from   to offset    delta refsys x/y\nx    1 1405   -125  0.04167  NAD83 [x]\ny    1  621  49.94 -0.04167  NAD83 [y]\n\n\nYou can import multiple raster data files into one stars object by simply supplying a vector of file names:\n\nfiles &lt;-\n  c(\n    \"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\",\n    \"Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil\"\n  )\n\n(\n  prism_tmax_201807_two &lt;- stars::read_stars(files)\n)\n\nstars object with 2 dimensions and 2 attributes\nattribute(s):\n                                 Min. 1st Qu. Median     Mean 3rd Qu.   Max.\n1_bil/PRISM_tmax_stable_4km...  3.623  25.318 31.596 29.50749  33.735 48.008\n2_bil/PRISM_tmax_stable_4km...  0.808  26.505 30.632 29.93235  33.632 47.999\n                                  NA's\n1_bil/PRISM_tmax_stable_4km...  390874\n2_bil/PRISM_tmax_stable_4km...  390874\ndimension(s):\n  from   to offset    delta refsys x/y\nx    1 1405   -125  0.04167  NAD83 [x]\ny    1  621  49.94 -0.04167  NAD83 [y]\n\n\nAs you can see, each file becomes an attribute. It is more convenient to have them as layers stacked along the third dimension. To do that you can add along = 3 option as follows:\n\n(\n  prism_tmax_201807_two &lt;- stars::read_stars(files, along = 3)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                                 Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_201...  4.693 19.5925 23.653 22.05224 24.5945 33.396\n                                 NA's\nPRISM_tmax_stable_4kmD2_201...  60401\ndimension(s):\n        from   to offset    delta refsys\nx          1 1405   -125  0.04167  NAD83\ny          1  621  49.94 -0.04167  NAD83\nnew_dim    1    2     NA       NA     NA\n                                                                                values\nx                                                                                 NULL\ny                                                                                 NULL\nnew_dim 1_bil/PRISM_tmax_stable_4kmD2_20180701, 2_bil/PRISM_tmax_stable_4kmD2_20180702\n        x/y\nx       [x]\ny       [y]\nnew_dim    \n\n\n\nNote that while the GeoTIFF format (and many other formats) can store multi-band (multi-layer) raster data allowing for an additional dimension beyond x and y, it does not store the values of the dimension like the date dimension we saw in prcp_tmax_PRISM_m8_y09. So, when you read a multi-layer raster data saved as a GeoTIFF, the third dimension of the resulting stars object will always be called band without any explicit time information. On the other hand, netCDF files are capable of storing the time dimension values. So, when you read a netCDF file with valid time dimension, you will have time dimension when it is read.\n\n(\n  stars::read_ncdf(system.file(\"nc/bcsd_obs_1999.nc\", package = \"stars\"))\n)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n                Min.   1st Qu.   Median      Mean   3rd Qu.      Max. NA's\npr [mm/m]  0.5900000 56.139999 81.88000 101.26433 121.07250 848.54999 7116\ntas [C]   -0.4209678  8.898887 15.65763  15.48932  21.77979  29.38581 7116\ndimension(s):\n          from to offset delta  refsys                    values x/y\nlongitude    1 81    -85 0.125  WGS 84                      NULL [x]\nlatitude     1 33     33 0.125  WGS 84                      NULL [y]\ntime         1 12     NA    NA POSIXct 1999-01-31,...,1999-12-31    \n\n\nThe refsys of the time dimension is POSIXct, which is one of the date classes.\n\n6.4.2 Writing a stars object to a file\nYou can write a stars object to a file using stars::write_stars() using one of the GDAL drivers.\nLet’s save prcp_tmax_PRISM_m8_y09[\"tmax\",,,], which has date dimension whose refsys is Date.\n\nstars::write_stars(prcp_tmax_PRISM_m8_y09[\"tmax\", , , ], \"Data/tmax_m8_y09_from_stars.tif\")\n\nLet’s read the file we just saved.\n\nstars::read_stars(\"Data/tmax_m8_y09_from_stars.tif\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n                             Min.  1st Qu. Median     Mean  3rd Qu.   Max.\ntmax_m8_y09_from_stars.tif  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to offset    delta refsys point x/y\nx       1 20 -121.7  0.04167  NAD83 FALSE [x]\ny       1 20  46.65 -0.04167  NAD83 FALSE [y]\nband    1 10     NA       NA     NA    NA    \n\n\nNotice that the third dimension is now called band and all the date information is lost. The loss of information happened when we saved prcp_tmax_PRISM_m8_y09[\"tmax\",,,] as a GeoTIFF file. One easy way to avoid this problem is to just save a stars object as an R dataset.\n\n#--- save it as an rds file ---#\nsaveRDS(prcp_tmax_PRISM_m8_y09[\"tmax\", , , ], \"Data/tmax_m8_y09_from_stars.rds\")\n\n\n#--- read it back ---#\nreadRDS(\"Data/tmax_m8_y09_from_stars.rds\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n       Min.  1st Qu. Median     Mean  3rd Qu.   Max.\ntmax  1.833 17.55575 21.483 22.03543 26.54275 39.707\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nAs you can see, date information is retained. So, if you are the only one who uses this data or all of your team members use R, then this is a nice solution to the problem. At the moment, it is not possible to use stars::write_stars() to write to a netCDF file that supports the third dimension as time. However, this may not be the case for a long time (See the discussion here).",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#sec-stars-set-time",
    "href": "chapters/06-stars.html#sec-stars-set-time",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.5 Setting the time dimension manually",
    "text": "6.5 Setting the time dimension manually\nFor this section, we will use PRISM precipitation data for U.S. for January, 2009.\n\n(\n  ppt_m1_y09_stars &lt;- stars::read_stars(\"Data/PRISM_ppt_y2009_m1.tif\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to offset    delta refsys point\nx       1 1405   -125  0.04167  NAD83 FALSE\ny       1  621  49.94 -0.04167  NAD83 FALSE\nband    1   31     NA       NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090101_bil,...,PRISM_ppt_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n\n\nAs you can see, when you read a GeoTIFF file, the third dimension will always be called band because GeoTIFF format does not support dimension values for the third dimension.3\n3 In Section 6.1, we read an rds, which is a stars object with dimension name set manually.You can use st_set_dimension() to set the third dimension (called band) as the time dimension using Date object. This can be convenient when you would like to filter the data by date using filter() as we will see later.\nFor ppt_m1_y09_stars, precipitation is observed on a daily basis from January 1, 2009 to January 31, 2009, where the band value of x corresponds to January x, 2009. So, we can first create a vector of dates as follows (If you are not familiar with Dates and the lubridate pacakge, this is a good resource to learn them.):\n\n#--- starting date ---#\nstart_date &lt;- lubridate::ymd(\"2009-01-01\")\n\n#--- ending date ---#\nend_date &lt;- lubridate::ymd(\"2009-01-31\")\n\n#--- sequence of dates  ---#\n(\ndates_ls_m1 &lt;- seq(start_date, end_date, \"days\")\n)\n\n [1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n [6] \"2009-01-06\" \"2009-01-07\" \"2009-01-08\" \"2009-01-09\" \"2009-01-10\"\n[11] \"2009-01-11\" \"2009-01-12\" \"2009-01-13\" \"2009-01-14\" \"2009-01-15\"\n[16] \"2009-01-16\" \"2009-01-17\" \"2009-01-18\" \"2009-01-19\" \"2009-01-20\"\n[21] \"2009-01-21\" \"2009-01-22\" \"2009-01-23\" \"2009-01-24\" \"2009-01-25\"\n[26] \"2009-01-26\" \"2009-01-27\" \"2009-01-28\" \"2009-01-29\" \"2009-01-30\"\n[31] \"2009-01-31\"\n\n\nWe can then use stars::st_set_dimensions() to change the third dimension to the dimension of date.\n\nstars::st_set_dimensions(\n  stars object,\n  dimension,\n  values = dimension values,\n  names = name of the dimension\n )\n\n\n(\n  ppt_m1_y09_stars &lt;-\n    stars::st_set_dimensions(\n      ppt_m1_y09_stars,\n      3,\n      values = dates_ls_m1,\n      names = \"date\"\n    )\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   31 2009-01-01   1 days   Date    NA    \n\n\nAs you can see, the third dimension has become date. The value of offset for the date dimension has become 2009-01-01 meaning the starting date value is now 2009-01-01. Further, the value of delta is now 1 days, so date dimension of x corresponds to 2009-01-01 + x - 1 = 2009-01-x.\nNote that the date dimension does not have to be regularly spaced. For example, you may have satellite images available for your area with a 5-day interval sometimes and a 6-day interval other times. This is perfectly fine. As an illustration, I will create a wrong sequence of dates for this data with a 2-day gap in the middle and assign them to the date dimension to see what happens.\n\n#--- 2009-01-23 removed and 2009-02-01 added ---#\n(\n  dates_ls_wrong &lt;- c(seq(start_date, end_date, \"days\")[-23], lubridate::ymd(\"2009-02-01\"))\n)\n\n [1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n [6] \"2009-01-06\" \"2009-01-07\" \"2009-01-08\" \"2009-01-09\" \"2009-01-10\"\n[11] \"2009-01-11\" \"2009-01-12\" \"2009-01-13\" \"2009-01-14\" \"2009-01-15\"\n[16] \"2009-01-16\" \"2009-01-17\" \"2009-01-18\" \"2009-01-19\" \"2009-01-20\"\n[21] \"2009-01-21\" \"2009-01-22\" \"2009-01-24\" \"2009-01-25\" \"2009-01-26\"\n[26] \"2009-01-27\" \"2009-01-28\" \"2009-01-29\" \"2009-01-30\" \"2009-01-31\"\n[31] \"2009-02-01\"\n\n\nNow assign these date values to ppt_m1_y09_stars:\n\n#--- set date values ---#\n(\n  ppt_m1_y09_stars_wrong &lt;-\n    stars::st_set_dimensions(\n      ppt_m1_y09_stars,\n      3,\n      values = dates_ls_wrong,\n      names = \"date\"\n    )\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to offset    delta refsys point                    values x/y\nx       1 1405   -125  0.04167  NAD83 FALSE                      NULL [x]\ny       1  621  49.94 -0.04167  NAD83 FALSE                      NULL [y]\ndate    1   31     NA       NA   Date    NA 2009-01-01,...,2009-02-01    \n\n\nSince the step between the date values is no longer \\(1\\) day for the entire sequence, the value of delta is now NA. However, notice that the value of date is no longer NULL. Since the date is not regular, you cannot represent date using three values (from, to, and delta) any more, and date values for each observation have to be stored now.\nFinally, note that just applying stars::st_set_dimensions() to a stars object does not change the dimension of the stars object (just like setNames() as we discussed above).\n\nppt_m1_y09_stars\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   31 2009-01-01   1 days   Date    NA    \n\n\nAs you can see, the date dimension has not been altered. You need to assign the results of stars::st_set_dimensions() to a stars object to see the changes in the dimension reflected just like we did above with the right date values.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#sec-dplyr-op",
    "href": "chapters/06-stars.html#sec-dplyr-op",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.6 dplyr-like operations",
    "text": "6.6 dplyr-like operations\nYou can use the dplyr language to do basic data operations on stars objects.\n\n6.6.1 dplyr::filter()\n\nThe dplyr::filter() function allows you to subset data by dimension values: x, y, and band (here date).\n\nspatial filtering\n\n#--- longitude greater than -100 ---#\ndplyr::filter(ppt_m1_y09_stars, x &gt; -100) %&gt;% plot()\n\n\n\n\n\n\n#--- latitude less than 40 ---#\ndplyr::filter(ppt_m1_y09_stars, y &lt; 40) %&gt;% plot()\n\n\n\n\n\n\n\n\ntemporal filtering\nFinally, since the date dimension is in Date, you can use Date math to filter the data.4\n4 This is possible only because we have assigned date values to the band dimension above.\n#--- dates after 2009-01-15  ---#\ndplyr::filter(ppt_m1_y09_stars, date &gt; lubridate::ymd(\"2009-01-21\")) %&gt;% plot()\n\n\n\n\n\n\n\n\nfilter by attribute?\nIn case you are wondering, filtering by attribute is not possible. This makes sense, as doing so would disrupt the regular grid structure of raster data.\n\ndplyr::filter(ppt_m1_y09_stars, ppt &gt; 20)\n\nError in `map2_int()`:\nℹ In index: 1.\nCaused by error in `glubort()`:\n! ``~``, `ppt &gt; 20` must refer to exactly one dimension, not ``\n\n\n\n6.6.2 dplyr::select()\n\nThe dply::select() function lets you pick certain attributes.\n\ndplyr::select(prcp_tmax_PRISM_m8_y09, ppt)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nppt     0       0      0 1.292334   0.011 30.851\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\n\n6.6.3 dplyr::mutate()\n\nYou can mutate attributes using the dplyr::mutate() function. For example, this can be useful to calculate NDVI in a stars object that has Red and NIR (spectral reflectance measurements in the red and near-infrared regions) as attributes. Here, we just simply convert the unit of precipitation from mm to inches.\n\n#--- mm to inches ---#\ndplyr::mutate(prcp_tmax_PRISM_m8_y09, ppt = ppt * 0.0393701)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s):\n       Min.  1st Qu. Median       Mean      3rd Qu.      Max.\nppt   0.000  0.00000  0.000  0.0508793 4.330711e-04  1.214607\ntmax  1.833 17.55575 21.483 22.0354348 2.654275e+01 39.707001\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\n\n6.6.4 dplyr::pull()\n\nYou can extract attribute values using dplyr::pull().\n\n#--- tmax values of the 1st date layer ---#\ndplyr::pull(prcp_tmax_PRISM_m8_y09[\"tmax\", , , 1], \"tmax\")\n\n, , 1\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] 25.495 27.150 22.281 19.553 21.502 19.616 21.664 24.458 21.712 19.326\n [2,] 27.261 27.042 21.876 19.558 19.141 19.992 18.694 24.310 21.094 20.032\n [3,] 27.568 21.452 23.220 19.428 18.974 19.366 20.293 24.418 20.190 21.127\n [4,] 23.310 20.357 20.111 21.380 19.824 19.638 22.135 20.885 20.121 18.396\n [5,] 19.571 20.025 18.407 19.142 19.314 22.759 22.652 19.566 18.817 16.065\n [6,] 17.963 16.581 17.121 16.716 19.680 20.521 17.991 18.007 17.430 14.586\n [7,] 20.911 19.202 16.309 14.496 17.429 19.083 18.509 18.723 17.645 16.415\n [8,] 17.665 16.730 17.691 14.370 16.849 18.413 17.787 20.000 19.319 18.401\n [9,] 16.795 18.091 20.460 16.405 18.331 19.005 19.142 21.226 21.184 19.520\n[10,] 19.208 19.624 17.210 19.499 18.492 20.854 18.684 19.811 22.058 19.923\n[11,] 23.148 18.339 19.676 20.674 18.545 21.126 19.013 19.722 21.843 21.271\n[12,] 23.254 21.279 21.921 19.894 19.445 21.499 19.765 20.742 21.560 22.989\n[13,] 23.450 21.956 19.813 18.970 20.173 20.567 21.152 20.932 19.836 20.347\n[14,] 24.075 21.120 20.166 19.177 20.428 20.908 21.060 19.832 19.764 19.981\n[15,] 24.318 20.943 20.024 20.022 19.040 19.773 20.452 20.152 20.321 20.304\n[16,] 22.538 19.461 20.100 21.149 19.958 20.486 20.535 20.445 21.564 21.493\n[17,] 20.827 20.192 21.165 22.369 21.488 22.031 21.552 21.089 21.687 23.375\n[18,] 21.089 21.451 22.692 21.793 22.160 23.049 22.562 22.738 23.634 24.697\n[19,] 22.285 22.992 23.738 23.497 24.255 25.177 25.411 24.324 24.588 26.032\n[20,] 23.478 23.584 24.589 24.719 26.114 26.777 27.310 26.643 26.516 26.615\n       [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\n [1,] 24.845 23.211 21.621 23.180 21.618 21.322 22.956 23.749 23.162 25.746\n [2,] 24.212 21.820 21.305 22.960 22.806 22.235 23.604 23.689 25.597 25.933\n [3,] 20.879 20.951 22.280 23.536 23.455 22.537 24.760 23.587 26.170 24.764\n [4,] 17.435 18.681 22.224 24.122 25.828 25.604 25.298 22.817 24.438 24.835\n [5,] 14.186 17.789 20.624 23.416 26.059 27.571 25.158 24.201 26.001 26.235\n [6,] 10.188 15.632 19.907 22.660 25.268 27.469 27.376 27.488 27.278 27.558\n [7,] 14.797 15.933 19.204 21.641 23.107 25.626 26.990 25.838 26.906 27.247\n [8,] 17.325 18.299 19.691 21.553 21.840 23.754 26.099 25.270 26.282 26.981\n [9,] 19.322 19.855 20.489 22.597 23.614 25.873 26.906 26.368 26.332 25.844\n[10,] 20.241 21.800 22.111 24.128 25.765 27.105 27.200 25.491 26.306 25.663\n[11,] 23.398 24.090 24.884 25.596 26.545 27.014 26.464 25.708 25.742 25.336\n[12,] 22.576 21.996 23.874 26.447 26.955 26.871 25.533 25.576 25.610 25.902\n[13,] 22.023 22.358 24.996 26.185 27.249 25.617 25.623 25.600 25.433 26.681\n[14,] 20.974 23.533 25.388 25.975 27.316 26.199 26.090 25.920 25.767 27.956\n[15,] 20.982 23.632 24.703 25.539 26.515 27.133 27.407 27.518 27.149 28.506\n[16,] 22.500 24.012 25.282 25.751 25.212 25.290 26.058 28.258 28.290 29.842\n[17,] 23.024 24.381 25.157 25.259 24.829 24.183 25.632 26.947 28.601 29.589\n[18,] 24.016 24.425 24.965 24.930 24.482 23.274 25.412 26.733 28.494 29.656\n[19,] 24.065 24.105 24.145 24.318 23.912 22.782 25.039 26.554 28.184 29.012\n[20,] 23.979 24.321 23.477 22.135 22.395 22.189 24.944 26.542 27.923 28.849",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#merging-stars-objects-using-c-and-starsst_mosaic",
    "href": "chapters/06-stars.html#merging-stars-objects-using-c-and-starsst_mosaic",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.7 Merging stars objects using c() and stars::st_mosaic()\n",
    "text": "6.7 Merging stars objects using c() and stars::st_mosaic()\n\n\n6.7.1 Merging stars objects along the third dimension (band)\nHere we learn how to merge multiple stars objects that have\n\nthe same attributes\nthe same spatial extent and resolution\n\ndifferent bands (dates here)\n\nFor example, consider merging PRISM precipitation data in January and February. Both of them have exactly the same spatial extent and resolutions and represent the same attribute (precipitation). However, they differ in the third dimension (date). So, you are trying to stack data of the same attributes along the third dimension (date) while making sure that spatial correspondence is maintained. This merge is kind of like rbind() that stacks multiple data.frames vertically while making sure the variables are aligned correctly.\nLet’s import the PRISM precipitation data for February, 2009.\n\n#--- read the February ppt data ---#\n(\n  ppt_m2_y09_stars &lt;- stars::read_stars(\"Data/PRISM_ppt_y2009_m2.tif\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m2.tif     0       0      0 0.1855858       0 11.634 60401\ndimension(s):\n     from   to offset    delta refsys point\nx       1 1405   -125  0.04167  NAD83 FALSE\ny       1  621  49.94 -0.04167  NAD83 FALSE\nband    1   28     NA       NA     NA    NA\n                                                                          values\nx                                                                           NULL\ny                                                                           NULL\nband PRISM_ppt_stable_4kmD2_20090201_bil,...,PRISM_ppt_stable_4kmD2_20090228_bil\n     x/y\nx    [x]\ny    [y]\nband    \n\n\nNote here that the third dimension of ppt_m2_y09_stars has not been changed to date.\nNow, let’s try to merge the two:\n\n#--- combine the two ---#\n(\n  ppt_m1_to_m2_y09_stars &lt;- c(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   59 2009-01-01   1 days   Date    NA    \n\n\nAs you noticed, the second object (ppt_m2_y09_stars) was assumed to have the same date characteristics as the first one: the data in February is observed daily (delta is 1 day). This causes no problem in this instance as the February data is indeed observed daily starting from 2009-02-01. However, be careful if you are appending the data that does not start from 1 day after (or more generally delta for the time dimension) the first data or the data that does not follow the same observation interval.\nFor this reason, it is advisable to first set the date values if it has not been set. Pretend that the February data actually starts from 2009-02-02 to 2009-03-01 to see what happens when the regular interval (delta) is not kept after merging.\n\n#--- starting date ---#\nstart_date &lt;- lubridate::ymd(\"2009-02-01\")\n\n#--- ending date ---#\nend_date &lt;- lubridate::ymd(\"2009-02-28\")\n\n#--- sequence of dates  ---#\ndates_ls &lt;- seq(start_date, end_date, \"days\")\n\n#--- pretend the data actually starts from `2009-02-02` to `2009-03-01` ---#\n(\n  ppt_m2_y09_stars &lt;- \n    stars::st_set_dimensions(\n      ppt_m2_y09_stars,\n      3,\n      values = c(dates_ls[-1], lubridate::ymd(\"2009-03-01\")),\n      name = \"date\"\n    )\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m2.tif     0       0      0 0.1855858       0 11.634 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   28 2009-02-02   1 days   Date    NA    \n\n\nIf you merge the two,\n\nc(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n                        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif     0       0  0.436 3.543222  3.4925 56.208 60401\ndimension(s):\n     from   to offset    delta refsys point                    values x/y\nx       1 1405   -125  0.04167  NAD83 FALSE                      NULL [x]\ny       1  621  49.94 -0.04167  NAD83 FALSE                      NULL [y]\ndate    1   59     NA       NA   Date    NA 2009-01-01,...,2009-03-01    \n\n\nThe date dimension does not have delta any more, and correctly so because there is a one-day gap between the end date of the first stars object (\"2009-01-31\") and the start date of the second stars object (\"2009-02-02\"). So, all the date values are now stored in values.\n\n6.7.2 Merging stars objects of different attributes\nHere we learn how to merge multiple stars objects that have\n\n\ndifferent attributes\nthe same spatial extent and resolution\nthe same bands (dates here)\n\nFor example, consider merging PRISM precipitation and tmax data in January. Both of them have exactly the same spatial extent and resolutions and the date characteristics (starting and ending on the same dates with the same time interval). However, they differ in what they represent: precipitation and tmax. This merge is kind of like cbind() that combines multiple data.frames of different variables while making sure the observation correspondence is correct.\nLet’s read the daily tmax data for January, 2009:\n\n(\n  tmax_m8_y09_stars &lt;- \n    stars::read_stars(\"Data/PRISM_tmax_y2009_m1.tif\") %&gt;%\n    #--- change the attribute name ---#\n    setNames(\"tmax\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to offset    delta refsys point\nx       1 1405   -125  0.04167  NAD83 FALSE\ny       1  621  49.94 -0.04167  NAD83 FALSE\nband    1   31     NA       NA     NA    NA\n                                                                            values\nx                                                                             NULL\ny                                                                             NULL\nband PRISM_tmax_stable_4kmD2_20090101_bil,...,PRISM_tmax_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n\n\nNow, let’s merge the PRISM ppt and tmax data in January, 2009.\n\nc(ppt_m1_y09_stars, tmax_m8_y09_stars)\n\nError in c.stars(ppt_m1_y09_stars, tmax_m8_y09_stars): don't know how to merge arrays: please specify parameter along\n\n\nOops. Well, the problem is that the third dimension of the two objects is not the same. Even though we know that the xth element of their third dimension represent the same thing, they look different to R’s eyes. So, we first need to change the third dimension of tmax_m8_y09_stars to be consistent with the third dimension of ppt_m1_y09_stars (dates_ls_m1 was defined in Section 6.5).\n\n(\n  tmax_m8_y09_stars &lt;- stars::st_set_dimensions(tmax_m8_y09_stars, 3, values = dates_ls_m1, names = \"date\")\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   31 2009-01-01   1 days   Date    NA    \n\n\nNow, we can merge the two.\n\n(\n  ppt_tmax_m8_y09_stars &lt;- c(ppt_m1_y09_stars, tmax_m8_y09_stars)\n)\n\nstars object with 3 dimensions and 2 attributes\nattribute(s), summary of first 1e+05 cells:\n                           Min. 1st Qu. Median      Mean 3rd Qu.   Max.  NA's\nPRISM_ppt_y2009_m1.tif    0.000   0.000  0.436  3.543222  3.4925 56.208 60401\ntmax                    -17.898  -7.761 -2.248 -3.062721  1.7180  7.994 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   31 2009-01-01   1 days   Date    NA    \n\n\nAs you can see, now we have another attribute called tmax.\n\n6.7.3 Merging stars objects of different spatial extents\nHere we learn how to merge multiple stars objects that have\n\nthe same attributes\n\ndifferent spatial extent but same resolution5\n\nthe same bands (dates here)\n\n5 Technically, you can actually merge stars objects of different spatial resolutions. But, you probably should not.Some times you have multiple separate raster datasets that have different spatial coverages and would like to combine them into one. You can do that using stars::st_mosaic().\nLet’s split tmax_m8_y09_stars into two parts (Figure 6.3 shows what they look like (only Jan 1, 1980)):\n\ntmax_1 &lt;- dplyr::filter(tmax_m8_y09_stars, x &lt;= -100)\ntmax_2 &lt;- dplyr::filter(tmax_m8_y09_stars, x &gt; -100)\n\n\n\n\n\nCodeg_1 &lt;- \n  ggplot() +\n  geom_stars(data = tmax_1[, , , 1]) +\n  theme_for_map +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  ggtitle(\"tmax_1\")\n\ng_2 &lt;- \n  ggplot() +\n  geom_stars(data = tmax_2[, , , 1]) +\n  theme_for_map +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  ggtitle(\"tmax_2\")\n\nlibrary(patchwork)\ng_1 + g_2\n\n\n\n\n\n\nFigure 6.3: Two spatially non-overlapping stars objects\n\n\n\n\nLet’s combine the two using stars::st_mosaic():\n\ntmax_combined &lt;- stars::st_mosaic(tmax_1, tmax_2)\n\nFigure 6.4 shows what the combined object looks like.\n\nCodeggplot() +\n  geom_stars(data = tmax_combined[, , , 1]) +\n  theme_for_map\n\n\n\n\n\n\nFigure 6.4: Map of the stars objects combined into one\n\n\n\n\n\nIt is okay to have the two stars objects to be combined have a spatial overlap. The following split creates two stars objects with a spatial overlap (Figure 6.5 shows what they look like).\n\ntmax_1 &lt;- dplyr::filter(tmax_m8_y09_stars, x &lt;= -100)\ntmax_2 &lt;- dplyr::filter(tmax_m8_y09_stars, x &gt; -110)\n\n\n\n\n\nCodeg_1 &lt;- \n  ggplot() +\n  geom_stars(data = tmax_m8_y09_stars[, , , 1], fill = NA) +\n  geom_stars(data = tmax_1[, , , 1]) +\n  theme_for_map +\n  ggtitle(\"tmax_1\") \n\ng_2 &lt;- \n  ggplot() +\n  geom_stars(data = tmax_m8_y09_stars[, , , 1], fill = NA) +\n  geom_stars(data = tmax_2[, , , 1]) +\n  theme_for_map +\n  ggtitle(\"tmax_2\")\n\nlibrary(patchwork)\ng_1 / g_2\n\n\n\n\n\n\nFigure 6.5: Two spatially overlapping stars objects\n\n\n\n\nAs you can see below, stars::st_mosaic() reconciles the spatial overlap between the two stars objects (Figure 6.6).\n\n(\n  tmax_combined &lt;- stars::st_mosaic(tmax_1, tmax_2)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n         Min. 1st Qu. Median      Mean 3rd Qu.  Max.  NA's\ntmax  -17.898  -7.761 -2.248 -3.062721   1.718 7.994 60401\ndimension(s):\n     from   to offset    delta refsys x/y\nx       1 1405   -125  0.04167  NAD83 [x]\ny       1  621  49.94 -0.04167  NAD83 [y]\nband    1   31     NA       NA     NA    \n\n\n\nCodeggplot() +\n  geom_stars(data = tmax_combined[, , , 1]) +\n  theme_for_map\n\n\n\n\n\n\nFigure 6.6: Map of the spatially-overlapping stars objects combined into one",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#apply-a-function-to-one-or-more-dimensions",
    "href": "chapters/06-stars.html#apply-a-function-to-one-or-more-dimensions",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.8 Apply a function to one or more dimensions",
    "text": "6.8 Apply a function to one or more dimensions\nAt times, you may want to apply a function to one or more dimensions of a stars object. For instance, you might want to calculate the total annual precipitation for each cell using daily precipitation data. To illustrate this, let’s create a single-attribute stars object that contains daily precipitation data for August 2009.\n\n(\n  ppt_m8_y09 &lt;- prcp_tmax_PRISM_m8_y09[\"ppt\"]\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n     Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nppt     0       0      0 1.292334   0.011 30.851\ndimension(s):\n     from to     offset    delta refsys point x/y\nx       1 20     -121.7  0.04167  NAD83 FALSE [x]\ny       1 20      46.65 -0.04167  NAD83 FALSE [y]\ndate    1 10 2009-08-11   1 days   Date    NA    \n\n\nLet’s find total precipitation for each cell in August in 2009 using stars::st_apply(), which works like this.\n\nstars::st_apply(\n  X = stars object,\n  MARGIN = margin,\n  FUN = function to apply\n)\n\nSince we want to sum the value of attribute (ppt) for each of the cells (each cell is represented by x-y), we specify MARGIN = c(\"x\", \"y\") and use sum() as the function (Figure 6.7 shows the results).\n\nmonthly_ppt &lt;-\n  stars::st_apply(\n    ppt_m8_y09,\n    MARGIN = c(\"x\", \"y\"),\n    FUN = sum\n  )\n\n\n\n\n\nCodeplot(monthly_ppt)\n\n\n\n\n\n\nFigure 6.7: Summed value of precipitation",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#sec-convert-to-rb",
    "href": "chapters/06-stars.html#sec-convert-to-rb",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.9 Convert from and to Raster\\(^*\\) or SpatRaster objects",
    "text": "6.9 Convert from and to Raster\\(^*\\) or SpatRaster objects\nWhen you need to convert a stars object to a Raster\\(^*\\) or SpatRaster object, you can use the as() function as follows:\n\n(\n  # to Raster* object\n  prcp_tmax_PRISM_m8_y09_rb &lt;- as(prcp_tmax_PRISM_m8_y09, \"Raster\")\n)\n\nclass      : RasterBrick \ndimensions : 20, 20, 400, 10  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=NAD83 +no_defs \nsource     : memory\nnames      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 \nmin values :       0,       0,       0,       0,       0,       0,       0,       0,       0,        0 \nmax values :   0.019,  18.888,  30.851,   4.287,   0.797,   0.003,   3.698,   1.801,   0.000,    0.000 \ntime       : 2009-08-11, 2009-08-12, 2009-08-13, 2009-08-14, 2009-08-15, 2009-08-16, 2009-08-17, 2009-08-18, 2009-08-19, 2009-08-20 \n\n(\n  # to SpatRaster\n  prcp_tmax_PRISM_m8_y09_sr &lt;- as(prcp_tmax_PRISM_m8_y09, \"SpatRaster\")\n)\n\nclass       : SpatRaster \ndimensions  : 20, 20, 10  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nnames       : date2~08-11, date2~08-12, date2~08-13, date2~08-14, date2~08-15, date2~08-16, ... \nmin values  :       0.000,       0.000,       0.000,       0.000,       0.000,       0.000, ... \nmax values  :       0.019,      18.888,      30.851,       4.287,       0.797,       0.003, ... \n\n\nAs you can see, date values in prcp_tmax_PRISM_m8_y09 appear in the time dimension of prcp_tmax_PRISM_m8_y09_rb (RasterBrick). However, in prcp_tmax_PRISM_m8_y09_sr (SpatRaster), date values appear in the names dimension with date added as prefix to the original date values.\nNote also that the conversion was done for only the ppt attribute. This is simply because the raster and terra package does not accommodate multiple attributes of 3-dimensional array. So, if you want a RasterBrick or SpatRaster of the precipitation data, then you need to do the following:\n\n(\n  # to RasterBrick\n  tmax_PRISM_m8_y09_rb &lt;- as(prcp_tmax_PRISM_m8_y09[\"ppt\", , , ], \"Raster\")\n)\n\nclass      : RasterBrick \ndimensions : 20, 20, 400, 10  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=NAD83 +no_defs \nsource     : memory\nnames      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 \nmin values :       0,       0,       0,       0,       0,       0,       0,       0,       0,        0 \nmax values :   0.019,  18.888,  30.851,   4.287,   0.797,   0.003,   3.698,   1.801,   0.000,    0.000 \ntime       : 2009-08-11, 2009-08-12, 2009-08-13, 2009-08-14, 2009-08-15, 2009-08-16, 2009-08-17, 2009-08-18, 2009-08-19, 2009-08-20 \n\n(\n  # to SpatRaster\n  tmax_PRISM_m8_y09_sr &lt;- as(prcp_tmax_PRISM_m8_y09[\"ppt\", , , ], \"SpatRaster\")\n)\n\nclass       : SpatRaster \ndimensions  : 20, 20, 10  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -121.7292, -120.8958, 45.8125, 46.64583  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nnames       : date2~08-11, date2~08-12, date2~08-13, date2~08-14, date2~08-15, date2~08-16, ... \nmin values  :       0.000,       0.000,       0.000,       0.000,       0.000,       0.000, ... \nmax values  :       0.019,      18.888,      30.851,       4.287,       0.797,       0.003, ... \n\n\nYou can convert a Raster\\(^*\\) object to a stars object using stars::st_as_stars() (you will see a use case in Section 8.4.2).\n\n(\n  tmax_PRISM_m8_y09_back_to_stars &lt;- stars::st_as_stars(tmax_PRISM_m8_y09_rb)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nlayer.1     0       0      0 1.292334   0.011 30.851\ndimension(s):\n     from to     offset    delta refsys x/y\nx       1 20     -121.7  0.04167  NAD83 [x]\ny       1 20      46.65 -0.04167  NAD83 [y]\nband    1 10 2009-08-11   1 days   Date    \n\n\nNotice that the original date values (stored in the date dimension) are recovered, but its dimension is now called just band.\nYou can do the same with a SpatRaster object.\n\n(\n  tmax_PRISM_m8_y09_back_to_stars &lt;- stars::st_as_stars(tmax_PRISM_m8_y09_sr)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n                Min. 1st Qu. Median     Mean 3rd Qu.   Max.\ndate2009-08-11     0       0      0 1.292334   0.011 30.851\ndimension(s):\n     from to offset    delta refsys                            values x/y\nx       1 20 -121.7  0.04167  NAD83                              NULL [x]\ny       1 20  46.65 -0.04167  NAD83                              NULL [y]\nband    1 10     NA       NA     NA date2009-08-11,...,date2009-08-20    \n\n\nNote that unlike the conversion from the RasterBrick object, the band dimension inherited values of the name dimension in tmax_PRISM_m8_y09_sr.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/06-stars.html#spatial-interactions-of-stars-and-sf-objects",
    "href": "chapters/06-stars.html#spatial-interactions-of-stars-and-sf-objects",
    "title": "6  Spatiotemporal Raster Data Handling with stars",
    "section": "\n6.10 Spatial interactions of stars and sf objects",
    "text": "6.10 Spatial interactions of stars and sf objects\n\n6.10.1 Spatial cropping (subsetting) to the area of interest\nIf the region of interest is smaller than the spatial extent of the stars raster data, there is no need to retain the irrelevant portions of the stars object. In such cases, you can crop the stars data to focus on the region of interest using sf::st_crop(). The general syntax of sf::st_crop() is:\n\n#--- NOT RUN ---#\nsf::st_crop(stars object, sf/bbox object)\n\nFor demonstration, we use PRISM tmax data for the U.S. for January 2019 as a stars object.\n\n(\ntmax_m8_y09_stars &lt;- \n  stars::read_stars(\"Data/PRISM_tmax_y2009_m8.tif\") %&gt;% \n  setNames(\"tmax\") %&gt;% \n  .[, , , 1:10] %&gt;%\n  stars::st_set_dimensions(\n    \"band\",\n    values = seq(\n      lubridate::ymd(\"2009-08-01\"),\n      lubridate::ymd(\"2009-08-10\"),\n      by = \"days\"\n    ), \n    name = \"date\"\n  )\n) \n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n        Min. 1st Qu. Median     Mean 3rd Qu.   Max.  NA's\ntmax  15.084 20.1775 22.584 23.86903 26.3285 41.247 60401\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx       1 1405       -125  0.04167  NAD83 FALSE [x]\ny       1  621      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   10 2009-08-01   1 days   Date    NA    \n\n\nThe region of interest is Michigan.\n\nMI_county_sf &lt;-\n  tigris::counties(state = \"Michigan\", cb = TRUE, progress_bar = FALSE) %&gt;% \n  #--- transform using the CRS of the PRISM stars data  ---#\n  sf::st_transform(sf::st_crs(tmax_m8_y09_stars))\n\nWe can crop the tmax data to the Michigan state border using st_crop() as follows:\n\n(\ntmax_MI &lt;- sf::st_crop(tmax_m8_y09_stars, MI_county_sf)\n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n        Min. 1st Qu. Median     Mean 3rd Qu.   Max.   NA's\ntmax  14.108  22.471 24.343 24.61851  26.208 35.373 206900\ndimension(s):\n     from   to     offset    delta refsys point x/y\nx     831 1023       -125  0.04167  NAD83 FALSE [x]\ny      41  198      49.94 -0.04167  NAD83 FALSE [y]\ndate    1   10 2009-08-01   1 days   Date    NA    \n\n\nNotice that from and to for x and y have changed to cover only the boundary box of the Michigan state border. Note that the values for the cells outside of the Michigan state border were set to NA. Figure 6.8 shows the cropping was successful.\n\nplot(tmax_MI[,,,1])\n\n\n\n\n\n\nFigure 6.8: PRISM tmax data cropped to the Michigan state border\n\n\n\n\nAlternatively, you could use [] like as follows to crop a stars object.\n\nplot(tmax_m8_y09_stars[MI_county_sf])\n\n\n\n\n\n\n\n:::{.callout-note title = “Use bounding box”} When using sf::st_crop(), it is much faster to crop to a bounding box instead of sf if that is satisfactory. For example, you may be cropping a stars object to speed up subsequent raster value extraction (see Section 9.2). In this case, it is far better to crop to the bounding box of the sf instead of sf. Confirm the speed difference between the two below:\n\ncrop_to_sf &lt;- sf::st_crop(tmax_m8_y09_stars, MI_county_sf)\n\ncrop_to_bbox &lt;- sf::st_crop(tmax_m8_y09_stars, sf::st_bbox(MI_county_sf))\n\nThe difference can be substantial especially when the number of observations are large in sf. :::\n\n6.10.2 Extracting values for points\nIn this section, we will learn how to extract cell values from raster layers as starts for spatial units represented as point sf data6.\n6 see Section 5.2 for a detailed explanation of what it means to extract raster values.\n6.10.2.1 Extraction using stars::st_extract()\n\nFor the illustrations in this section, we use the following datasets:\n\nPoints: Irrigation wells in Kansas\nRaster: daily PRISM tmax data for August, 2009\n\nPRISM tmax data\n\n(\n  tmax_m8_y09_stars &lt;-\n    stars::read_stars(\"Data/PRISM_tmax_y2009_m8.tif\") %&gt;%\n    setNames(\"tmax\") %&gt;%\n    .[, , , 1:10] %&gt;%\n    stars::st_set_dimensions(\n      \"band\",\n      values = seq(\n        lubridate::ymd(\"2009-08-01\"),\n        lubridate::ymd(\"2009-08-10\"),\n        by = \"days\"\n      ),\n      name = \"date\"\n    )\n) \n\nIrrigation wells in Kansas:\n\n#--- read in the KS points data ---#\n(\nKS_wells &lt;- readRDS(\"Data/Chap_5_wells_KS.rds\")\n)\n\nSimple feature collection with 37647 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id                   geometry\n1        1 POINT (-100.4423 37.52046)\n2        3 POINT (-100.7118 39.91526)\n3        5 POINT (-99.15168 38.48849)\n4        7 POINT (-101.8995 38.78077)\n5        8  POINT (-100.7122 38.0731)\n6        9 POINT (-97.70265 39.04055)\n7       11 POINT (-101.7114 39.55035)\n8       12 POINT (-95.97031 39.16121)\n9       15 POINT (-98.30759 38.26787)\n10      17 POINT (-100.2785 37.71539)\n\n\nFigure 6.9 show the spatial distribution of the irrigation wells over the PRISM grids:\n\nCodeggplot() +\n  geom_stars(data = st_crop(tmax_m8_y09_stars[,,,1], st_bbox(KS_wells))) +\n  scale_fill_viridis(name = \"tmax\") +\n  geom_sf(data = st_transform(KS_wells, st_crs(tmax_m8_y09_stars)), size = 0.3) +\n  theme_for_map +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\nFigure 6.9: Map of Kansas county borders, irrigation wells, and PRISM tmax\n\n\n\n\n\nWe can extract the value of raster cells in which points are located using stars::st_extract().\n\n#--- NOT RUN ---#\nstars::st_extract(stars object, sf of points)\n\nBefore we extract values from the stars raster data, let’s crop it to the spatial extent of the KS_wells.\n\ntmax_m8_y09_KS_stars &lt;- sf::st_crop(tmax_m8_y09_stars, KS_wells)\n\nWe also should change the CRS of KS_wells to that of tmax_m8_y09_KS_stars.\n\nKS_wells &lt;- sf::st_transform(KS_wells, sf::st_crs(tmax_m8_y09_stars))\n\nWe now extract the value of rasters in which points are located:\n\n(\nextracted_tmax &lt;- stars::st_extract(tmax_m8_y09_KS_stars, KS_wells) \n)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n        Min. 1st Qu. Median     Mean  3rd Qu.   Max.\ntmax  24.605  30.681  33.08 33.22354 36.38175 40.759\ndimension(s):\n         from    to     offset  delta refsys point\ngeometry    1 37647         NA     NA  NAD83  TRUE\ndate        1    10 2009-08-01 1 days   Date    NA\n                                                            values\ngeometry POINT (-100.4423 37.52046),...,POINT (-99.96733 39.88535)\ndate                                                          NULL\n\n\nThe returned object is a stars object of simple features. You can convert this to a more familiar-looking sf object using sf::st_as_sf():\n\n(\nextracted_tmax_sf &lt;- sf::st_as_sf(extracted_tmax) \n)\n\nSimple feature collection with 37647 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   2009-08-01 2009-08-02 2009-08-03 2009-08-04 2009-08-05 2009-08-06 2009-08-07\n1      30.131     27.070     32.288     37.496     33.977     32.563     33.733\n2      31.533     28.373     35.927     36.942     30.557     31.235     31.121\n3      30.591     27.968     33.279     38.182     33.783     32.790     29.852\n4      31.307     27.565     34.945     38.056     32.832     30.040     31.744\n5      30.715     27.836     33.458     38.083     33.912     31.895     32.497\n6      29.661     27.592     32.192     36.787     33.856     31.655     29.434\n7      31.308     27.876     35.709     36.806     32.254     29.634     31.048\n8      28.776     25.930     30.293     34.316     30.861     29.100     29.051\n9      30.583     26.527     32.315     37.495     32.576     31.702     28.578\n10     29.715     27.051     32.638     37.328     33.863     32.001     33.281\n   2009-08-08 2009-08-09 2009-08-10                   geometry\n1      36.471     37.635     31.506 POINT (-100.4423 37.52046)\n2      37.016     31.317     29.902 POINT (-100.7118 39.91526)\n3      36.954     38.100     33.257 POINT (-99.15168 38.48849)\n4      36.498     32.738     29.350 POINT (-101.8995 38.78077)\n5      36.021     37.251     30.389  POINT (-100.7122 38.0731)\n6      36.524     39.329     36.532 POINT (-97.70265 39.04055)\n7      36.319     30.805     30.004 POINT (-101.7114 39.55035)\n8      34.211     35.462     35.193 POINT (-95.97031 39.16121)\n9      37.337     38.112     36.962 POINT (-98.30759 38.26787)\n10     36.322     37.686     31.488 POINT (-100.2785 37.71539)\n\n\nAs you can see each date forms a column of extracted values for the points because the third dimension of tmax_MI is date. So, you can easily turn the outcome to an sf object with date as Date object as follows.\n\n(\nextracted_tmax_long &lt;- \n  extracted_tmax_sf %&gt;%\n  tidyr::pivot_longer(- geometry, names_to = \"date\", values_to = \"tmax\") %&gt;% \n  sf::st_as_sf() %&gt;% \n  mutate(date = lubridate::ymd(date))\n)\n\nSimple feature collection with 376470 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\n# A tibble: 376,470 × 3\n               geometry date        tmax\n *          &lt;POINT [°]&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 (-100.4423 37.52046) 2009-08-01  30.1\n 2 (-100.4423 37.52046) 2009-08-02  27.1\n 3 (-100.4423 37.52046) 2009-08-03  32.3\n 4 (-100.4423 37.52046) 2009-08-04  37.5\n 5 (-100.4423 37.52046) 2009-08-05  34.0\n 6 (-100.4423 37.52046) 2009-08-06  32.6\n 7 (-100.4423 37.52046) 2009-08-07  33.7\n 8 (-100.4423 37.52046) 2009-08-08  36.5\n 9 (-100.4423 37.52046) 2009-08-09  37.6\n10 (-100.4423 37.52046) 2009-08-10  31.5\n# ℹ 376,460 more rows\n\n\nNote that all the variables other than geometry in the points data (KS_wells) are lost at the time of applying stars::st_extract(). You take advantage the fact that the the order of the observations in the object returned by stars::st_extract() is the order of the points (KS_wells).\n\n(\nextracted_tmax_long &lt;- \n  extracted_tmax_sf %&gt;% \n  mutate(well_id = KS_wells$well_id) %&gt;% \n  pivot_longer(c(- geometry, - well_id), names_to = \"date\", values_to = \"tmax\") %&gt;% \n  st_as_sf() %&gt;% \n  mutate(date = ymd(date))\n)\n\nSimple feature collection with 376470 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\n# A tibble: 376,470 × 4\n               geometry well_id date        tmax\n *          &lt;POINT [°]&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 (-100.4423 37.52046)       1 2009-08-01  30.1\n 2 (-100.4423 37.52046)       1 2009-08-02  27.1\n 3 (-100.4423 37.52046)       1 2009-08-03  32.3\n 4 (-100.4423 37.52046)       1 2009-08-04  37.5\n 5 (-100.4423 37.52046)       1 2009-08-05  34.0\n 6 (-100.4423 37.52046)       1 2009-08-06  32.6\n 7 (-100.4423 37.52046)       1 2009-08-07  33.7\n 8 (-100.4423 37.52046)       1 2009-08-08  36.5\n 9 (-100.4423 37.52046)       1 2009-08-09  37.6\n10 (-100.4423 37.52046)       1 2009-08-10  31.5\n# ℹ 376,460 more rows\n\n\nWe can now summarize the data by well_id and then merge it back to KS_wells.\n\nKS_wells &lt;-\n  extracted_tmax_long %&gt;%\n  #--- geometry no longer needed ---#\n  # if you do not do this, summarize() takes a long time\n  sf::st_drop_geometry() %&gt;%\n  dplyr::group_by(well_id, month(date)) %&gt;%\n  dplyr::summarize(mean(tmax)) %&gt;%\n  dplyr::left_join(KS_wells, ., by = \"well_id\")\n\n\n6.10.3 Extract and summarize values for polygons\nIn this section, we will learn how to extract cell values from raster layers as starts for spatial units represented as polygons sf data (see Section 5.2 for a more detailed explanation of this operation).\nIn order to extract cell values from stars objects (just like Chapter 5) and summarize them for polygons, you can use aggregate.stars(). Although introduced here because it natively accepts stars objects, aggregate.stars() is not recommended unless the raster data is small (with a small number of cells). It is almost always slower than the two other alternatives: terra::extract() and exactextractr::exact_extract() even though they involve conversion of stars objects to SpatRaster objects. For a comparison of its performance relative to these alternatives, see Section 9.3. If you just want to see the better alternatives, you can just skip to Section 6.10.3.2.\n\n6.10.3.1 aggregate.stars()\n\nThe syntax of aggregate.stars() is as follows:\n\n#--- NOT RUN ---#\naggregate(stars object, sf object, FUN = function to apply)\n\nFor each polygon, aggregate() identifies the cells whose centroids lie within the polygon, extracts their values, and applies the specified function to those values. Let’s now see a demonstration of how to use aggregate(). For this example, we will use Kansas counties as the polygon data.\n\nKS_county_sf &lt;-\n  tigris::counties(state = \"Kansas\", cb = TRUE, progress_bar = FALSE)%&gt;%\n  #--- transform using the CRS of the PRISM stars data  ---#\n  sf::st_transform(sf::st_crs(tmax_m8_y09_stars)) %&gt;%\n  #--- generate unique id ---#\n  dplyr::mutate(id = 1:nrow(.))\n\nFigure 6.10 shows polygons (counties) superimposed on top of the tmax raster data:\n\n\n\n\nCodeggplot() +\n  geom_stars(data = sf::st_crop(tmax_m8_y09_stars, sf::st_bbox(KS_county_sf))[,,,1]) +\n  scale_fill_viridis() +\n  geom_sf(data = KS_county_sf, fill = NA) +\n  theme_void()\n\n\n\n\n\n\nFigure 6.10: Map of Kansas counties over tmax raster data\n\n\n\n\nFor example, the following code will find the mean of the tmax values for each county:\n\n(\nmean_tmax_stars &lt;- aggregate(tmax_m8_y09_stars, KS_county_sf, FUN = mean) \n)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n          Min.  1st Qu.   Median    Mean  3rd Qu.     Max.\ntmax  24.91317 29.65889 32.56404 32.5487 35.34826 39.70888\ndimension(s):\n         from  to     offset  delta refsys point\ngeometry    1 105         NA     NA  NAD83 FALSE\ndate        1  10 2009-08-01 1 days   Date    NA\n                                                                values\ngeometry MULTIPOLYGON (((-101.0681...,...,MULTIPOLYGON (((-96.50168...\ndate                                                              NULL\n\n\nAs you can see, the aggregate() operation also returns a stars object for polygons just like stars::st_extract() did. You can convert the stars object into an sf object using sf::st_as_sf():\n\nmean_tmax_sf &lt;- sf::st_as_sf(mean_tmax_stars)\n\nAs you can see, aggregate() function extracted values for the polygons from all the layers across the date dimension, and the values from individual dates become variables where the variables names are the corresponding date values.\nJust like the case of raster data extraction for points data we saw in Section 6.10.2, no information from the polygons (KS_county_sf) except the geometry column remains. For further processing of the extracted data and easy merging with the polygons data (KS_county_sf), we can assign the unique county id just like we did for the case of points data.\n\n(\n  mean_tmax_long &lt;-\n    mean_tmax_sf %&gt;%\n    #--- drop geometry ---#\n    sf::st_drop_geometry() %&gt;%\n    #--- assign id before transformation ---#\n    dplyr::mutate(id = KS_county_sf$id) %&gt;%\n    #--- then transform ---#\n    tidyr::pivot_longer(-id, names_to = \"date\", values_to = \"tmax\")\n)\n\n\n\n\naggregate() will return NA for polygons that intersect with raster cells that have NA values. To ignore the NA values when applying a function, we can add na.rm=TRUE option like this:\n\n(\n  mean_tmax_stars &lt;-\n    aggregate(\n      tmax_m8_y09_stars,\n      KS_county_sf,\n      FUN = mean,\n      na.rm = TRUE\n    ) %&gt;%\n    sf::st_as_sf()\n)\n\n\n6.10.3.2 exactextractr::exact_extract()\n\nA great alternative to aggregate() is to extract raster cell values using exactextractr::exact_extract() or terra::extract() and then summarize the results yourself. Both are considerably faster than aggregate.stars(). Although they do not natively accept stars objects, you can easily convert a stars object to a SpatRaster using as(stars, \"SpatRaster\") and then use these methods.\nLet’s first convert tmax_m8_y09_KS_stars (a stars object) into a SpatRaster object.\n\n(\ntmax_m8_y09_KS_sr &lt;- as(tmax_m8_y09_KS_stars, \"SpatRaster\")\n)\n\nclass       : SpatRaster \ndimensions  : 73, 179, 10  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -102.0625, -94.60417, 36.97917, 40.02083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nnames       : date2~08-01, date2~08-02, date2~08-03, date2~08-04, date2~08-05, date2~08-06, ... \nmin values  :      27.639,      24.605,      28.094,      32.711,      28.972,      26.871, ... \nmax values  :      32.844,      32.341,      37.560,      40.759,      39.466,      36.303, ... \n\n\nNow, we can use exactextractr::exact_extract() as follows:\n\nextracted_values &lt;- exactextractr::exact_extract(tmax_m8_y09_KS_sr, KS_county_sf, include_cols = \"COUNTYFP\")\n\nThe returned outcome is a list of data.frame. Let’s take a look at the first 6 rows of the first 3 elements of the list.\n\nextracted_values[1:3] %&gt;% lapply(., head)\n\n[[1]]\n  COUNTYFP date2009-08-01 date2009-08-02 date2009-08-03 date2009-08-04\n1      175         30.954         27.629         34.411         37.547\n2      175             NA             NA             NA             NA\n3      175         30.866         27.506         34.229         37.464\n4      175         30.837         27.398         34.126         37.364\n5      175         30.759         27.369         33.969         37.323\n6      175         30.584         27.399         33.734         37.284\n  date2009-08-05 date2009-08-06 date2009-08-07 date2009-08-08 date2009-08-09\n1         34.264         33.468         35.103         36.674         37.961\n2             NA             NA             NA             NA             NA\n3         34.217         33.348         34.925         36.552         37.861\n4         34.198         33.314         34.860         36.442         37.762\n5         34.159         33.186         34.728         36.463         37.778\n6         34.098         33.092         34.512         36.487         37.723\n  date2009-08-10 coverage_fraction\n1         31.715         0.1074141\n2             NA         0.8066747\n3         31.564         0.8066615\n4         31.432         0.8066448\n5         31.463         0.8061629\n6         31.592         0.8054324\n\n[[2]]\n  COUNTYFP date2009-08-01 date2009-08-02 date2009-08-03 date2009-08-04\n1      027         29.623         26.370         32.274         35.627\n2      027         29.657         26.306         32.209         35.527\n3      027         29.665         26.254         32.213         35.421\n4      027             NA             NA             NA             NA\n5      027             NA             NA             NA             NA\n6      027             NA             NA             NA             NA\n  date2009-08-05 date2009-08-06 date2009-08-07 date2009-08-08 date2009-08-09\n1         31.836         30.141         29.131         35.980         38.073\n2         31.787         30.081         29.089         35.968         37.983\n3         31.719         30.010         29.049         35.951         37.949\n4             NA             NA             NA             NA             NA\n5             NA             NA             NA             NA             NA\n6             NA             NA             NA             NA             NA\n  date2009-08-10 coverage_fraction\n1         34.794        0.03732148\n2         34.905        0.10592906\n3         34.810        0.10249268\n4             NA        0.10018899\n5             NA        0.10074520\n6             NA        0.09701102\n\n[[3]]\n  COUNTYFP date2009-08-01 date2009-08-02 date2009-08-03 date2009-08-04\n1      171             NA             NA             NA             NA\n2      171             NA             NA             NA             NA\n3      171             NA             NA             NA             NA\n4      171             NA             NA             NA             NA\n5      171             NA             NA             NA             NA\n6      171             NA             NA             NA             NA\n  date2009-08-05 date2009-08-06 date2009-08-07 date2009-08-08 date2009-08-09\n1             NA             NA             NA             NA             NA\n2             NA             NA             NA             NA             NA\n3             NA             NA             NA             NA             NA\n4             NA             NA             NA             NA             NA\n5             NA             NA             NA             NA             NA\n6             NA             NA             NA             NA             NA\n  date2009-08-10 coverage_fraction\n1             NA         0.1813289\n2             NA         0.3062004\n3             NA         0.2972469\n4             NA         0.2887933\n5             NA         0.2825042\n6             NA         0.2799884\n\n\nAs you can see, each data.frame has variables called COUNTYFP, date2009-08-01, \\(\\dots\\), date2009-08-10 (they are from the layer names in tmax_m8_y09_KS_sr) and coverage_fraction. COUNTYFP was inherited from KS_county_sf thanks to include_cols = \"COUNTYFP\" and let us merge the extracted values with KS_county_sf.\nIn order to make the results easier to work with, you can process them to get a single data.frame, taking advantage of dplyr::bind_rows() to combine the list of the datasets into one dataset.\n\nextracted_values_df &lt;-\n  extracted_values %&gt;%\n  #--- combine the list of data.frames ---#\n  dplyr::bind_rows()\n\nhead(extracted_values_df)\n\n  COUNTYFP date2009-08-01 date2009-08-02 date2009-08-03 date2009-08-04\n1      175         30.954         27.629         34.411         37.547\n2      175             NA             NA             NA             NA\n3      175         30.866         27.506         34.229         37.464\n4      175         30.837         27.398         34.126         37.364\n5      175         30.759         27.369         33.969         37.323\n6      175         30.584         27.399         33.734         37.284\n  date2009-08-05 date2009-08-06 date2009-08-07 date2009-08-08 date2009-08-09\n1         34.264         33.468         35.103         36.674         37.961\n2             NA             NA             NA             NA             NA\n3         34.217         33.348         34.925         36.552         37.861\n4         34.198         33.314         34.860         36.442         37.762\n5         34.159         33.186         34.728         36.463         37.778\n6         34.098         33.092         34.512         36.487         37.723\n  date2009-08-10 coverage_fraction\n1         31.715         0.1074141\n2             NA         0.8066747\n3         31.564         0.8066615\n4         31.432         0.8066448\n5         31.463         0.8061629\n6         31.592         0.8054324\n\n\nNow, let’s find area-weighted mean of tmax for each of the county-date combinations. The following code\n\nreshapes extracted_values_df to a long format\nrecovers date as Date object\ncalculates area-weighted mean of tmax by county-date\n\n\nextracted_values_df %&gt;%\n  #--- long to wide ---#\n  tidyr::pivot_longer(\n    c(-COUNTYFP, -coverage_fraction),\n    names_to = \"date\",\n    values_to = \"tmax\"\n  ) %&gt;%\n  #--- remove date  ---#\n  dplyr::mutate(\n    #--- remove \"date\" from date and then convert it to Date ---#\n    date = gsub(\"date\", \"\", date) %&gt;% lubridate::ymd()\n  ) %&gt;%\n  #--- mean area-weighted tmax by county-date ---#\n  dplyr::group_by(COUNTYFP, date) %&gt;%\n  na.omit() %&gt;%\n  dplyr::summarize(sum(tmax * coverage_fraction)/sum(coverage_fraction))\n\n# A tibble: 960 × 3\n# Groups:   COUNTYFP [96]\n   COUNTYFP date       `sum(tmax * coverage_fraction)/sum(coverage_fraction)`\n   &lt;chr&gt;    &lt;date&gt;                                                      &lt;dbl&gt;\n 1 005      2009-08-01                                                   28.2\n 2 005      2009-08-02                                                   26.0\n 3 005      2009-08-03                                                   29.3\n 4 005      2009-08-04                                                   33.6\n 5 005      2009-08-05                                                   30.4\n 6 005      2009-08-06                                                   28.1\n 7 005      2009-08-07                                                   28.9\n 8 005      2009-08-08                                                   33.3\n 9 005      2009-08-09                                                   34.8\n10 005      2009-08-10                                                   34.4\n# ℹ 950 more rows\n\n\nThis can now be readily merged with KS_county_sf using COUNTYFP as the key.\n\n6.10.3.3 Summarizing the extracted values inside exactextractr::exact_extract()\n\nInstead of returning the value from all the intersecting cells, exactextractr::exact_extract() can summarize the extracted values by polygon and then return the summarized numbers. This is much like how aggregate() works, which we saw above. There are multiple default options you can choose from. All you need to do is to add the desired summary function name as the third argument of exactextractr::exact_extract(). For example, the following will get us the mean of the extracted values weighted by coverage_fraction.\n\nextacted_mean &lt;- exactextractr::exact_extract(tmax_m8_y09_KS_sr, KS_county_sf, \"mean\", append_cols = \"COUNTYFP\", progress = FALSE)\n\nhead(extacted_mean)\n\nNotice that you use append_cols instead of include_cols when you summarize within exactextractr::exact_extract().\n\n(\nmean_tmax_long &lt;- \n  extacted_mean %&gt;% \n  #--- wide to long ---#\n  pivot_longer(-COUNTYFP, names_to = \"date\", values_to = \"tmax\") %&gt;%\n  #--- recover date ---#\n  dplyr::mutate(date = gsub(\"mean.date\", \"\", date) %&gt;% lubridate::ymd())\n)\n\n# A tibble: 1,050 × 3\n   COUNTYFP date        tmax\n   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt;\n 1 175      2009-08-01  30.8\n 2 175      2009-08-02  27.7\n 3 175      2009-08-03  34.0\n 4 175      2009-08-04  37.2\n 5 175      2009-08-05  34.8\n 6 175      2009-08-06  33.9\n 7 175      2009-08-07  35.2\n 8 175      2009-08-08  36.8\n 9 175      2009-08-09  38.0\n10 175      2009-08-10  32.3\n# ℹ 1,040 more rows\n\n\nThere are other summary function options that may be of interest, such as “max”, “min.” You can see all the default options at the package website.\n\n6.10.3.4 area-weighted v.s. coverage-fraction-weighted summary (Optional)\nWhen we found the mean of tmax weighted by coverage fraction, each raster cell was assumed to cover the same area. This is not exactly correct when a raster layer in geographic coordinates (latitude/longitude) is used. To see this, let’s find the area of each cell using terra::cellSize().\n\n(\nraster_area_data &lt;- terra::cellSize(tmax_m8_y09_KS_sr)\n)\n\nclass       : SpatRaster \ndimensions  : 73, 179, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -102.0625, -94.60417, 36.97917, 40.02083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nname        :     area \nmin value   : 16461242 \nmax value   : 17149835 \n\n\nThe output is a SpatRaster, where the area of the cells are stored as values. Figure 6.11 shows the map of the PRISM raster cells in Kansas, color-differentiated by area.\n\nCodeplot(raster_area_data)\n\n\n\n\n\n\nFigure 6.11: Area of PRISM raster cells\n\n\n\n\nThe area of a raster cell becomes slightly larger as the latitude increases. This is mainly due to the fact that 1 degree in longitude is longer in actual length on the earth surface at a higher latitude than at a lower latitude in the northern hemisphere.7 The mean of extracted values weighted by coverage fraction ignores this fact and implicitly assumes the all the cells have the same area.\n7 The opposite happens in the southern hemisphere.In order to get an area-weighted mean instead of a coverage-weighted mean, you can use the “weighted_mean” option as the third argument and also supply the SpatRaster of area (raster_area_data here) like this:\n\nextracted_weighted_mean &lt;-\n  exactextractr::exact_extract(\n    tmax_m8_y09_KS_sr,\n    KS_county_sf,\n    \"weighted_mean\",\n    weights = raster_area_data,\n    append_cols = \"COUNTYFP\",\n    progress = FALSE\n  )   \n\nLet’s compare the difference in the calculated means from the two methods for the first polygon.\n\nextracted_weighted_mean[, 2] - extacted_mean[, 2]\n\n  [1]  1.316071e-04 -8.201599e-05 -1.201630e-04 -1.029968e-04  3.585815e-04\n  [6]  8.010864e-05 -2.212524e-04 -1.335144e-05 -8.392334e-05 -1.640320e-04\n [11] -7.057190e-05 -4.386902e-05 -1.850128e-04 -1.506805e-04  6.504059e-04\n [16]  1.983643e-04  0.000000e+00  3.623962e-04  8.201599e-05  3.509521e-04\n [21]  3.814697e-06  1.258850e-04 -1.602173e-04 -9.155273e-05 -7.247925e-05\n [26] -4.768372e-05  0.000000e+00 -2.861023e-05 -4.005432e-05  4.577637e-05\n [31]  1.716614e-05  1.869202e-04  5.340576e-05 -4.386902e-05  1.621246e-04\n [36] -2.365112e-04  3.814697e-05  2.803802e-04 -4.005432e-05           NaN\n [41]  6.103516e-05  4.005432e-05  7.629395e-05 -1.392365e-04  2.593994e-04\n [46]  3.623962e-05 -3.814697e-05 -3.814697e-05  4.386902e-05 -1.907349e-06\n [51]  2.670288e-04  6.675720e-05  3.623962e-04  4.005432e-04 -1.029968e-04\n [56] -1.010895e-04 -7.247925e-05  0.000000e+00 -6.294250e-05 -7.629395e-06\n [61]           NaN -3.147125e-04 -5.149841e-05 -4.768372e-05 -1.487732e-04\n [66] -9.155273e-05           NaN  8.773804e-05 -1.525879e-05 -3.204346e-04\n [71]           NaN -5.531311e-04  5.722046e-06  1.201630e-03  6.866455e-05\n [76] -6.294250e-05 -7.629395e-06 -1.907349e-05  1.964569e-04 -1.869202e-04\n [81] -1.125336e-04  2.803802e-04  2.861023e-05 -1.964569e-04           NaN\n [86] -9.536743e-06  7.057190e-05 -2.536774e-04 -4.005432e-05 -3.623962e-05\n [91] -1.945496e-04 -2.593994e-04 -1.029968e-04           NaN           NaN\n [96]  4.959106e-05 -5.531311e-05           NaN -6.866455e-05  7.762909e-04\n[101]  1.010895e-04 -7.629395e-06  5.531311e-05           NaN -8.392334e-05\n\n\nAs you can see the error is minimal. So, the consequence of using coverage-weighted means should be negligible in most cases. Indeed, unless polygons span a wide range of latitude and longitude, the error introduce by using the coverage-weighted mean instead of area-weighted mean should be negligible.\n\n\n\n\n\n\nPebesma, Edzer. 2020. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatiotemporal Raster Data Handling with `stars`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html",
    "href": "chapters/07-CreateMaps-ggplot.html",
    "title": "7  Creating Maps using ggplot2",
    "section": "",
    "text": "Before you start\nIn previous chapters, we explored how to quickly create simple maps using vector and raster data. This section will focus on using ggplot2 to produce high-quality maps suitable for publication in journal articles, conference presentations, and professional reports. Achieving this level of quality requires refining the map’s aesthetics beyond default settings. This includes choosing appropriate color schemes, removing extraneous elements, and formatting legends. The focus here is on creating static maps, not the interactive ones commonly found on the web.\nCreating maps differs from creating non-spatial figures in some aspects, but the underlying principles and syntax in ggplot2 for both are quite similar. If you already have experience with ggplot2, you will find map-making intuitive and straightforward, even if you have never used it for spatial data before. The primary difference lies in the choice of geom_*() functions. For spatial data visualization, several specialized geom_*() types are available.\nThese geom_*() functions enable the visualization of both vector and raster data using the consistent and straightforward syntax of ggplot2. In the following sections, Section 7.1 and Section 7.2, we will explore each of the geom_*()s individually to understand their basic usage. You will also notice that the principles discussed in the subsequent sections are not specific to spatial data—they are general and applicable to any type of figure.\nFinally, another great package for mapping is the tmap package (package website). It also follows the grammar of graphics like ggplot2, and those who are already familiar with ggplot2 will find it very intuitive to use. Given that the package website and Chapter 9 of Geocomputation with R already have great treatments of how to create maps with the package. This book does not cover this topic.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#before-you-start",
    "href": "chapters/07-CreateMaps-ggplot.html#before-you-start",
    "title": "7  Creating Maps using ggplot2",
    "section": "",
    "text": "ggplot2::geom_sf() for sf (vector) objects\n\ntidyterra::geom_spatraster() for SpatRaster (raster) objects\n\nstars::geom_stars() for stars (raster) objects1\n\n\n1 see Chapter 6 for how stars package works\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough this chapter does not require extensive knowledge of ggplot2, having a basic understanding of it will be extremely helpful. If you are unfamiliar with ggplot2 or feel that your knowledge is insufficient, Appendix Appendix B offers a minimal introduction to data visualization using the ggplot2 package. This should provide enough background to help you follow along with the content in this chapter. ggplot2: Elegant Graphics for Data Analysis provides a fuller treatment of how ggplot2 works\n\n\nDirection for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, file paths are set relative to my working directory (which is not shown). To run the code without having to adjust the file paths, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nPackages\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  raster, # raster data handling\n  terra, # raster data handling\n  tidyterra, # for tidyverse like operations on terra objects\n  sf, # vector data handling\n  dplyr, # data wrangling\n  stringr, # string manipulation\n  lubridate, # dates handling\n  data.table, # data wrangling\n  patchwork, # arranging figures\n  tigris, # county border\n  colorspace, # color scale\n  viridis, # arranging figures\n  tidyr, # reshape\n  ggspatial, # north arrow and scale bar\n  ggplot2, # make maps\n  ggmapinset # make inset maps\n)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#sec-geom-sf",
    "href": "chapters/07-CreateMaps-ggplot.html#sec-geom-sf",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.1 Creating maps from sf objects",
    "text": "7.1 Creating maps from sf objects\nThis section explains how to create maps from vector data stored as an sf object via geom_sf().\n\n7.1.1 Datasets\nThe following datasets will be used for illustrations.\nWells in Kansas as points (Figure 7.1)\n\n\nwell_id: well ID\n\naf_used: total annual groundwater pumping at individual irrigation wells\n\n\n#--- read in the KS wells data ---#\n(\n  gw_KS_sf &lt;- readRDS(\"Data/gw_KS_sf.rds\")\n)\n\nSimple feature collection with 56225 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id year   af_used                   geometry\n1        1 2010  67.00000 POINT (-100.4423 37.52046)\n2        1 2011 171.00000 POINT (-100.4423 37.52046)\n3        3 2010  30.93438 POINT (-100.7118 39.91526)\n4        3 2011  12.00000 POINT (-100.7118 39.91526)\n5        7 2010   0.00000 POINT (-101.8995 38.78077)\n6        7 2011   0.00000 POINT (-101.8995 38.78077)\n7       11 2010 154.00000 POINT (-101.7114 39.55035)\n8       11 2011 160.00000 POINT (-101.7114 39.55035)\n9       12 2010  28.17239 POINT (-95.97031 39.16121)\n10      12 2011  89.53479 POINT (-95.97031 39.16121)\n\n\n\n\n\n\nggplot(gw_KS_sf) +\n  geom_sf(size = 0.5) +\n  theme_void()\n\n\n\n\n\n\nFigure 7.1: Map of wells in Kansas\n\n\n\n\nKansas county border as polygons (Figure 7.2)\n\n\nCOUNTYFP: county FIPs code\n\nNAME: county name\n\n\n(\n  KS_county &lt;-\n    tigris::counties(state = \"Kansas\", cb = TRUE, progress_bar = FALSE) %&gt;%\n    sf::st_as_sf() %&gt;%\n    sf::st_transform(st_crs(gw_KS_sf)) %&gt;%\n    dplyr::select(COUNTYFP, NAME)\n)\n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n    COUNTYFP         NAME                       geometry\n40       175       Seward MULTIPOLYGON (((-101.0681 3...\n181      027         Clay MULTIPOLYGON (((-97.3707 39...\n182      171        Scott MULTIPOLYGON (((-101.1284 3...\n183      047      Edwards MULTIPOLYGON (((-99.56988 3...\n373      147     Phillips MULTIPOLYGON (((-99.62821 3...\n485      149 Pottawatomie MULTIPOLYGON (((-96.72774 3...\n486      055       Finney MULTIPOLYGON (((-101.103 37...\n487      167      Russell MULTIPOLYGON (((-99.04234 3...\n488      135         Ness MULTIPOLYGON (((-100.2477 3...\n489      093       Kearny MULTIPOLYGON (((-101.5419 3...\n\n\n\n\n\n\nCodeggplot(KS_county) + \n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 7.2: Map of Kansas county borders\n\n\n\n\nRailroads as lines (Figure 7.3)\n\n(\n  KS_railroads &lt;- \n    sf::st_read(\"Data/tl_2015_us_rails.shp\") %&gt;%\n    sf::st_crop(KS_county)\n)\n\n\n\n\n\nCodeggplot(KS_railroads) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 7.3: Map of railroads\n\n\n\n\n\n\nSimple feature collection with 5796 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99769 xmax: -94.58841 ymax: 40.06304\nGeodetic CRS:  NAD83\nFirst 10 features:\n        LINEARID                         FULLNAME MTFCC\n2124 11051038759                          Bnsf RR R1011\n2136 11051038771                          Bnsf RR R1011\n2141 11051038776                          Bnsf RR R1011\n2186 11051047374                   Rock Island RR R1011\n2240 11051048071  Burlington Northern Santa Fe RR R1011\n2252 11051048083  Burlington Northern Santa Fe RR R1011\n2256 11051048088  Burlington Northern Santa Fe RR R1011\n2271 11051048170 Chicago Burlington and Quincy RR R1011\n2272 11051048171 Chicago Burlington and Quincy RR R1011\n2293 11051048193 Chicago Burlington and Quincy RR R1011\n                           geometry\n2124 LINESTRING (-94.58841 39.15...\n2136 LINESTRING (-94.59017 39.11...\n2141 LINESTRING (-94.58841 39.15...\n2186 LINESTRING (-94.58893 39.11...\n2240 LINESTRING (-94.58841 39.15...\n2252 LINESTRING (-94.59017 39.11...\n2256 LINESTRING (-94.58841 39.15...\n2271 LINESTRING (-94.58862 39.15...\n2272 LINESTRING (-94.58883 39.11...\n2293 LINESTRING (-94.58871 39.11...\n\n\n\n7.1.2 Basic usage of geom_sf()\n\ngeom_sf() enables the visualization of sf objects. It automatically detects the geometry type of spatial data stored in sf and renders maps accordingly. For example, the following code generates maps for Kansas wells (points), Kansas counties (polygons), and Kansas railroads (lines):\n\n(\n  g_wells &lt;-\n    ggplot(data = gw_KS_sf) +\n    geom_sf()\n)\n\n\n\n\n\n\n\n\n(\n  g_county &lt;-\n    ggplot(data = KS_county) +\n    geom_sf()\n)\n\n\n\n\n\n\n\n\n(\n  g_rail &lt;-\n    ggplot(data = KS_railroads) +\n    geom_sf()\n)\n\n\n\n\n\n\n\nAs demonstrated, different geometry types—such as points, polygons, and lines—are handled seamlessly by a single geom function, geom_sf(). Additionally, note that in the example code, neither the x-axis (longitude) nor the y-axis (latitude) needs to be explicitly specified. When creating a map, longitude and latitude are automatically assigned to the x- and y-axes, respectively. geom_sf() intelligently detects the geometry type and renders spatial objects accordingly.\n\n7.1.3 Specifying the aesthetics\nThere are various aesthetics options you can use. Available aesthetics vary by the type of geometry. This section shows the basics of how to specify the aesthetics of maps. Finer control of aesthetics will be discussed later.\n\n7.1.3.1 Points\n\n\ncolor: color of the points\n\nfill: available for some shapes (but likely useless)\n\nshape: shape of the points\n\nsize: size of the points (rarely useful)\n\nFor illustration here, let’s focus on the wells in one county so it is easy to detect the differences across various aesthetics configurations.\n\n#--- wells in Stevens County ---#\ngw_Stevens &lt;- \n  KS_county %&gt;%\n  dplyr::filter(NAME == \"Stevens\") %&gt;%\n  sf::st_crop(gw_KS_sf, .)\n\nexample 1\n\n\ncolor: dependent on af_used (the amount of groundwater extraction)\n\nsize: constant across the points (bigger than default)\n\n\n(\n  ggplot(data = gw_Stevens) +\n    geom_sf(aes(color = af_used), size = 2)\n)\n\n\n\n\n\n\n\nexample 2\n\n\ncolor: constant across the points (blue)\n\nsize: dependent on af_used\n\n\nshape: constant across the points (square)\n\n\n(\n  ggplot(data = gw_Stevens) +\n    geom_sf(aes(size = af_used), color = \"blue\", shape = 15)\n)\n\n\n\n\n\n\n\nexample 3\n\n\ncolor: dependent on whether located east of west of -101.3 in longitude\n\nshape: dependent on whether located east of west of -101.3 in longitude\n\n\n(\n  gw_Stevens %&gt;%\n    cbind(., st_coordinates(.)) %&gt;%\n    dplyr::mutate(east_west = ifelse(X &lt; -101.3, \"west\", \"east\")) %&gt;%\n    ggplot(data = .) +\n    geom_sf(aes(shape = east_west, color = east_west))\n)\n\n\n\n\n\n\n\n\n7.1.3.2 Polygons\n\n\ncolor: color of the borders of the polygons\n\nfill: color of the inside of the polygons\n\nshape: not available\n\nsize: not available\n\nexample 1\n\n\ncolor: constant (red)\n\nfill: constant (dark green)\n\n\nggplot(data = KS_county) +\n  geom_sf(color = \"red\", fill = \"darkgreen\")\n\n\n\n\n\n\n\nexample 2\n\n\ncolor: default (black)\n\nfill: dependent on the total amount of pumping in 2010\n\n\nKS_county_with_pumping &lt;-\n  gw_KS_sf %&gt;%\n  #--- only year == 2010 ---#\n  dplyr::filter(year == 2010) %&gt;%\n  #--- get total pumping by county ---#\n  aggregate(., KS_county, sum, na.rm = TRUE)\n\nggplot(data = KS_county_with_pumping) +\n  geom_sf(aes(fill = af_used))\n\n\n\n\n\n\n\n\n7.1.4 Plotting multiple spatial objects in one figure\nYou can combine all the layers created by geom_sf() additively so they appear in a single map:\n\nggplot() +\n  #--- this one uses KS_wells ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_county ---#\n  geom_sf(data = KS_county) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")\n\n\n\n\n\n\n\nOops, you cannot see wells (points) in the figure. The order of geom_sf() matters. The layer added later will come on top of the preceding layers. That’s why wells are hidden beneath Kansas counties. So, let’s do this:\n\nggplot(data = KS_county) +\n  #--- this one uses KS_county ---#\n  geom_sf() +\n  #--- this one uses KS_county ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")\n\n\n\n\n\n\n\nBetter.\nNote that since you are using different datasets for each layer, you need to specify the dataset to use in each layer except for the first geom_sf() which inherits data = KS_wells from ggplot(data = KS_wells). Of course, this will create exactly the same map:\n\n(\n  g_all &lt;- \n    ggplot() +\n    #--- this one uses KS_county ---#\n    geom_sf(data = KS_county) +\n    #--- this one uses KS_wells ---#\n    geom_sf(data = gw_KS_sf, size = 0.4) +\n    #--- this one uses KS_railroads ---#\n    geom_sf(data = KS_railroads, color = \"red\")\n)\n\n\n\n\n\n\n\nThere is no rule that you need to supply data to ggplot().2\n2 Supplying data in ggplot() can be convenient if you are creating multiple geom from the data because you do not need to tell what data to use in each of the subsequent geoms.Alternatively, you could add fill = NA to geom_sf(data = KS_county) instead of switching the order.\n\nggplot() +\n  #--- this one uses KS_wells ---#\n  geom_sf(data = gw_KS_sf, size = 0.4) +\n  #--- this one uses KS_county ---#\n  geom_sf(data = KS_county, fill = NA) +\n  #--- this one uses KS_railroads ---#\n  geom_sf(data = KS_railroads, color = \"red\")\n\n\n\n\n\n\n\nThis is fine as long as you do not intend to color-code counties.\n\n7.1.5 CRS\nggplot() uses the CRS of the sf to draw a map. For example, right now the CRS of KS_county is this:\n\nsf::st_crs(KS_county)\n\nCoordinate Reference System:\n  User input: EPSG:4269 \n  wkt:\nGEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n        SPHEROID[\"GRS 1980\",6378137,298.257222101,\n            AUTHORITY[\"EPSG\",\"7019\"]],\n        TOWGS84[0,0,0,0,0,0,0],\n        AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]]\n\n\nLet’s convert the CRS to WGS 84/ UTM zone 14N (EPSG code: 32614), make a map, and compare the ones with different CRS side by side.\n\ng_32614 &lt;-\n  sf::st_transform(KS_county, 32614) %&gt;%\n  ggplot(data = .) +\n  geom_sf()\n\n\ng_county / g_32614\n\n\n\n\n\n\n\nAlternatively, you could use coord_sf() to alter the CRS on the map, but not the CRS of the sf object itself.\n\nggplot() +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_county) +\n  coord_sf(crs = 32614)\n\n\n\n\n\n\n\nWhen multiple layers are used for map creation, the CRS of the first layer is applied for all the layers.\n\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads)\n\n\n\n\n\n\n\ncoord_sf() applies to all the layers.\n\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads) +\n  #--- using 4269 ---#\n  coord_sf(crs = 4269)\n\n\n\n\n\n\n\nFinally, you could limit the geographic scope of the map to be created by adding xlim() and ylim().\n\nggplot() +\n  #--- epsg: 32614 ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- epsg: 4269 ---#\n  geom_sf(data = KS_railroads) +\n  #--- using 4269 ---#\n  coord_sf(crs = 4269) +\n  #--- limit the geographic scope of the map ---#\n  xlim(-99, -97) +\n  ylim(37, 39)\n\n\n\n\n\n\n\n\n7.1.6 Faceting\nFaceting splits the data into groups and generates a figure for each group, where the aesthetics of the figures are consistent across the groups. Faceting can be done using facet_wrap() or facet_grid(). Let’s try to create a map of groundwater use at wells by year where the points are color differentiated by the amount of groundwater use (af_used).\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(. ~ year)\n\n\n\n\n\n\n\nNote that the above code creates a single legend that applies to both panels, which allows you to compare values across panels (years here). Further, also note that the values of the faceting variable (year) are displayed in the gray strips above the maps. You can have panels stacked vertically by using the ncol option (or nrow also works) in facet_wrap(. ~ year) as follows:\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(. ~ year, ncol = 1)\n\n\n\n\n\n\n\nTwo-way faceting is possible by supplying a variable name (or expression) in place of . in facet_wrap(. ~ year). The code below uses an expression (af_used &gt; 200) in place of .. This divides the dataset by whether water use is greater than 200 or not and by year.\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used &gt; 200) ~ year)\n\n\n\n\n\n\n\nThe values of the expression (TRUE or FALSE) appear in the gray strips, which is not informative. We will discuss in detail how to control texts in the strips Section 7.6.\nIf you feel like the panels are too close to each other, you could provide more space between them using panel.spacing (both vertically and horizontally), panel.spacing.x (horizontally), and panel.spacing.y (vertically) options in theme(). Suppose you would like to place more space between the upper and lower panels, then you use panel.spacing.y like this:\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used &gt; 200) ~ year) +\n  #--- add more space between panels ---#\n  theme(panel.spacing.y = unit(2, \"lines\"))\n\n\n\n\n\n\n\n\n7.1.7 Adding texts (labels) on a map\nYou can add labels to a map using geom_sf_text() or geom_sf_label() from the ggplot2 pacakge, specifying aes(label = x) where x is the variable containing the labels to be displayed on the map.\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    size = 3,\n    color = \"blue\"\n  )\n\n\n\n\n\n\n\nIf you would like to have overlapping labels not printed, you can add check_overlap = TRUE.\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\"\n  )\n\n\n\n\n\n\n\nThe nudge_x and nudge_y options let you shift the labels.\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_county,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  )\n\n\n\n\n\n\n\nIf you would like a fine control on a few objects, you can always work on them separately.\n\nCheyenne &lt;- dplyr::filter(KS_county, NAME == \"Cheyenne\")\nKS_less_Cheyenne &lt;- dplyr::filter(KS_county, NAME != \"Cheyenne\")\n\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_less_Cheyenne,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  ) +\n  geom_sf_text(\n    data = Cheyenne,\n    aes(label = NAME),\n    size = 2.5,\n    color = \"red\",\n    nudge_y = 0.2\n  )\n\n\n\n\n\n\n\nYou could also use annotate() to place texts on a map, which can be useful if you would like to place arbitrary texts that are not part of sf object.\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = KS_county) +\n  geom_sf_text(\n    data = KS_less_Cheyenne,\n    aes(label = NAME),\n    check_overlap = TRUE,\n    size = 3,\n    color = \"blue\",\n    nudge_x = -0.1,\n    nudge_y = 0.1\n  ) +\n  #--- use annotate to add texts on the map ---#\n  annotate(\n    geom = \"text\",\n    x = -102,\n    y = 39.8,\n    size = 3,\n    label = \"Cheyennes\",\n    color = \"red\"\n  )\n\n\n\n\n\n\n\nAs you can see, you need to tell where the texts should be placed with x and y, provide the texts you want on the map to label.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#sec-geom-raster",
    "href": "chapters/07-CreateMaps-ggplot.html#sec-geom-raster",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.2 Raster data visualization: geom_spatraster() and geom_stars()\n",
    "text": "7.2 Raster data visualization: geom_spatraster() and geom_stars()\n\nThis section shows how to use tidyterra::geom_spatraster() and stars::geom_stars() to create maps from raster datasets of two object classes: SpatRaster from the terra package and stars objects from the stars package, respectively.\nWhile ggplot2::geom_raster() can be used to create a map from raster, it requires a data.frame containing coordinates to generate maps, making it a two-step process.\n\nconvert raster dataset into a data.frame with coordinates\nuse ggplot2::geom_raster() to make a map\n\nOn the other hand, geom_spatraster() and geom_stars() accepts a SpatRaster (from the terra package) and stars object (from the stars package), respectively, and no data transformation is necessary for either of them. For this reason,  the use of geom_spatraster() or geom_stars is recommended over geom_raster(). \n\n7.2.1 Data Preparation\nWe use the following objects that have the same information but come in different object classes for illustration in this section.\nRaster as stars\n\ntmax_Jan_09 &lt;- readRDS(\"Data/tmax_Jan_09_stars.rds\")\n\nRaster as SpatRaster\n\n#--- convert RasterStack to SpatRaster ---#\ntmax_Jan_09_sr &lt;- terra::rast(\"Data/tmax_Jan_09.tif\")\n\n#--- change the name of the layers ---#\nterra::set.names(\n  tmax_Jan_09_sr, \n  seq(\n      lubridate::ymd(\"2009-01-01\"), \n      lubridate::ymd(\"2009-01-05\"),\n      by = \"days\"\n    )\n  )\n\n\n7.2.2 Visualize with tidyterra::geom_spatraster()\n\nCreating a map from a SpatRaster is as straightforward as creating one from an sf object. You can use tidyterra::geom_spatraster(), setting the data argument to your SpatRaster object.\n\nggplot() + geom_spatraster(data = tmax_Jan_09_sr)\n\n\n\n\n\n\n\nNotice here that the fill color was automatically set to cell values (here, tmax). tmax_Jan_09_sr is a five-layer raster object, and you might be wondering which of the five layers were used for the map above. The answer is: all of them. To select a specific layer, you can specify aes(fill = layer_name) within geom_spatraster(), as shown below:\n\nggplot() + \n  geom_spatraster(data = tmax_Jan_09_sr, aes(fill = `2009-01-01`))\n\n\n\n\n\n\n\nTo facet by layer, apply facet_wrap(~ lyr) as shown below.\n\nggplot() +\n  geom_spatraster(data = tmax_Jan_09_sr) +\n  facet_wrap(~ lyr, ncol = 2)\n\n\n\n\n\n\n\nSince this is built within the ggplot2 framework, you can easily customize the figure if you are familiar with ggplot2.\n\nggplot() +\n  geom_spatraster(data = tmax_Jan_09_sr) +\n  facet_wrap(~ lyr, ncol = 2) +\n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"º\"),\n    n.breaks = 20,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  labs(fill = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you always have lyr as the faceting variable with geom_spatraster() as there is no other variable to be faceted with in a multi-layer SpatRaster object.\n\n\n\n7.2.3 Visualize stars with geom_stars()\n\nstars::geom_stars() allows you to directly use a stars object to easily create a map within the ggplot2 framework. It works similarly to geom_sf() and geom_spatraster()—simply provide the stars object as the data argument in geom_stars().\n\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  theme_void()\n\n\n\n\n\n\n\nThe fill color of the raster cells are automatically set to the attribute (here tmax) as if aes(fill = tmax).\n\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  theme_void()\n\n\n\n\n\n\n\nBy default, geom_stars() plots only the first band. In order to present all the layers at the same time, you can add facet_wrap( ~ x) where x is the name of the third dimension of the stars object (date here).\n\nggplot() +\n  geom_stars(data = tmax_Jan_09) +\n  facet_wrap(~ date) +\n  theme_void()\n\n\n\n\n\n\n\n\n7.2.4 adding geom_sf() layers\nYou can easily add geom_sf() layers to a map created with geom_spatraster() and geom_stars(). Let’s crop the tmax data to Kansas and create a map of tmax values displayed on top of the Kansas county borders.\n\n#--- crop to KS ---#\nKS_tmax_Jan_09_stars &lt;- \n  tmax_Jan_09 %&gt;%\n  sf::st_crop(KS_county)\n\nKS_tmax_Jan_09_sr &lt;- \nas(KS_tmax_Jan_09_stars, \"SpatRaster\")\n\nMixing either geom_spatraster() or geom_stars() layers with geom_sf() layers is not any different from mixing different non-spatial geom_*()s under the ggplot2 framework. You can easily see in the example below that mixes geom_stars() and geom_sf() layers:\nwith geom_stars()\n\nggplot() +\n  geom_stars(data = KS_tmax_Jan_09_stars) +\n  geom_sf(data = KS_county, fill = NA) +\n  facet_wrap(~date) +\n  theme_void()\n\n\n\n\n\n\n\nwith geom_spatraster()\n\nggplot() +\n  geom_spatraster(data = KS_tmax_Jan_09_sr) +\n  geom_sf(data = KS_county, fill = NA) +\n  facet_wrap(~ lyr) +\n  theme_void()\n\n\n\n\n\n\n\nOne caveat with geom_stars() is that vector and raster datasets need to have the same GRS/CRS. The code below transform the CRS of KS_county from NAD83 GRS to WGS 84 / UTM zone 14N before plotting and see what happens.\n\nggplot() +\n  geom_stars(data = KS_tmax_Jan_09_stars) +\n  geom_sf(data = sf::st_transform(KS_county, 32614), fill = NA) +\n  facet_wrap(~date) +\n  theme_void()\n\n\n\n\n\n\n\nThis is not the case for geom_spatraster().\n\nggplot() +\n  geom_spatraster(data = KS_tmax_Jan_09_sr) +\n  geom_sf(data = sf::st_transform(KS_county, 32614), fill = NA) +\n  facet_wrap(~ lyr) +\n  theme_void()",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#sec-color-scale",
    "href": "chapters/07-CreateMaps-ggplot.html#sec-color-scale",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.3 Color scale",
    "text": "7.3 Color scale\n\nA color scale defines how variable values are mapped to colors in a figure. For example, in the illustration below, the color scale for fill maps the values of the cells to a gradient, with dark blue representing low values and light blue indicating high values of tmax.\n\ng_col_scale &lt;-\n  ggplot() +\n  geom_spatraster(data = tmax_Jan_09_sr) +\n  facet_wrap(~ lyr) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\nOften, it’s aesthetically desirable to customize the default color scale in ggplot2. For instance, when visualizing temperature values, you might prefer a gradient that ranges from blue for lower temperatures to red for higher ones.\nYou can control the color scale using scale_*() functions. The specific scale_*() function you need depends on the type of aesthetic (fill or color) and whether the variable is continuous or discrete. Using an incorrect scale_*() function will result in an error.\n\n7.3.1 Viridis color maps\nThe ggplot2 package provides scale_A_viridis_B() functions for applying the Viridis color map, where A refers to the type of aesthetic attribute (fill, color), and B refers to the type of variable. For example, scale_fill_viridis_c() is used for fill aesthetics applied to a continuous variable.\nThe Viridis color map offers five different palettes, which can be selected using the option argument (Figure 7.4 is a visualization of all five palettes).\n\n\n\n\nCodedata(\"geyser\", package = \"MASS\")\n\nggplot(geyser, aes(x = duration, y = waiting)) +\n  xlim(0.5, 6) +\n  ylim(40, 110) +\n  stat_density2d(aes(fill = ..level..), geom = \"polygon\") +\n  theme_bw() +\n  theme(panel.grid = element_blank()) -&gt; gg\n\n(gg + scale_fill_viridis_c(option = \"A\") + labs(x = \"magma\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"B\") + labs(x = \"inferno\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"C\") + labs(x = \"plasma\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"D\") + labs(x = \"viridis\", y = NULL)) /\n  (gg + scale_fill_viridis_c(option = \"E\") + labs(x = \"cividis\", y = NULL))\n\n\n\n\n\n\nFigure 7.4: Five Viridis options\n\n\n\n\nLet’s see what the PRISM tmax maps look like using Option A and D (default). Since the aesthetics type is fill and tmax is continuous, scale_fill_viridis_c() is the appropriate one here.\n\ng_col_scale + scale_fill_viridis_c(option = \"A\")\n\n\n\n\n\n\n\n\ng_col_scale + scale_fill_viridis_c(option = \"D\")\n\n\n\n\n\n\n\nYou can reverse the order of the color by adding direction = -1.\n\ng_col_scale + scale_fill_viridis_c(option = \"D\", direction = -1)\n\n\n\n\n\n\n\nLet’s now work on aesthetics mapping based on a discrete variable. The code below groups af_used into five groups of ranges.\n\n#--- convert af_used to a discrete variable ---#\ngw_Stevens &lt;- dplyr::mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\n\n#--- take a look ---#\nhead(gw_Stevens)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -101.4232 ymin: 37.04683 xmax: -101.1111 ymax: 37.29295\nGeodetic CRS:  NAD83\n    well_id year  af_used                   geometry    af_used_cat\n197     234 2010 461.9565  POINT (-101.4232 37.1165)      (377,508]\n198     234 2011 486.0534  POINT (-101.4232 37.1165)      (377,508]\n247     290 2010 457.4391 POINT (-101.2301 37.29295)      (377,508]\n248     290 2011 580.4156 POINT (-101.2301 37.29295) (508,1.19e+03]\n275     317 2010 258.0000 POINT (-101.1111 37.04683)      (157,264]\n276     317 2011 255.0000 POINT (-101.1111 37.04683)      (157,264]\n\n\nSince we would like to control color aesthetics based on a discrete variable, we should be using scale_color_viridis_d().\n\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used_cat), size = 2) +\n  scale_color_viridis_d(option = \"C\")\n\n\n\n\n\n\n\n\n7.3.2 RColorBrewer: scale_*_distiller() and scale_*_brewer()\n\nThe RColorBrewer package provides a set of color scales that are useful. Here is the list of color scales you can use.\n\n#--- load RColorBrewer ---#\nlibrary(RColorBrewer)\n\n#--- disply all the color schemes from the package ---#\ndisplay.brewer.all()\n\n\n\n\n\n\n\nThe first set of color palettes are sequential palettes and are suitable for a variable that has ordinal meaning: temperature, precipitation, etc. The second set of palettes are qualitative palettes and suitable for qualitative or categorical data. Finally, the third set of palettes are diverging palettes and can be suitable for variables that take both negative and positive values like changes in groundwater level.\nTwo types of scale functions can be used to use these palettes:\n\n\nscale_*_distiller() for a continuous variable\n\nscale_*_brewer() for a discrete variable\n\nTo use a specific color palette, you can simply add palette = \"palette name\" inside scale_fill_distiller(). The codes below applies “Spectral” as an example.\n\ng_col_scale + \n  theme_void() +\n  scale_fill_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\nYou can reverse the color order by adding trans = \"reverse\" option.\n\ng_col_scale + \n  theme_void() +\n  scale_fill_distiller(palette = \"Spectral\", trans = \"reverse\")\n\n\n\n\n\n\n\nIf you are specifying the color aesthetics based on a continuous variable, then you use scale_color_distiller().\n\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used), size = 2) +\n  scale_color_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\nNow, suppose the variable of interest comes with categories of ranges of values. The code below groups af_used into five ranges using ggplo2::cut_number().\n\ngw_Stevens &lt;- dplyr::mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\n\nSince af_used_cat is a discrete variable, you can use scale_color_brewer() instead.\n\nggplot(data = gw_Stevens) +\n  geom_sf(aes(color = af_used_cat), size = 2) +\n  scale_color_brewer(palette = \"Spectral\")\n\n\n\n\n\n\n\n\n7.3.3 colorspace package\nIf you are not satisfied with the viridis color map or the ColorBrewer palette options, you might want to try the colorspace package.\nHere is the palettes the colorspace package offers.\n\n#--- plot the palettes ---#\ncolorspace::hcl_palettes(plot = TRUE)\n\n\n\n\n\n\n\nThe packages offers its own scale_*() functions that follows the following naming convention:\nscale_aesthetic_datatype_colorscale where\n\naesthetic: fill or color\n\ndatatype: continuous or discrete\n\ncolorscale: qualitative, sequential, diverging, divergingx\n\n\nFor example, to add a sequential color scale to the following map, we would use scale_fill_continuous_sequential() and then pick a palette from the set of sequential palettes shown above. The code below uses the Viridis palette with the reverse option:\n\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_wrap(. ~ year) +\n  scale_fill_continuous_sequential(palette = \"Viridis\", trans = \"reverse\")\n\n\nIf you still cannot find a palette that satisfies your need (or obsession at this point), then you can easily make your own. The package offers hclwizard(), which starts shiny-based web application to let you design your own color palette.\nAfter running this,\n\nhclwizard()\n\nyou should see a web application pop up that looks like this.\n\nknitr::include_graphics(\"assets/hclwizard.png\")\n\n\n\n\n\n\n\nAfter you find a color scale you would like to use, you can go to the Exporttab, select the R tab, and then copy the code that appear in the highlighted area.\n\nknitr::include_graphics(\"assets/hclwizard_pick_theme.png\")\n\n\n\n\n\n\n\nYou could register the color palette by completing the register = option in the copied code if you think you will use it other times. Otherwise, you can delete the option.\n\ncol_palette &lt;- colorspace::sequential_hcl(n = 7, h = c(36, 200), c = c(60, NA, 0), l = c(25, 95), power = c(0.7, 1.3))\n\nWe then use the code as follows:\n\ng_col_scale + \n  theme_void() +\n  scale_fill_gradientn(colors = col_palette)\n\n\n\n\n\n\n\nNote that you are now using scale_*_gradientn() with this approach.\nFor a discrete variable, you can use scale_*_manual():\n\ncol_discrete &lt;- colorspace::sequential_hcl(n = 5, h = c(240, 130), c = c(30, NA, 33), l = c(25, 95), power = c(1, NA), rev = TRUE)\n\nggplot() +\n  geom_sf(data = gw_Stevens, aes(color = af_used_cat), size = 2) +\n  scale_color_manual(values = col_discrete)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#arranging-maps",
    "href": "chapters/07-CreateMaps-ggplot.html#arranging-maps",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.4 Arranging maps",
    "text": "7.4 Arranging maps\npatchwork combines ggplot objects (maps) using simple operators: +, /, and |. Let’s first create maps of tmax and precipitation separately.\n\n#--- tmax ---#\ntmax_20000101 &lt;- terra::rast(\"Data/PRISM/PRISM_tmax_stable_4kmD2_20000101_bil/PRISM_tmax_stable_4kmD2_20000101_bil.bil\")\n\n(\n  g_tmax &lt;- \n    ggplot() +\n    geom_spatraster(data = tmax_20000101) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n#--- ppt ---#\nppt_20000101 &lt;- terra::rast(\"Data/PRISM/PRISM_ppt_stable_4kmD2_20000101_bil/PRISM_ppt_stable_4kmD2_20000101_bil.bil\")\n\n(\n  g_ppt &lt;- \n    ggplot() +\n    geom_spatraster(data = ppt_20000101) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\nIt is best to just look at examples to get the sense of how patchwork works. A fuller treatment of patchwork is found here.\nExample 1\n\ng_tmax + g_ppt\n\n\n\n\n\n\n\nExample 2\n\ng_tmax / g_ppt / g_tmax\n\n\n\n\n\n\n\nExample 3\n\ng_tmax + g_ppt + plot_layout(nrow = 2)\n\n\n\n\n\n\n\nExample 4\n\ng_tmax + g_ppt + g_tmax + g_ppt + plot_layout(nrow = 3, byrow = FALSE)\n\n\n\n\n\n\n\nExample 5\n\ng_tmax | (g_ppt / g_tmax)\n\n\n\n\n\n\n\n\nSometimes figures are placed too close to each other. In such a case, you can pad a figure at the time of generating individual figures by adding the plot.margin option to theme(). For example, the following code creates space at the bottom of g_tmax (5 cm), and vertically stack g_tmax and g_ppt.\n\n#--- space at the bottom ---#\ng_tmax &lt;- \n  g_tmax + \n  theme(plot.margin = unit(c(0, 0, 5, 0), \"cm\"))\n\n#--- vertically stack ---#\ng_tmax / g_ppt\n\n\n\n\n\n\n\nIn plot.margin = unit(c(a, b, c, d), \"cm\"), here is which margin a, b, c, and d refers to.\n\n\na: top\n\nb: right\n\nc: bottom\n\nd: left",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#inset-maps-displaying-a-map-within-a-map",
    "href": "chapters/07-CreateMaps-ggplot.html#inset-maps-displaying-a-map-within-a-map",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.5 Inset Maps: Displaying a Map Within a Map",
    "text": "7.5 Inset Maps: Displaying a Map Within a Map\nSometimes, it is useful to present a map that covers a larger geographical range than the area of interest in the same map. This provides a better sense of the geographic extent and location of the area of interest relative to the larger geographic extent that the readers are more familiar with. For example, suppose your work is restricted to three counties in Kansas: Cheyenne, Sherman, and Wallace (Figure 7.5 presents their locations in Kansas)\n\n\n\n\nCode(\n  g_three_counties &lt;-\n    ggplot() +\n    geom_sf(data = KS_county, fill = NA) +\n    geom_sf(data = three_counties, fill = \"red\", alpha = 0.3) +\n    geom_sf_text(data = three_counties, aes(label = NAME)) +\n    theme_void()\n)\n\n\n\n\n\n\nFigure 7.5: Cheyenne, Sherman, and Wallace Counties of Kansas\n\n\n\n\n\n(\nthree_counties &lt;- dplyr::filter(KS_county, NAME %in% c(\"Cheyenne\", \"Sherman\", \"Wallace\"))\n)\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 38.69757 xmax: -101.3891 ymax: 40.00316\nGeodetic CRS:  NAD83\n  COUNTYFP     NAME                       geometry\n1      199  Wallace MULTIPOLYGON (((-102.0472 3...\n2      181  Sherman MULTIPOLYGON (((-102.0498 3...\n3      023 Cheyenne MULTIPOLYGON (((-102.0517 4...\n\n\nFor those unfamiliar with Kansas, it can be helpful to show its location on the same map (or even where Kansas is within the U.S.).\n\n7.5.1 Using the ggmapinset package\nThe goal here is to create a map like Figure 7.6.\n\n\n\n\nCodeg_three_counties &lt;-\n    ggplot() +\n    geom_sf(data = KS_county, fill = NA) +\n    geom_sf(data = three_counties, fill = \"red\", alpha = 0.3) +\n    theme_void()\n(\ng_three_counties +\n  geom_sf_inset(data = three_counties, map_base = \"none\") +\n  geom_sf_text_inset(data = three_counties, aes(label = NAME)) +\n  geom_inset_frame() +\n  coord_sf_inset(inset = \n    configure_inset(\n      #--- centroid of the three counties as the center ---#\n      centre = sf::st_centroid(sf::st_union(three_counties)),\n      #--- move 300 east and 250 south ---#\n      translation = c(-600, 0),\n      #--- radius of 80 km ---#\n      radius = 80,\n      #--- scale up by 2 ---#\n      scale = 6,\n      units = \"km\"\n    )\n  )\n)\n\n\n\n\n\n\nFigure 7.6: Inset map created with ggmapinset\n\n\n\n\nThe first step of making an inset map using ggmapinset is to create the base map layer (Kansas county borders), a part of which is going to be expanded as an inset later.\nAs the base map, we will create a map of all the counties in Kansas with only the three counties (Perkins, Chase, and Dundy) colored red.\n\n(\n  g_three_counties &lt;-\n    ggplot() +\n    geom_sf(data = KS_county, fill = NA) +\n    geom_sf(data = three_counties, fill = \"red\", alpha = 0.3) +\n    theme_void()\n)\n\n\n\n\n\n\n\nWe now configure the inset using configure_inset(). Below is the list of parameters to specify:\n\n\ncentre: The geographic coordinates of the small circle from which the enlargement begins.\n\ntranslation: The amount of shift in the x and y directions from the center to position the enlarged circle.\n\nradius: The radius of the original small circle.\n\nscale: The factor by which the circle is enlarged.\n\nunits: The unit of measurement for the radius.\n\nIt is hard to get translation, radius, and scale right the first time. After some try and errors, you can get them right. In the code below, the center of the circle is set at the centroid of the three counties combined (sf::st_centroid(sf::st_union(three_counties)). translation = c(300, -250) along with units = \"km\" means that the inset map (map inside the circle in Figure 7.6) is 300km east and 250km south (-250 north) to the base map. The radius of the circle is specified to be 80km with radius = 80.\n\ninset_config &lt;- \n  configure_inset(\n    #--- centroid of the three counties as the center ---#\n    centre = sf::st_centroid(sf::st_union(three_counties)),\n    #--- move 300 east and 250 south ---#\n    translation = c(300, -250),\n    #--- radius of 80 km ---#\n    radius = 80,\n    #--- scale up by 2 ---#\n    scale = 2,\n    units = \"km\"\n  )\n\nWith the inset configuration ready, we can now add the inset. For this, use the following functions:\n\n\ngeom_sf_inset() and/or geom_sf_text_inset() to create the layers that will appear in the inset (the circle).\n\ngeom_inset_frame() to add the frame, which includes the small circle, the large circle, and connecting lines.\n\ncoord_sf_inset(inset = inset_config) to apply the configuration you set up earlier.\n\n\n(\ng_three_counties +\n  geom_sf_inset(data = three_counties, map_base = \"none\") +\n  geom_sf_text_inset(data = three_counties, aes(label = NAME)) +\n  geom_inset_frame() +\n  coord_sf_inset(inset = inset_config)\n)\n\n\n\n\n\n\n\nWell, this is not quite right. Let’s re-configure the inset parameters like below:\n\ninset_config &lt;- \n  configure_inset(\n    #--- centroid of the three counties as the center ---#\n    centre = sf::st_centroid(sf::st_union(three_counties)),\n    #--- move 300 east and 250 south ---#\n    translation = c(-600, 0),\n    #--- radius of 80 km ---#\n    radius = 80,\n    #--- scale up by 2 ---#\n    scale = 6,\n    units = \"km\"\n  )\n\n\n(\ng_three_counties +\n  geom_sf_inset(data = three_counties, map_base = \"none\") +\n  geom_sf_text_inset(data = three_counties, aes(label = NAME)) +\n  geom_inset_frame() +\n  coord_sf_inset(inset = inset_config)\n)\n\n\n\n\n\n\n\nHere, scale is set to a higher number at 6 so that the inset map is much bigger relative to the base map. translation was also altered significantly so that the inset map does not overlap with the base map after scaling up the inset map.\n\n\n\n\n\n\nNote on map_base\n\n\n\nBy default, geom_sf_inset() creates two copies of the map layer specified by geom_sf_inset() and/or geom_sf_text_inset(): one for the base map and the other for the inset map. map_base option determines whether you create the copy for the base map or not.\nIn the code below, map_base = \"none\" is not specified unlike the map created just above, meaning that geom_sf_inset(data = three_counties, fill = \"black\") will be applied for both the base and inset maps. Consequently, you see that the default fill color of “gray” is used for both the inset and base maps, overwriting the aesthetics for the three counties in the base map.\n\n(\ng_three_counties +\n  geom_sf_inset(data = three_counties) +\n  geom_sf_text_inset(data = three_counties, aes(label = NAME)) +\n  geom_inset_frame() +\n  coord_sf_inset(inset = inset_config)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit the ggmapinset website for more examples and other functionalities beyond what is presented here, including multiple insets.\n\n\n\n7.5.2 Using ggplotGrob() and annotation_custom()\n\nYou can also use ggplotGrob() and annotation_custom() to create an inset map. The steps to achieve this are as follows:\n\ncreate a map of the area of interest and turn it into a grob object using ggplotGrob()\n\ncreate a map of the region that includes the area of interest and turn it into a grob object using ggplotGrob()\n\ncombine the two using annotation_custom()\n\n\nStep 1\nLet’s first create a map of the area of interest (Figure 7.7).\n\n#--- Create a map of the area of interest ---#\n(\ng_three_counties &lt;-\n  ggplot() +\n  geom_sf(data = three_counties) +\n  geom_sf_text(data = three_counties, aes(label = NAME)) +\n  theme_void()\n)\n\n\n\n\n\n\nFigure 7.7: Map of the area of interest\n\n\n\n\nLet’s now convert g_three_counties to a grob object using ggplotGrob().\n\n#--- convert the ggplot into a grob ---#\ngrob_aoi &lt;- ggplotGrob(g_three_counties)\n\n#--- check the class ---#\nclass(grob_aoi)\n\n[1] \"gtable\" \"gTree\"  \"grob\"   \"gDesc\" \n\n\n\nStep 2\nCreate a map of the region that include the area of interest (Figure 7.8) and then turn it into a grob.\n\n#--- create a map of Kansas ---#\n(\ng_region &lt;-\n  ggplot() +\n  geom_sf(data = KS_county) +\n  geom_sf(data = three_counties, fill = \"blue\", color = \"red\", alpha = 0.5) +\n  theme_void()\n)\n\n#--- convert to a grob ---#\ngrob_region &lt;- ggplotGrob(g_region)\n\n\n\n\n\n\nFigure 7.8: Map of the region that include the area of interest\n\n\n\n\n\nStep 3\nNow that we have two maps, we can now put them on the same map using annotation_custom(). The first task is to initiate a ggplot with coord_equal() as follows:\n\n(\n  g_inset &lt;- \n    ggplot() +\n    coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE)\n)\n\n\n\n\n\n\n\nYou now have a blank canvas to put the images on. Let’s add a layer to the canvas using annotation_custom() in which you provide the grob object (a map) and specify the range of the canvas the map occupies. Since the extent of x and y are set to [0, 1] above with coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE), the following code put the grob_aoi to cover the entire y range and up to 0.8 of x from 0.\n\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  )\n\n\n\n\n\n\n\nSimilarly, we can add grob_region using annotation_custom(). Let’s put it at the right lower corner of the map.\n\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  ) +\n  annotation_custom(grob_region,\n    xmin = 0.6, xmax = 1, ymin = 0,\n    ymax = 0.3\n  )\n\n\n\n\n\n\n\nNote that the resulting map still has the default theme because it does not inherit the theme of maps added by annotation_custom(). So, you can add theme_void() to the map to make the border disappear.\n\ng_inset +\n  annotation_custom(grob_aoi,\n    xmin = 0, xmax = 0.8, ymin = 0,\n    ymax = 1\n  ) +\n  annotation_custom(grob_region,\n    xmin = 0.6, xmax = 1, ymin = 0,\n    ymax = 0.3\n  ) +\n  theme_void()",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#sec-fine-tune",
    "href": "chapters/07-CreateMaps-ggplot.html#sec-fine-tune",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.6 Fine-tuning maps for publication",
    "text": "7.6 Fine-tuning maps for publication\nThis section offers several tips to enhance the appearance of your maps, making them suitable for publication in professional reports. Academic journals often have their own guidelines for figures, and reviewers or supervisors may have specific preferences for how maps should look. Regardless of the requirements, you will need to adapt your maps accordingly. It is worth noting that these techniques are not exclusive to maps—they apply to any type of figure. However, we will focus on specific components of a figure that are commonly modified when creating maps. In particular, we will cover how to use pre-made ggplot2 themes and how to adjust legends and facet strips.\n\n(\n  gw_by_county &lt;- \n    sf::st_join(KS_county, gw_KS_sf) %&gt;%\n    data.table() %&gt;%\n    .[, .(af_used = sum(af_used, na.rm = TRUE)), by = .(COUNTYFP, year)] %&gt;%\n    left_join(KS_county, ., by = c(\"COUNTYFP\")) %&gt;%\n    filter(!is.na(year))\n)\n\nSimple feature collection with 184 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   COUNTYFP     NAME year    af_used                       geometry\n1       175   Seward 2010 163703.719 MULTIPOLYGON (((-101.0681 3...\n2       175   Seward 2011 218863.983 MULTIPOLYGON (((-101.0681 3...\n3       027     Clay 2010   5929.143 MULTIPOLYGON (((-97.3707 39...\n4       027     Clay 2011   8358.903 MULTIPOLYGON (((-97.3707 39...\n5       171    Scott 2010  42542.602 MULTIPOLYGON (((-101.1284 3...\n6       171    Scott 2011  51620.454 MULTIPOLYGON (((-101.1284 3...\n7       047  Edwards 2010 109726.389 MULTIPOLYGON (((-99.56988 3...\n8       047  Edwards 2011 141487.791 MULTIPOLYGON (((-99.56988 3...\n9       147 Phillips 2010   4657.746 MULTIPOLYGON (((-99.62821 3...\n10      147 Phillips 2011   5076.198 MULTIPOLYGON (((-99.62821 3...\n\n\n\n(\n  g_base &lt;- ggplot() +\n    geom_sf(data = gw_by_county, aes(fill = af_used)) +\n    facet_wrap(. ~ year)\n)\n\n\n\n\n\n\n\n\n7.6.1 Setting the theme\nRight now, the map shows geographic coordinates, gray background, and grid lines. They are not very aesthetically appealing. Adding the pre-defined theme by theme_*() can alter the theme of a map very quickly. One of the themes suitable for maps is theme_void().\n\ng_base + theme_void()\n\n\n\n\n\n\n\nAs you can see, all the axes information (axis.title, axis.ticks, axis.text,) and panel information (panel.background, panel.border, panel.grid) are gone among other parts of the figure. You can confirm this by evaluating theme_void().\n\ntheme_void()\n\nList of 136\n $ line                            : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ rect                            : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.title.x                    : NULL\n $ axis.title.x.top                : NULL\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    : NULL\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              : NULL\n $ axis.text                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.text.x                     : NULL\n $ axis.text.x.top                 : NULL\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     : NULL\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               : NULL\n $ axis.text.theta                 : NULL\n $ axis.text.r                     : NULL\n $ axis.ticks                      : NULL\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : NULL\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 0points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'simpleUnit' num 0points\n  ..- attr(*, \"unit\")= int 8\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : NULL\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               : NULL\n $ legend.margin                   : NULL\n $ legend.spacing                  : NULL\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : NULL\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"right\"\n $ legend.position.inside          : NULL\n $ legend.direction                : NULL\n $ legend.byrow                    : NULL\n $ legend.justification            : NULL\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : NULL\n $ legend.box.just                 : NULL\n $ legend.box.margin               : NULL\n $ legend.box.background           : NULL\n $ legend.box.spacing              : NULL\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\nApplying the theme to a map obviates the need to suppress parts of a figure individually. You can suppress parts of the figure individually using theme(). For example, the following code gets rid of axis.text.\n\ng_base + theme(axis.text = element_blank())\n\n\n\n\n\n\n\nSo, if theme_void() is overdoing things, you can build your own theme specifically for maps. For example, this is the theme I used for maps in Chapter 1.\n\ntheme_for_map &lt;-\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    axis.line = element_blank(),\n    panel.border = element_blank(),\n    panel.grid = element_line(color = \"transparent\"),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n  )\n\nApplying the theme to the map:\n\ng_base + theme_for_map\n\n\n\n\n\n\n\nThis is very similar to theme_void() except that strip.background is not lost.\nYou can use theme_void() as a starting point and override components of it like this.\n\n#--- bring back a color to strip.background  ---#\ntheme_for_map_2 &lt;- theme_void() + theme(strip.background = element_rect(fill = \"gray\"))\n\n#--- apply the new theme ---#\ng_base + theme_for_map_2\n\n\n\n\n\n\n\n\ntheme_bw() is also a good theme for maps.\n\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_grid(year ~ .) +\n  theme_bw()\n\n\n\n\n\n\n\nIf you do not like the gray grid lines, you can remove them like this.\n\nggplot() +\n  geom_sf(data = gw_by_county, aes(fill = af_used)) +\n  facet_grid(year ~ .) +\n  theme_bw() +\n  theme(\n    panel.grid = element_line(color = \"transparent\")\n  )\n\n\n\n\n\n\n\n\nNot all themes are suitable for maps. For example, theme_classic() is not a very good option as you can see below:\n\ng_base + theme_classic()\n\n\n\n\n\n\n\nIf you are not satisfied with theme_void() and not willing to make up your own theme, then you may want to take a look at other pre-made themes that are available from ggplot2 (see here) and ggthemes (see here). Note that some themes are more invasive than theme_void(), altering the default color scale.\n\n7.6.2 Legend\nLegends can be modified using legend.*() options for theme() and guide_*(). It is impossible to discuss every single one of all the options for these functions. So, this section focuses on the most common and useful (that I consider) modifications you can make to legends.\nA legend consists of three elements: legend title, legend key (e.g., color bar), and legend label (or legend text). For example, in the figure below, af_used is the legend title, the color bar is the legend key, and the numbers below the color bar are legend labels. Knowing the name of these elements helps because the name of the options contains the name of the specific part of the legend.\n\n(\n  g_legend &lt;- \n    ggplot() +\n    geom_sf(data = gw_by_county, aes(fill = af_used)) +\n    facet_wrap(. ~ year) +\n    theme_void()\n)\n\n\n\n\n\n\n\nLet’s first change the color scale to Viridis using scale_fill_viridis_c() (see Section 7.3 for picking a color scale).\n\ng_legend +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\nRight now, the legend title is af_used, which does not tell the readers what it means. In general, you can change the title of a legend by using the name option inside the scale function for the legend (here, scale_fill_viridis_c()). So, this one works:\n\ng_legend +\n  scale_fill_viridis_c(name = \"Groundwater pumping (acre-feet)\")\n\n\n\n\n\n\n\nAlternatively, you can use labs() function.3. Since the legend is for the fill aesthetic attribute, you should add fill = \"legend title\" as follows:\n3 labs() can also be used to specify the x- and y-axis titles.\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\")\n\n\n\n\n\n\n\nSince the legend title is long, the legend is taking up about the half of the space of the entire figure. So, let’s put the legend below the maps (bottom of the figure) by adding theme(legend.position = \"bottom\").\n\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIt would be aesthetically better to have the legend title on top of the color bar. This can be done by using the guides() function. Since we would like to alter the aesthetics of the legend for fill involving a color bar, we use fill = guide_colorbar(). To place the legend title on top, you add title.position=\"top\" inside the guide_colorbar() function as follows:\n\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"top\"))\n\n\n\n\n\n\n\nThis looks better. But, the legend labels are too close to each other so it is hard to read them because the color bar is too short. Let’s elongate the color bar so that we have enough space between legend labels using legend.key.width = option for theme(). Let’s also make the legend thinner using legend.key.height = option.\n\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(\n    legend.position = \"bottom\",\n    #--- NEW LINES HERE!! ---#\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\")\n  ) +\n  guides(fill = guide_colorbar(title.position = \"top\"))\n\n\n\n\n\n\n\nIf the journal you are submitting an article to is requesting a specific font family for the texts in the figure, you can use legend.text = element_text() and legend.title = element_text() inside theme() for the legend labels and legend title, respectively. The following code uses the font family of “Times” and font size of 12 for both the labels and the title.\n\ng_legend +\n  scale_fill_viridis_c() +\n  labs(fill = \"Groundwater pumping (acre-feet)\") +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.text = element_text(size = 12, family = \"Times\"),\n    legend.title = element_text(size = 12, family = \"Times\")\n    #--- NEW LINES HERE!! ---#\n  ) +\n  guides(fill = guide_colorbar(title.position = \"top\"))\n\n\n\n\n\n\n\nFor the other options to control the legend with a color bar, see here.\n\nWhen the legend is made for discrete values, you can use guide_legend(). Let’s use the following map as a starting point.\n\n#--- convert af_used to a discrete variable ---#\ngw_Stevens &lt;- dplyr::mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5))\n\n\n(\n  g_legend_2 &lt;-\n    ggplot(data = gw_Stevens) +\n    geom_sf(aes(color = af_used_cat), size = 2) +\n    scale_color_viridis(discrete = TRUE, option = \"C\") +\n    labs(color = \"Groundwater pumping (acre-feet)\") +\n    theme_void() +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\nThe legend is too long, so first put the legend title on top of the legend labels using the code below:\n\ng_legend_2 +\n  guides(\n    color = guide_legend(title.position = \"top\")\n  )\n\n\n\n\n\n\n\nSince the legend is for the color aesthetic attribute, color = guide_legend() was used. The legend labels are still a bit too long, so let’s arrange them in two rows using the nrow = option.\n\ng_legend_2 +\n  guides(\n    color = guide_legend(title.position = \"top\", nrow = 2)\n  )\n\n\n\n\n\n\n\nFor the other options for guide_legend(), see here.\n\n\n7.6.3 Facet strips\nFacet strips refer to the area of boxed where the values of faceting variables are printed. In the figure below, it’s gray strips on top of the maps. You can change how they look using strip.* options in theme() and also partially inside facet_wrap() and facet_grid(). Here is the list of available options:\n\n\nstrip.background, strip.background.x, strip.background.y\n\nstrip.placement\n\nstrip.text, strip.text.x, strip.text.y\n\nstrip.switch.pad.grid\nstrip.switch.pad.wrap\n\n\nggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap((af_used &gt; 500) ~ year) +\n  theme_void() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nTo make texts in the strips more descriptive of what they actually mean you can make variable that have texts you want to show on the map as their values.\n\ngw_KS_sf &lt;-\n  gw_KS_sf %&gt;%\n  mutate(\n    high_low = ifelse(af_used &gt; 500, \"High water use\", \"Low water use\"),\n    year_txt = paste(\"Year: \", year)\n  )\n\n\n(\n  g_facet &lt;-\n    ggplot() +\n    #--- KS county boundary ---#\n    geom_sf(data = sf::st_transform(KS_county, 32614)) +\n    #--- wells ---#\n    geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n    #--- facet by year (side by side) ---#\n    facet_wrap(high_low ~ year_txt) +\n    theme_void() +\n    scale_color_viridis_c() +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\nYou probably noticed that the high water use cases are now appear on top. This is because the panels of figures are arranged in a way that the strip texts are alphabetically ordered. High water use precedes Low water use. Sometimes, this is not desirable. To force a specific order, you can turn the faceting variable (here high_low) into a factor with the order of its values defined using levels =. The following code converts high_low into a factor where “Low water use” is the first level and “High water use” is the second level.\n\ngw_KS_sf &lt;-\n  mutate(\n    gw_KS_sf,\n    high_low = factor(\n      high_low,\n      levels = c(\"Low water use\", \"High water use\")\n    )\n  )\n\nNow, “Low water use” cases appear first (on top).\n\ng_facet &lt;- \n  ggplot() +\n  #--- KS county boundary ---#\n  geom_sf(data = sf::st_transform(KS_county, 32614)) +\n  #--- wells ---#\n  geom_sf(data = gw_KS_sf, aes(color = af_used)) +\n  #--- facet by year (side by side) ---#\n  facet_wrap(high_low ~ year_txt) +\n  theme_void() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"bottom\")\n\ng_facet\n\n\n\n\n\n\n\nYou can control how strip texts and strip boxes appear using strip.text and strip.background options. Here is an example:\n\ng_facet + \n  theme(\n    strip.text.x = element_text(size = 12, family = \"Times\", color = \"blue\"),\n    strip.background = element_rect(fill = \"red\", color = \"black\")\n  )\n\n\n\n\n\n\n\nInstead of having descriptions of cases on top of the figures, you could have one of the descriptions on the right side of the figures using facet_grid().\n\ng_facet +\n  #--- this overrides facet_wrap(high_low ~ year_txt) ---#\n  facet_grid(high_low ~ year_txt)\n\n\n\n\n\n\n\nNow case descriptions for high_low are too long and it is squeezing the space for maps. Let’s flip high_low and year.\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low)\n\n\n\n\n\n\n\nThis is slightly better than before, but not much. Let’s rotate strip texts for year using the angle = option.\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90)\n  )\n\n\n\n\n\n\n\nSince we only want to change the angle of strip texts for the second faceting variable, we need to work on strip.text.y (if you want to work on the first one, you use strip.text.x.).\nLet’s change the size of the strip texts to 12 and use Times font family.\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    #--- moves up the strip texts ---#\n    strip.text.x = element_text(size = 12, family = \"Times\")\n  )\n\n\n\n\n\n\n\nThe strip texts for high_low are too close to maps that letter “g” in “High” is truncated. Let’s move them up.\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    #--- moves up the strip texts ---#\n    strip.text.x = element_text(vjust = 2, size = 12, family = \"Times\")\n  )\n\n\n\n\n\n\n\nNow the upper part of the letters is truncated. We could just put more margin below the texts using the margin = margin(top, right, bottom, left, unit in text) option.\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, \"cm\"), size = 12, family = \"Times\")\n  )\n\n\n\n\n\n\n\nFor completeness, let’s make the legend look better as well (this is discussed in Section 7.6.1).\n\ng_facet +\n  #--- this overrides facet_grid(high_low ~ year_txt) ---#\n  facet_grid(year_txt ~ high_low) +\n  theme(\n    strip.text.y = element_text(angle = -90, size = 12, family = \"Times\"),\n    strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, \"cm\"), size = 12, family = \"Times\")\n  ) +\n  theme(legend.position = \"bottom\") +\n  labs(color = \"Groundwater pumping (acre-feet)\") +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(2, \"cm\"),\n    legend.text = element_text(size = 12, family = \"Times\"),\n    legend.title = element_text(size = 12, family = \"Times\")\n    #--- NEW LINES HERE!! ---#\n  ) +\n  guides(color = guide_colorbar(title.position = \"top\"))\n\n\n\n\n\n\n\nAlright, setting aside the problem of whether the information provided in the maps is meaningful or not, the maps look great at least.\n\n7.6.4 North arrow and scale bar\nThe ggspatial package lets you put a north arrow and scale bar on a map using annotation_scale() and annotation_north_arrow().\n\n#--- get North Carolina county borders ---#\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n(\n  #--- create a simple map ---#\n  g_nc &lt;- ggplot(nc) +\n    geom_sf() +\n    theme_void()\n)\n\n\n\n\n\n\n\nHere is an example code that adds a scale bar:\n\n#--- load ggspatial ---#\nlibrary(ggspatial)\n\n#--- add scale bar ---#\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2\n  )\n\nlocation determines where the scale bar is. The first letter is either t (top) or b (bottom), and the second letter is either l (left) or r (right). width_hint is the length of the scale bar relative to the plot. The distance number (200 km) was generated automatically according to the length of the bar.\nYou can add pads from the plot border to fine tune the location of the scale bar:\n\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2,\n    pad_x = unit(3, \"cm\"),\n    pad_y = unit(2, \"cm\")\n  )\n\n\n\n\n\n\n\nA positive number means that the scale bar will be placed further away from closest border of the plot.\nYou can add a north arrow using annotation_north_arrow(). Accepted arguments are similar to those for annotation_scale().\n\ng_nc +\n  annotation_scale(\n    location = \"bl\",\n    width_hint = 0.2,\n    pad_x = unit(3, \"cm\")\n  ) +\n  #--- add north arrow ---#\n  annotation_north_arrow(\n    location = \"tl\",\n    pad_x = unit(0.5, \"in\"),\n    pad_y = unit(0.1, \"in\"),\n    style = north_arrow_fancy_orienteering\n  )\n\n\n\n\n\n\n\nThere are several styles you can pick from. Run ?north_arrow_orienteering to see other options.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/07-CreateMaps-ggplot.html#sec-ggsave",
    "href": "chapters/07-CreateMaps-ggplot.html#sec-ggsave",
    "title": "7  Creating Maps using ggplot2",
    "section": "\n7.7 Saving a ggplot object as an image",
    "text": "7.7 Saving a ggplot object as an image\nMaps created with ggplot2 can be saved using ggsave() with the following syntax:\n\nggsave(filename = file name, plot = ggplot object)\n\n#--- or just this ---#\nggsave(file name, ggplot object)\n\nMany different file formats are supported including pdf, svg, eps, png, jpg, tif, etc. One thing you want to keep in mind is the type of graphics:\n\nvector graphics (pdf, svg, eps)\nraster graphics (jpg, png, tif)\n\nWhile vector graphics are scalable, raster graphics are not. If you enlarge raster graphics, the cells making up the figure become visible, making the figure unappealing. So, unless it is required to save figures as raster graphics, it is encouraged to save figures as vector graphics.\nLet’s try to save the following ggplot object.\n\nCode#--- get North Carolina county borders ---#\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nCode(\n  #--- create a map ---#\n  g_nc &lt;- \n    ggplot(nc) +\n    geom_sf()\n)\n\n\n\n\n\n\nFigure 7.9: Map to save\n\n\n\n\nggsave() automatically detects the file format from the file name. For example, the following code saves g_nc as nc.pdf. ggsave() knows the ggplot object was intended to be saved as a pdf file from the extension of the specified file name.\n\nggsave(\"nc.pdf\", g_nc)\n\nSimilarly,\n\n#--- save as an eps file ---#\nggsave(\"nc.eps\", g_nc)\n\n#--- save as an eps file ---#\nggsave(\"nc.svg\", g_nc)\n\nYou can change the output size with height and width options. For example, the following code creates a pdf file of height = 5 inches and width = 7 inches.\n\nggsave(\"nc.pdf\", g_nc, height = 5, width = 7)\n\nYou change the unit with the units option. By default, in (inches) is used.\nYou can control the resolution of the output image by specifying DPI (dots per inch) using the dpi option. The default DPI value is 300, but you can specify any value suitable for the output image, including “retina” (320) or “screen” (72). 600 or higher is recommended when a high resolution output is required.\n\n#--- dpi = 320 ---#\nggsave(\"nc_dpi_320.png\", g_nc, height = 5, width = 7, dpi = 320)\n\n#--- dpi = 72 ---#\nggsave(\"nc_dpi_screen.png\", g_nc, height = 5, width = 7, dpi = \"screen\")",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating Maps using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html",
    "href": "chapters/08-DownloadSpatialData.html",
    "title": "8  Download and process spatial datasets from within R",
    "section": "",
    "text": "Before you start\nThere are many publicly available spatial datasets that can be downloaded using R. Programming data downloading using R instead of manually downloading data from websites can save lots of time and also enhances the reproducibility of your analysis. In this section, we will introduce some of such datasets and show how to download and process those data.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#before-you-start",
    "href": "chapters/08-DownloadSpatialData.html#before-you-start",
    "title": "8  Download and process spatial datasets from within R",
    "section": "",
    "text": "Direction for replication\nDatasets\nNo datasets to download for this Chapter.\nPackages\n\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  stars, # spatiotemporal data handling\n  terra, # raster data handling\n  raster, # raster data handling\n  sf, # vector data handling\n  dplyr, # data wrangling\n  stringr, # string manipulation\n  lubridate, # dates handling\n  data.table, # data wrangling\n  tidyr, # reshape\n  tidyUSDA, # download USDA NASS data\n  keyring, # API key management\n  FedData, # download Daymet data\n  daymetr, # download Daymet data\n  ggplot2, # make maps\n  ggthemes, # ggplot themes\n  future.apply, # parallel processing\n  CropScapeR, # download CDL data\n  prism, # download PRISM data\n  exactextractr # extract raster values to sf\n)\n\n\nRun the following code to define the theme for map:\n\n\nggplot2::theme_set(theme_bw())\n\ntheme_for_map &lt;- theme(\n  axis.ticks = element_blank(),\n  axis.text = element_blank(),\n  axis.line = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(color = \"transparent\"),\n  panel.grid.minor = element_line(color = \"transparent\"),\n  panel.background = element_blank(),\n  plot.background = element_rect(fill = \"transparent\", color = \"transparent\")\n)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#sec-nass-quick",
    "href": "chapters/08-DownloadSpatialData.html#sec-nass-quick",
    "title": "8  Download and process spatial datasets from within R",
    "section": "\n8.1 USDA NASS QuickStat with tidyUSDA\n",
    "text": "8.1 USDA NASS QuickStat with tidyUSDA\n\nThere are several packages available to download data from the USDA NASS QuickStat. In this example, we will use the tidyUSDA package (Lindblad 2020). A great feature of tidyUSDA is that it allows you to download data as an sf object, which means you can immediately visualize or spatially interact with other spatial data.\nThe first step is to obtain an API key from the this website, as you will need it to download data.\nTo download data, use the tidyUSDA::getQuickstat() function. There are numerous options to narrow the scope of the data, such as data_item, geographic_level, year, commodity, and more. You can refer to the package manual for a complete list of available parameters.\nAs an example, the code below downloads corn-related data by county in Illinois for the year 2016 as an sf object:\n\n(\n  IL_corn_yield &lt;-\n    getQuickstat(\n      #--- put your API key in place of key_get(\"usda_nass_qs_api\") ---#\n      key = key_get(\"usda_nass_qs_api\"),\n      program = \"SURVEY\",\n      commodity = \"CORN\",\n      geographic_level = \"COUNTY\",\n      state = \"ILLINOIS\",\n      year = \"2016\",\n      geometry = TRUE\n    ) %&gt;%\n    #--- keep only some of the variables ---#\n    dplyr::select(\n      year, county_name, county_code, state_name,\n      state_fips_code, short_desc, Value\n    )\n)\n\n\n\nSimple feature collection with 384 features and 7 fields (with 16 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.01993 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year county_name county_code state_name state_fips_code           short_desc\n1  2016      BUREAU         011   ILLINOIS              17 CORN - ACRES PLANTED\n2  2016     CARROLL         015   ILLINOIS              17 CORN - ACRES PLANTED\n3  2016       HENRY         073   ILLINOIS              17 CORN - ACRES PLANTED\n4  2016  JO DAVIESS         085   ILLINOIS              17 CORN - ACRES PLANTED\n5  2016         LEE         103   ILLINOIS              17 CORN - ACRES PLANTED\n6  2016      MERCER         131   ILLINOIS              17 CORN - ACRES PLANTED\n7  2016        OGLE         141   ILLINOIS              17 CORN - ACRES PLANTED\n8  2016      PUTNAM         155   ILLINOIS              17 CORN - ACRES PLANTED\n9  2016 ROCK ISLAND         161   ILLINOIS              17 CORN - ACRES PLANTED\n10 2016  STEPHENSON         177   ILLINOIS              17 CORN - ACRES PLANTED\n    Value                       geometry\n1  273500 MULTIPOLYGON (((-89.85691 4...\n2  147500 MULTIPOLYGON (((-90.16133 4...\n3  235000 MULTIPOLYGON (((-90.43247 4...\n4  100500 MULTIPOLYGON (((-90.50668 4...\n5  258500 MULTIPOLYGON (((-89.63118 4...\n6  142500 MULTIPOLYGON (((-90.99255 4...\n7  228000 MULTIPOLYGON (((-89.68598 4...\n8   37200 MULTIPOLYGON (((-89.33303 4...\n9   65000 MULTIPOLYGON (((-90.33573 4...\n10 179500 MULTIPOLYGON (((-89.92577 4...\n\n\nAs you can see, the result is an sf object with a geometry column, thanks to the geometry = TRUE option. This allows you to immediately create a map with the data (Figure 8.1).\n\n\n\n\nCodeggplot() +\n  geom_sf(\n    data = filter(IL_corn_yield, short_desc == \"CORN, GRAIN - YIELD, MEASURED IN BU / ACRE\"),\n    aes(fill = Value)\n  ) +\n  theme_for_map\n\n\n\n\n\n\nFigure 8.1: Corn Yield (bu/acre) in Illinois in 2016\n\n\n\n\nYou can also download data for multiple states and years simultaneously, as shown in the example below. If you want data for the entire U.S., simply omit the state parameter.\n\n(\n  IL_CO_NE_corn &lt;-\n    getQuickstat(\n      key = key_get(\"usda_nass_qs_api\"),\n      program = \"SURVEY\",\n      commodity = \"CORN\",\n      geographic_level = \"COUNTY\",\n      state = c(\"ILLINOIS\", \"COLORADO\", \"NEBRASKA\"),\n      year = paste(2014:2018),\n      geometry = TRUE\n    ) %&gt;%\n    #--- keep only some of the variables ---#\n    dplyr::select(\n      year, county_name, county_code, state_name,\n      state_fips_code, short_desc, Value\n    )\n)\n\n\n\nSimple feature collection with 6384 features and 7 fields (with 588 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0459 ymin: 36.9703 xmax: -87.01993 ymax: 43.00171\nGeodetic CRS:  NAD83\nFirst 10 features:\n   year               county_name county_code state_name state_fips_code\n1  2018 OTHER (COMBINED) COUNTIES         998   COLORADO              08\n2  2017 OTHER (COMBINED) COUNTIES         998   COLORADO              08\n3  2016 OTHER (COMBINED) COUNTIES         998   COLORADO              08\n4  2015 OTHER (COMBINED) COUNTIES         998   COLORADO              08\n5  2014 OTHER (COMBINED) COUNTIES         998   COLORADO              08\n6  2017                   BOULDER         013   COLORADO              08\n7  2016                   BOULDER         013   COLORADO              08\n8  2016                   LARIMER         069   COLORADO              08\n9  2015                   LARIMER         069   COLORADO              08\n10 2014                   LARIMER         069   COLORADO              08\n             short_desc  Value                       geometry\n1  CORN - ACRES PLANTED 107600             MULTIPOLYGON EMPTY\n2  CORN - ACRES PLANTED 108900             MULTIPOLYGON EMPTY\n3  CORN - ACRES PLANTED 163600             MULTIPOLYGON EMPTY\n4  CORN - ACRES PLANTED   3100             MULTIPOLYGON EMPTY\n5  CORN - ACRES PLANTED   5200             MULTIPOLYGON EMPTY\n6  CORN - ACRES PLANTED   3000 MULTIPOLYGON (((-105.6486 4...\n7  CORN - ACRES PLANTED   3300 MULTIPOLYGON (((-105.6486 4...\n8  CORN - ACRES PLANTED  12800 MULTIPOLYGON (((-105.8225 4...\n9  CORN - ACRES PLANTED  14900 MULTIPOLYGON (((-105.8225 4...\n10 CORN - ACRES PLANTED  13600 MULTIPOLYGON (((-105.8225 4...\n\n\n\n8.1.1 Look for parameter values\nThis package includes a function that allows you to view all possible parameter values for many of the parameters. For instance, if you know you want data on irrigated corn yields in Colorado but are unsure of the exact string to provide for the data_item parameter, you can do the following:1\n1 Alternatively, you can visit the QuickStat website to find the correct text values.\n#--- get all the possible values for data_item ---#\nall_items &lt;- tidyUSDA::allDataItem\n\n#--- take a look at the first six ---#\nhead(all_items)\n\n                                                                           short_desc1 \n                                                                     \"AG LAND - ACRES\" \n                                                                           short_desc2 \n                                                      \"AG LAND - NUMBER OF OPERATIONS\" \n                                                                           short_desc3 \n                                                   \"AG LAND - OPERATIONS WITH TREATED\" \n                                                                           short_desc4 \n                                                \"AG LAND - TREATED, MEASURED IN ACRES\" \n                                                                           short_desc5 \n                           \"AG LAND, (EXCL CROPLAND & PASTURELAND & WOODLAND) - ACRES\" \n                                                                           short_desc6 \n\"AG LAND, (EXCL CROPLAND & PASTURELAND & WOODLAND) - AREA, MEASURED IN PCT OF AG LAND\" \n\n\nYou can use key words like “CORN”, “YIELD”, “IRRIGATED” to narrow the entire list using grep()2:\n2 grep() is part of the base package, so no additional package install is needed.\nall_items %&gt;%\n  grep(pattern = \"CORN\", ., value = TRUE) %&gt;%\n  grep(pattern = \"YIELD\", ., value = TRUE) %&gt;%\n  grep(pattern = \"IRRIGATED\", ., value = TRUE)\n\n                                                          short_desc9227 \n                 \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9228 \n     \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE\" \n                                                          short_desc9233 \n    \"CORN, GRAIN, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9236 \n   \"CORN, GRAIN, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9238 \n   \"CORN, GRAIN, IRRIGATED, PART OF CROP - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9249 \n             \"CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / ACRE\" \n                                                          short_desc9250 \n \"CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE\" \n                                                          short_desc9291 \n              \"CORN, SILAGE, IRRIGATED - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9296 \n \"CORN, SILAGE, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9299 \n\"CORN, SILAGE, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9301 \n\"CORN, SILAGE, IRRIGATED, PART OF CROP - YIELD, MEASURED IN TONS / ACRE\" \n                                                          short_desc9307 \n          \"CORN, SILAGE, NON-IRRIGATED - YIELD, MEASURED IN TONS / ACRE\" \n                                                         short_desc28557 \n                 \"SWEET CORN, IRRIGATED - YIELD, MEASURED IN CWT / ACRE\" \n                                                         short_desc28564 \n             \"SWEET CORN, NON-IRRIGATED - YIELD, MEASURED IN CWT / ACRE\" \n\n\nLooking at the list, we know the exact text value we want, which is the first entry of the vector.\n\n(\n  CO_ir_corn_yield &lt;-\n    getQuickstat(\n      key = key_get(\"usda_nass_qs_api\"),\n      program = \"SURVEY\",\n      data_item = \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\",\n      geographic_level = \"COUNTY\",\n      state = \"COLORADO\",\n      year = \"2018\",\n      geometry = TRUE\n    ) %&gt;%\n    #--- keep only some of the variables ---#\n    dplyr::select(year, NAME, county_code, short_desc, Value)\n)\n\nBelow is the complete list of functions that provide the possible values for the parameters used in getQuickstat().\n\ntidyUSDA::allCategory\ntidyUSDA::allSector\ntidyUSDA::allGroup\ntidyUSDA::allCommodity\ntidyUSDA::allDomain\ntidyUSDA::allCounty\ntidyUSDA::allProgram\ntidyUSDA::allDataItem\ntidyUSDA::allState\ntidyUSDA::allGeogLevel\n\n\n8.1.2 Caveats\nYou cannot retrieve more than \\(50,000\\) rows of data (this limit is set by QuickStat). The query below requests far more than \\(50,000\\) observations, and therefore, will fail. In such cases, you need to narrow your search and break the task into smaller, manageable queries.\n\n#--- this results in an error ---#\nmany_states_corn &lt;- \n  getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    commodity = \"CORN\",\n    geographic_level = \"COUNTY\",\n    state = c(\"ILLINOIS\", \"COLORADO\", \"NEBRASKA\", \"IOWA\", \"KANSAS\"),\n    year = paste(1995:2018),\n    geometry = TRUE\n  )\n\nError: API did not return results. First verify that your input parameters work on the NASS\n    website: https://quickstats.nass.usda.gov/. If correct, try again in a few minutes; the API may\n    be experiencing heavy traffic.\n\n\nAnother caveat is that a query will return an error if no observations meet your query criteria. For example, even though “CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE” is a valid value for data_item, there are no entries for this statistic in Illinois in 2018. As a result, the following query will fail.\n\nmany_states_corn &lt;-\n  getQuickstat(\n    key = key_get(\"usda_nass_qs_api\"),\n    program = \"SURVEY\",\n    data_item = \"CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE\",\n    geographic_level = \"COUNTY\",\n    state = \"ILLINOIS\",\n    year = \"2018\",\n    geometry = TRUE\n  )\n\nError: API did not return results. First verify that your input parameters work on the NASS\n    website: https://quickstats.nass.usda.gov/. If correct, try again in a few minutes; the API may\n    be experiencing heavy traffic.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#sec-CropScapeR",
    "href": "chapters/08-DownloadSpatialData.html#sec-CropScapeR",
    "title": "8  Download and process spatial datasets from within R",
    "section": "\n8.2 CDL with CropScapeR\n",
    "text": "8.2 CDL with CropScapeR\n\nThe Cropland Data Layer (CDL) is a data product produced by the National Agricultural Statistics Service (NASS) of the U.S. Department of Agriculture. The CDL provides geo-referenced, high-accuracy crop-specific land cover information at 30-meter resolution (since 2007) or 56-meter resolution (in 2006 and 2007) for up to 48 contiguous U.S. states, covering data from 1997 to the present. This dataset has been widely used in agricultural research. CropScape is an interactive web-based system for exploring CDL data. It was developed to query, visualize, disseminate, and analyze CDL data geospatially using standard geospatial web services, all within a publicly accessible online environment (Han et al., 2012).\nThis section demonstrates how to use the CropScapeR package (Chen 2020) to download and explore CDL data. The package implements some of the most useful geospatial processing services provided by CropScape, allowing users to efficiently process CDL data within the R environment. The CropScapeR package provides four key functions that implement different geospatial processing services offered by CropScape. This section introduces these functions with examples. The CropScapeR::GetCDLData() function is particularly important, as it enables users to download raw CDL data. The other functions allow users to obtain CDL data that is summarized or transformed in specific ways to meet various needs.\n\n8.2.1 CropScapeR::GetCDLData: Download the CDL data as raster data\nThe function CropScapeR::GetCDLData() allows us to obtain CDL data for any Area of Interest (AOI) for a given year. It requires three parameters to make a valid data request:\n\n\naoi: The Area of Interest.\n`year: The year for which the data is requested.\n\ntype: The type of AOI.\n\nThe following AOI-type combinations are accepted:\n\nany spatial object as an sf or sfc object: type = \"b\"\n\ncounty (defined by a 5-digit county FIPS code): type = \"f\"\n\nstate (defined by a 2-digit state FIPS code): type = \"f\"\n\nbounding box (defined by four corner points): type = \"b\"\n\npolygon area (defined by at least three coordinates): type = \"ps\"\n\nsingle point (defined by a coordinate): type = \"p\"\n\n\nThis section discusses how to download data for an sf object, county, and state as they are likely to be the most common AOI. See its Github repository to see how the other options work.\n\n8.2.1.1 Downloading CDL data for sf, county, and state\nDownloading CDL data for sf\nLet’s download the 2018 CDL data for the area covering Champaign, Vermilion, Ford, and Iroquois counties in Illinois (Figure 8.2).\n\n#--- get the sf for all the counties in Illinois ---#\nIL_county &lt;-\n  tigris::counties(state = \"IL\", cb = TRUE, progress_bar = FALSE) %&gt;%\n  st_as_sf()\n\n#--- get the four counties  ---#\nIL_county_4 &lt;- dplyr::filter(IL_county, NAME %in% c(\"Champaign\", \"Vermilion\", \"Ford\", \"Iroquois\"))\n\n\n\n\n\nCodeggplot() +\n  geom_sf(data = IL_county) +\n  geom_sf(data = IL_county_4, fill = \"lightblue\") +\n  theme_void()\n\n\n\n\n\n\nFigure 8.2: Location of the four counties in Illinois\n\n\n\n\nWhen using an sf object for aoi, the CDL data will be downloaded for the bounding box (hence type = \"b\") that encompasses the entire geographic area of the sf object, regardless of the type of objects within it (whether they are points, polygons, or lines). In this case, the CDL data is downloaded for the red area shown in Figure 8.3.\n\n\n\n\nCodeggplot() +\n  geom_sf(data = IL_county) +\n  geom_sf(data = st_as_sfc(st_bbox(IL_county_4)), fill = \"red\", alpha = 0.4) +\n  theme_void()\n\n\n\n\n\n\nFigure 8.3: Bounding box of the four counties\n\n\n\n\nLet’s now download CDL data for the four counties:\n\n(\n  cdl_IL_4 &lt;-\n    CropScapeR::GetCDLData(\n      aoi = IL_county_4,\n      year = \"2018\",\n      type = \"b\"\n    )\n)\n\n\n\nclass      : RasterLayer \ndimensions : 4431, 2826, 12522006  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 631935, 716715, 1898745, 2031675  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource     : IL4.tif \nnames      : Layer_1 \n\n\nAs you can see, the downloaded data is a RasterLayer object3. You might want to convert it to SpatRaster if you are more familiar with the terra package (Figure 8.4 presents the map).\n3 an object class defined by the raster package. See Chapter 4.\ncdl_IL_4 &lt;- terra::rast(cdl_IL_4)\n\n\n\n\n\nCodeplot(cdl_IL_4)\n\n\n\n\n\n\nFigure 8.4: Map of CDL values\n\n\n\n\nNote that the CDL data uses the Albers equal-area conic projection.\n\nterra::crs(cdl_IL_4)\n\n[1] \"PROJCRS[\\\"unknown\\\",\\n    BASEGEOGCRS[\\\"unknown\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ID[\\\"EPSG\\\",6269]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8901]]],\\n    CONVERSION[\\\"unknown\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"(E)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"(N)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\n\n\nDownloading CDL data for county\nThe following code requests to download the CDL data for Champaign County, Illinois, for the year 2018 (Figure 8.5).\n\n(\n  cdl_Champaign &lt;- \n    CropScapeR::GetCDLData(aoi = 17019, year = 2018, type = \"f\") %&gt;%\n    terra::rast()\n)\n\n\n\nclass       : SpatRaster \ndimensions  : 2060, 1626, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 633825, 682605, 1898745, 1960545  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : ch.tif \ncolor table : 1 \nname        : Layer_1 \n\n\nIn the above code, the FIPS code for Champaign County (17019) was supplied to the aoi option. Because a county is used here, the type argument is specified as \"f\".\n\n\n\n\nCodeplot(cdl_Champaign)\n\n\n\n\n\n\nFigure 8.5: CDL values for Champaign County in Illinois\n\n\n\n\n\nDownloading CDL data for state\nThe following code makes a request to download the CDL data for the state of Illinois in the year 2018 (Figure 8.6).\n\n(\n  cdl_IL &lt;- \n    CropScapeR::GetCDLData(aoi = 17, year = 2018, type = \"f\") %&gt;%\n    terra::rast()\n)\n\n\n\n\n\nCodeplot(cdl_IL)\n\n\n\n\n\n\nFigure 8.6: CDL values for Illinois\n\n\n\n\nIn the above code, the state FIPS code for Illinois (\\(17\\)) was supplied to the aoi option. Because a county is used here, the type argument is specified as \"f\".\n\n8.2.1.2 Other format options\nGeoTiff\nYou can save the downloaded CDL data as a tif file by adding save_path = option to CropScapeR::GetCDLData() as follows:\n\n(\n  cdl_IL_4 &lt;- \n    CropScapeR::GetCDLData(\n      aoi = IL_county_4,\n      year = \"2018\",\n      type = \"b\",\n      save_path = \"Data/IL_4.tif\"\n    )\n)\n\nWith this code, the downloaded data will be saved as “IL_4.tif” in the “Data” folder located in the current working directory.\n\nsf\nThe CropScapeR::GetCDLData() function lets you download CDL data as an sf of points, where the coordinates of the points are the coordinates of the centroid of the raster cells. This can be done by adding format = sf as an option.\n\n(\n  cdl_sf &lt;- CropScapeR::GetCDLData(aoi = 17019, year = 2018, type = \"f\", format = \"sf\")\n)\n\nThe first column (value) is the crop code. Of course, you can manually convert a RasterLayer to an sf of points as follows:\n\n\n\n\n\n\nWarning\n\n\n\nIt is very unlikely that you need to have raster data as sf rather then RasterLayer or SpatRaster.\n\n\n\n8.2.2 Data processing after downloading data\nThe downloaded raster data is not immediately ready for analysis. Typically, the variable of interest is the frequency of land use types or their shares. You can use terra::freq() to obtain the frequency (i.e., the number of raster cells) for each land use type.\n\n(\n  crop_freq &lt;- terra::freq(cdl_Champaign)\n)\n\n   layer value   count\n1      1     0  476477\n2      1     1 1211343\n3      1     4      15\n4      1     5 1173150\n5      1    23       8\n6      1    24    8869\n7      1    26    1168\n8      1    27      34\n9      1    28      52\n10     1    36    4418\n11     1    37    6804\n12     1    43       2\n13     1    59    1064\n14     1    60      79\n15     1    61      54\n16     1   111    6112\n17     1   121  111191\n18     1   122  155744\n19     1   123   38898\n20     1   124   12232\n21     1   131    1333\n22     1   141   49012\n23     1   142      15\n24     1   143       7\n25     1   152      77\n26     1   176   84463\n27     1   190    6545\n28     1   195     339\n29     1   222       1\n30     1   229      16\n31     1   241      38\n\n\nClearly, once frequencies are found, you can easily get shares as well:\n\n(\n  crop_data &lt;- \n    crop_freq %&gt;%\n    #--- matrix to data.frame ---#\n    data.frame(.) %&gt;%\n    #--- find share ---#\n    mutate(share = count / sum(count))\n)\n\n   layer value   count        share\n1      1     0  476477 1.422506e-01\n2      1     1 1211343 3.616424e-01\n3      1     4      15 4.478200e-06\n4      1     5 1173150 3.502400e-01\n5      1    23       8 2.388373e-06\n6      1    24    8869 2.647810e-03\n7      1    26    1168 3.487025e-04\n8      1    27      34 1.015059e-05\n9      1    28      52 1.552443e-05\n10     1    36    4418 1.318979e-03\n11     1    37    6804 2.031312e-03\n12     1    43       2 5.970933e-07\n13     1    59    1064 3.176537e-04\n14     1    60      79 2.358519e-05\n15     1    61      54 1.612152e-05\n16     1   111    6112 1.824717e-03\n17     1   121  111191 3.319570e-02\n18     1   122  155744 4.649685e-02\n19     1   123   38898 1.161287e-02\n20     1   124   12232 3.651823e-03\n21     1   131    1333 3.979627e-04\n22     1   141   49012 1.463237e-02\n23     1   142      15 4.478200e-06\n24     1   143       7 2.089827e-06\n25     1   152      77 2.298809e-05\n26     1   176   84463 2.521615e-02\n27     1   190    6545 1.953988e-03\n28     1   195     339 1.012073e-04\n29     1   222       1 2.985467e-07\n30     1   229      16 4.776747e-06\n31     1   241      38 1.134477e-05\n\n\nAt this point, the data does not indicate which value corresponds to which crop. To find the crop names associated with the crop codes (value), you can use the reference table provided by the CropScapeR package with data(linkdata).\n\n#--- load the crop code reference data ---#\ndata(\"linkdata\", package = \"CropScapeR\")\n\n#--- take a look ---#\nhead(linkdata)\n\n   MasterCat     Crop\n       &lt;int&gt;   &lt;char&gt;\n1:         0   NoData\n2:         1     Corn\n3:         2   Cotton\n4:         3     Rice\n5:         4  Sorghum\n6:         5 Soybeans\n\n\nYou can merge the two data sets using value from the CDL data and MasterCat from linkdata as the merging keys:\n\n(\n  crop_data &lt;- dplyr::left_join(crop_data, linkdata, by = c(\"value\" = \"MasterCat\"))\n)\n\n   layer value   count        share                     Crop\n1      1     0  476477 1.422506e-01                   NoData\n2      1     1 1211343 3.616424e-01                     Corn\n3      1     4      15 4.478200e-06                  Sorghum\n4      1     5 1173150 3.502400e-01                 Soybeans\n5      1    23       8 2.388373e-06             Spring_Wheat\n6      1    24    8869 2.647810e-03             Winter_Wheat\n7      1    26    1168 3.487025e-04 Dbl_Crop_WinWht/Soybeans\n8      1    27      34 1.015059e-05                      Rye\n9      1    28      52 1.552443e-05                     Oats\n10     1    36    4418 1.318979e-03                  Alfalfa\n11     1    37    6804 2.031312e-03    Other_Hay/Non_Alfalfa\n12     1    43       2 5.970933e-07                 Potatoes\n13     1    59    1064 3.176537e-04           Sod/Grass_Seed\n14     1    60      79 2.358519e-05              Switchgrass\n15     1    61      54 1.612152e-05     Fallow/Idle_Cropland\n16     1   111    6112 1.824717e-03               Open_Water\n17     1   121  111191 3.319570e-02     Developed/Open_Space\n18     1   122  155744 4.649685e-02  Developed/Low_Intensity\n19     1   123   38898 1.161287e-02  Developed/Med_Intensity\n20     1   124   12232 3.651823e-03 Developed/High_Intensity\n21     1   131    1333 3.979627e-04                   Barren\n22     1   141   49012 1.463237e-02         Deciduous_Forest\n23     1   142      15 4.478200e-06         Evergreen_Forest\n24     1   143       7 2.089827e-06             Mixed_Forest\n25     1   152      77 2.298809e-05                Shrubland\n26     1   176   84463 2.521615e-02        Grassland/Pasture\n27     1   190    6545 1.953988e-03           Woody_Wetlands\n28     1   195     339 1.012073e-04      Herbaceous_Wetlands\n29     1   222       1 2.985467e-07                   Squash\n30     1   229      16 4.776747e-06                 Pumpkins\n31     1   241      38 1.134477e-05   Dbl_Crop_Corn/Soybeans\n\n\nThe NoData in the Crop column corresponds to the black areas in the figure above, representing portions of the raster data that do not overlap with the boundary of Champaign County. You can remove these points with NoData by using the dplyr::filter function.\n\n8.2.3 Other forms of CDL data\nInstead of downloading the raw CDL data, CropScape provides an option to download summarized CDL data.\n\n\nCropScapeR::GetCDLComp(): request data on land use changes\n\nCropScapeR::GetCDLStat(): get acreage estimates from the CDL\n\nCropScapeR::GetCDLImage(): download the image files of the CDL data\n\nThese may come handy if they satisfy your needs because you can skip post-downloading processing steps.\n\nCropScapeR::GetCDLComp(): request data on land use changes\nThe CropScapeR::GetCDLComp() function allows users to request data on land cover changes over time from the CDL. Specifically, this function returns the acreage that has changed from one crop category to another between two years for a user-defined AOI.\nLet’s look at an example. The following code requests data on acreage changes in land cover for Champaign County (FIPS = 17019) from 2017 (year1 = 2017) to 2018 (year2 = 2018).\n\n(\n  data_change &lt;- CropScapeR::GetCDLComp(aoi = \"17019\", year1 = 2017, year2 = 2018, type = \"f\")\n)\n\n                     From                       To   Count  Acreage    aoi\n                   &lt;char&gt;                   &lt;char&gt;   &lt;int&gt;    &lt;num&gt; &lt;char&gt;\n  1:                 Corn                     Corn  181490  40362.4  17019\n  2:                 Corn                  Sorghum       1      0.2  17019\n  3:                 Corn                 Soybeans 1081442 240506.9  17019\n  4:                 Corn             Winter Wheat    1950    433.7  17019\n  5:                 Corn Dbl Crop WinWht/Soybeans     110     24.5  17019\n ---                                                                      \n241:  Herbaceous Wetlands      Herbaceous Wetlands      18      4.0  17019\n242: Dbl Crop WinWht/Corn                     Corn       1      0.2  17019\n243:             Pumpkins                     Corn      69     15.3  17019\n244:             Pumpkins                  Sorghum       2      0.4  17019\n245:             Pumpkins                 Soybeans      62     13.8  17019\n\n\nThe result is a data.frame (or data.table) with five columns. The From and To columns represent the crop names, Count indicates the pixel count, and Acreage provides the corresponding acreage for those pixel counts. The last column, aoi, refers to the selected Area of Interest (AOI). For example, the first row of the returned data shows 40,362 acres of continuous corn during 2017 and 2018, while the third row shows 240,506 acres rotated from corn to soybeans over the same period.\nKeep in mind that the spatial resolution of the CDL changes from 56 meters to 30 meters starting in 2008. This means that when you request land-use changes from 2007 to 2008, the two CDL raster layers have different spatial resolutions. As a result, the CropScape API cannot resolve this issue and will return an error message such as “Mismatch size of file 1 and file 2.”\nThe CropScapeR::GetCDLComp() function automatically addresses this problem by resampling the two CDL raster files using the nearest-neighbor resampling technique so that both rasters have the same spatial resolution. The finer-resolution raster is downscaled to match the lower resolution. The resampled raster layers are then merged to calculate cropland changes. Users can disable this default behavior by setting manual_try = FALSE, in which case an error message from the CropScape API will be returned without land-use change results.\n\ndata_change &lt;- CropScapeR::GetCDLComp(aoi = \"17019\", year1 = 2007, year2 = 2008, type = \"f\", `manual_try` = FALSE)\n\nError in GetCDLCompF(fips = aoi, year1 = year1, year2 = year2, mat = mat, : Error: The requested data might not exist in the CDL database. \nError message from CropScape is :&lt;faultstring&gt;Error: Mismatch size of file 1 and file 2.\n&lt;/faultstring&gt;\n\n\n\nCropScapeR::GetCDLStat(): get acreage estimates from the CDL\nThe CropScapeR::GetCDLStat() function allows users to retrieve acreage by land cover category for a user-defined Area of Interest (AOI) in a specific year. For example, the following code requests data on acreage by land cover categories for Champaign County, Illinois, in 2018. You’ll notice that the pixel counts are already converted to acres, and the corresponding category names are included.\n\n(\n  data_stat &lt;- CropScapeR::GetCDLStat(aoi = 17019, year = 2018, type = \"f\")\n)\n\n\n\n    Value                   Category  Acreage\n    &lt;int&gt;                     &lt;char&gt;    &lt;num&gt;\n 1:     1                       Corn 269396.2\n 2:     4                    Sorghum      3.3\n 3:     5                   Soybeans 260902.3\n 4:    23               Spring Wheat      1.8\n 5:    24               Winter Wheat   1972.4\n 6:    26   Dbl Crop WinWht/Soybeans    259.8\n 7:    27                        Rye      7.6\n 8:    28                       Oats     11.6\n 9:    36                    Alfalfa    982.5\n10:    37      Other Hay/Non Alfalfa   1513.2\n11:    43                   Potatoes      0.4\n12:    59             Sod/Grass Seed    236.6\n13:    60                Switchgrass     17.6\n14:    61       Fallow/Idle Cropland     12.0\n15:   111                 Open Water   1359.3\n16:   121       Developed/Open Space  24728.3\n17:   122    Developed/Low Intensity  34636.6\n18:   123 Developed/Medium Intensity   8650.7\n19:   124   Developed/High Intensity   2720.3\n20:   131                     Barren    296.5\n21:   141           Deciduous Forest  10900.0\n22:   142           Evergreen Forest      3.3\n23:   143               Mixed Forest      1.6\n24:   152                  Shrubland     17.1\n25:   176              Grass/Pasture  18784.1\n26:   190             Woody Wetlands   1455.6\n27:   195        Herbaceous Wetlands     75.4\n28:   222                     Squash      0.2\n29:   229                   Pumpkins      3.6\n30:   241     Dbl Crop Corn/Soybeans      8.5\n    Value                   Category  Acreage\n\n\n\nCropScapeR::GetCDLImage(): Download the image files of the CDL data\nThe CropScapeR::GetCDLImage function allows users to download image files of CDL data. This function works similarly to CropScapeR::GetCDLData, except that it returns image files instead of data. It’s particularly useful if you only want to visualize the CDL data. By default, the image is saved in “png” format, but you can also save it in “kml” format if needed.\n\nCropScapeR::GetCDLImage(aoi = 17019, year = 2018, type = \"f\", verbose = F)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#sec-download-prism",
    "href": "chapters/08-DownloadSpatialData.html#sec-download-prism",
    "title": "8  Download and process spatial datasets from within R",
    "section": "\n8.3 PRISM with prism\n",
    "text": "8.3 PRISM with prism\n\n\n8.3.1 Basics\nPRISM dataset provides model-based estimates of precipitation, maximum temperature (tmax), and minimum temperature (tmin) for the U.S. at a 4 km by 4 km spatial resolution. To download daily data, we can use the prism::get_prism_dailys() function from the prism package (Hart and Bell 2015). Below is its general syntax:\n\n#--- NOT RUN ---#\nprism::get_prism_dailys(\n  type = variable type,\n  minDate = starting date as character,\n  maxDate = ending date as character,\n  keepZip = TRUE or FALSE\n)\n\nThe variables types you can select from is \"ppt\" (precipitation), \"tmean\" (mean temperature), \"tmin\" (minimum temperature), and \"tmax\" (maximum temperature). For minDate and maxDate, the dates must be specified in a specific format of “YYYY-MM-DD”. keepZip = FALSE does not keep the zipped folders of the downloaded files as the name suggests.\nBefore downloading PRISM data using the prism::get_prism_dailys() function, it’s recommended to set the path to the folder where the downloaded data will be stored using options(prism.path = \"path\"). For example, the following code sets the path to \"Data/PRISM/\" relative to the current working directory:\n\noptions(prism.path = \"Data/PRISM/\")\n\nThe following code downloads daily tmax data from January 1, 2000 to Jan 10, 2000.\n\nprism::get_prism_dailys(\n  type = \"tmax\",\n  minDate = \"2000-01-01\",\n  maxDate = \"2000-01-10\",\n  keepZip = FALSE\n)\n\nWhen you download data using the above code, you will notice that it creates one folder for one day. For example, for tmax data for “2000-01-01”, you can get the path to the downloaded file as follows:\n\nvar_type &lt;- \"tmax\" # variable type\ndates_prism_txt &lt;- str_remove_all(\"2000-01-01\", \"-\") # date without dashes\n\n#--- folder name ---#\nfolder_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\")\n\n#--- file name of the downloaded data inside the above folder ---#\nfile_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\")\n\n#--- path to the file relative to the designated data folder (here, it's \"Data/PRISM/\") ---#\n(\n  file_path &lt;- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n)\n\n[1] \"Data/PRISM/PRISM_tmax_stable_4kmD2_20000101_bil/PRISM_tmax_stable_4kmD2_20000101_bil.bil\"\n\n\nWe can then easily read the data using terra::rast() or stars::read_stars() if you prefer the stars way.\n\n#--- as SpatRaster ---#\n(\n  prism_2000_01_01_sr &lt;- terra::rast(file_path)\n)\n\nclass       : SpatRaster \ndimensions  : 621, 1405, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -125.0208, -66.47917, 24.0625, 49.9375  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 \nsource      : PRISM_tmax_stable_4kmD2_20000101_bil.bil \nname        : PRISM_tmax_stable_4kmD2_20000101_bil \nmin value   :                              -16.309 \nmax value   :                               29.073 \n\n#--- as stars ---#\n(\n  prism_2000_01_01_stars &lt;- stars::read_stars(file_path)\n)\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n                                   Min. 1st Qu. Median     Mean 3rd Qu.   Max.\nPRISM_tmax_stable_4kmD2_200...  -16.309   4.095 10.075 10.20043  16.953 29.073\n                                  NA's\nPRISM_tmax_stable_4kmD2_200...  390874\ndimension(s):\n  from   to offset    delta refsys x/y\nx    1 1405   -125  0.04167  NAD83 [x]\ny    1  621  49.94 -0.04167  NAD83 [y]\n\n\n\n\n\n\nCodeplot(prism_2000_01_01_sr)\n\n\n\n\n\n\nFigure 8.7: PRISM tmax data for January 1, 2000\n\n\n\n\nFigure 8.7 presents a quick visualization of the data. As you can see, the dataset covers the entire contiguous U.S.\n\n8.3.2 Download daily PRISM data for many years and build your own datasets\nHere, we provide an example of how to create your own PRISM datasets. Building such datasets and storing them locally can be beneficial if you plan to use the data for multiple projects in the future.\nSuppose we are interested in saving daily PRISM precipitation data by year and month, from 1980 to 2018. To accomplish this, we will write a loop that iterates over all year-month combinations. Before constructing the loop, let’s start by working with a specific year-month combination—December 1990.\nWe will write the code in a way that can be easily adapted for looped operations later. Specifically, we will define the following variables and use them as placeholders for the values that will be looped over.\n\n#--- month to work on ---#\ntemp_month &lt;- 12\n\n#--- year to work on ---#\ntemp_year &lt;- 1990\n\nWe first need to set the path to the folder in which daily PRISM files will be downloaded.\n\n#--- set your own path ---#\noptions(prism.path = \"Data/PRISM/\")\n\nWe then set the start and end dates for prism::get_prism_dailys().\n\n#--- starting date of the working month-year ---#\n(\n  start_date &lt;- lubridate::dmy(paste0(\"1/\", temp_month, \"/\", temp_year))\n)\n\n[1] \"1990-12-01\"\n\n#--- ending date: add a month and then go back 1 day ---#\n(\n  end_date &lt;- start_date %m+% months(1) - 1\n)\n\n[1] \"1990-12-31\"\n\n\nWe now download PRISM data for the year-month we are working on.\n\n#--- download daily PRISM data for the working month-year ---#\nprism::get_prism_dailys(\n  type = \"ppt\",\n  minDate = as.character(start_date),\n  maxDate = as.character(end_date),\n  keepZip = FALSE\n)\n\nOnce all the data are downloaded, we will read and import them onto R. To do so, we will need the path to all the downloaded files.\n\n#--- list of dates of the working month-year ---#\ndates_ls &lt;- seq(start_date, end_date, \"days\")\n\n#--- remove dashes ---#\ndates_prism_txt &lt;- stringr::str_remove_all(dates_ls, \"-\")\n\n#--- folder names ---#\nfolder_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\")\n\n#--- the file name of the downloaded data ---#\nfile_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\")\n\n#--- complete path to the downloaded files ---#\n(\n  file_path &lt;- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n)\n\n [1] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901201_bil/PRISM_tmax_stable_4kmD2_19901201_bil.bil\"\n [2] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901202_bil/PRISM_tmax_stable_4kmD2_19901202_bil.bil\"\n [3] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901203_bil/PRISM_tmax_stable_4kmD2_19901203_bil.bil\"\n [4] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901204_bil/PRISM_tmax_stable_4kmD2_19901204_bil.bil\"\n [5] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901205_bil/PRISM_tmax_stable_4kmD2_19901205_bil.bil\"\n [6] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901206_bil/PRISM_tmax_stable_4kmD2_19901206_bil.bil\"\n [7] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901207_bil/PRISM_tmax_stable_4kmD2_19901207_bil.bil\"\n [8] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901208_bil/PRISM_tmax_stable_4kmD2_19901208_bil.bil\"\n [9] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901209_bil/PRISM_tmax_stable_4kmD2_19901209_bil.bil\"\n[10] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901210_bil/PRISM_tmax_stable_4kmD2_19901210_bil.bil\"\n[11] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901211_bil/PRISM_tmax_stable_4kmD2_19901211_bil.bil\"\n[12] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901212_bil/PRISM_tmax_stable_4kmD2_19901212_bil.bil\"\n[13] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901213_bil/PRISM_tmax_stable_4kmD2_19901213_bil.bil\"\n[14] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901214_bil/PRISM_tmax_stable_4kmD2_19901214_bil.bil\"\n[15] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901215_bil/PRISM_tmax_stable_4kmD2_19901215_bil.bil\"\n[16] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901216_bil/PRISM_tmax_stable_4kmD2_19901216_bil.bil\"\n[17] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901217_bil/PRISM_tmax_stable_4kmD2_19901217_bil.bil\"\n[18] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901218_bil/PRISM_tmax_stable_4kmD2_19901218_bil.bil\"\n[19] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901219_bil/PRISM_tmax_stable_4kmD2_19901219_bil.bil\"\n[20] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901220_bil/PRISM_tmax_stable_4kmD2_19901220_bil.bil\"\n[21] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901221_bil/PRISM_tmax_stable_4kmD2_19901221_bil.bil\"\n[22] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901222_bil/PRISM_tmax_stable_4kmD2_19901222_bil.bil\"\n[23] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901223_bil/PRISM_tmax_stable_4kmD2_19901223_bil.bil\"\n[24] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901224_bil/PRISM_tmax_stable_4kmD2_19901224_bil.bil\"\n[25] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901225_bil/PRISM_tmax_stable_4kmD2_19901225_bil.bil\"\n[26] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901226_bil/PRISM_tmax_stable_4kmD2_19901226_bil.bil\"\n[27] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901227_bil/PRISM_tmax_stable_4kmD2_19901227_bil.bil\"\n[28] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901228_bil/PRISM_tmax_stable_4kmD2_19901228_bil.bil\"\n[29] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901229_bil/PRISM_tmax_stable_4kmD2_19901229_bil.bil\"\n[30] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901230_bil/PRISM_tmax_stable_4kmD2_19901230_bil.bil\"\n[31] \"Data/PRISM/PRISM_tmax_stable_4kmD2_19901231_bil/PRISM_tmax_stable_4kmD2_19901231_bil.bil\"\n\n\nNext, we read the data as a stars object, set the third dimension as the date using the Date object class, and then save it as an R dataset. This ensures that the date dimension is preserved (see Section 6.4).\n\n(\n  #--- combine all the PRISM files as stars ---#\n  temp_stars &lt;-\n    stars::read_stars(file_path, along = 3) %&gt;%\n    #--- set the third dimension as data ---#\n    stars::st_set_dimensions(\"new_dim\", values = dates_ls, name = \"date\")\n)\n\n#--- save the stars as an rds file ---#\nsaveRDS(\n  temp_stars,\n  paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".rds\")\n)\n\nYou could alternatively read the files into a SpatRaster object and save it data as a GeoTIFF file.\n\n(\n  #--- combine all the PRISM files as a SpatRaster ---#\n  temp_stars &lt;- terra::rast(file_path)\n)\n\n#--- save as a multi-band GeoTIFF file ---#\nterra::writeRaster(temp_stars, paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".tif\"), overwrite = T)\n\nNote that this option of course does not have date as the third dimension. Moreover, the RDS file above takes up only 14 Mb, while the tif file occupies 108 Mb.\nFinally, if you would like, you can delete all the individual PRISM files:\n\n#--- delete all the downloaded files ---#\nunlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE)\n\n\nOkay, now that we know what to do with a particular year-month combination, we can easily write a loop to go over all the year-month combinations for the period of interest. Since all the processes we observed above for a single year-month combination is embarrassingly parallel, it is easy to parallelize using future.apply::future_lapply() or parallel::mclapply() (Linux/Mac users only). Here we use future_lapply() (see Appendix A if you are not familiar with looping and parallel processing). Let’s first get the number of logical cores.\n\nnum_cores &lt;- parallel::detectCores()\n\nfuture::plan(multisession, workers = num_cores)\n\nThe following function goes through all the steps we saw above for a single year-month combination.\n\n#--- define a function to download and save PRISM data stacked by month ---#\nget_save_prism &lt;- function(i, var_type) {\n  \n  #++++++++++++++++++++++++++++++++++++\n  #+ Debug\n  #++++++++++++++++++++++++++++++++++++\n  # i &lt;- 1\n  # var_type &lt;- \"ppt\"\n\n  #++++++++++++++++++++++++++++++++++++\n  #+ Main\n  #++++++++++++++++++++++++++++++++++++\n  print(paste0(\"working on \", i))\n\n  temp_month &lt;- month_year_data[i, month] # working month\n  temp_year &lt;- month_year_data[i, year] # working year\n\n  #--- starting date of the working month-year ---#\n  start_date &lt;- lubridate::dmy(paste0(\"1/\", temp_month, \"/\", temp_year))\n  #--- end date ---#\n  end_date &lt;- start_date %m+% months(1) - 1\n\n\n  #--- download daily PRISM data for the working month-year ---#\n  prism::get_prism_dailys(\n    type = var_type,\n    minDate = as.character(start_date),\n    maxDate = as.character(end_date),\n    keepZip = FALSE\n  )\n\n  #--- list of dates of the working month-year ---#\n  dates_ls &lt;- seq(start_date, end_date, \"days\")\n\n  #--- remove dashes ---#\n  dates_prism_txt &lt;- stringr::str_remove_all(dates_ls, \"-\")\n\n  #--- folder names ---#\n  folder_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\")\n  #--- the file name of the downloaded data ---#\n  file_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\")\n  #--- complete path to the downloaded files ---#\n  file_path &lt;- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n  #--- combine all the PRISM files as a SpatRaster ---#\n  temp_stars &lt;-\n    terra::rast(file_path) %&gt;%\n    #--- convert to stars ---#\n    stars::st_as_stars() %&gt;%\n    #--- set the third dimension as data ---#\n    stars::st_set_dimensions(\"band\", values = dates_ls, name = \"date\")\n\n  #--- save the stars as an rds file ---#\n  saveRDS(\n    temp_stars,\n    paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \"_m\", temp_month, \".rds\")\n  )\n\n  #--- delete all the downloaded files ---#\n  unlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE)\n}\n\nWe then create a data.frame of all the year-month combinations:\n\n(\n  #--- create a set of year-month combinations to loop over ---#\n  month_year_data &lt;- data.table::CJ(month = 1:12, year = 1990:2018)\n)\n\nKey: &lt;month, year&gt;\n     month  year\n     &lt;int&gt; &lt;int&gt;\n  1:     1  1990\n  2:     1  1991\n  3:     1  1992\n  4:     1  1993\n  5:     1  1994\n ---            \n344:    12  2014\n345:    12  2015\n346:    12  2016\n347:    12  2017\n348:    12  2018\n\n\nWe now do parallelized loop over all the year-month combinations (by looping over the rows of month_year_data):\n\n#--- run the above code in parallel ---#\nfuture.apply::future_lapply(\n  1:nrow(month_year_data),\n  function(x) get_save_prism(x, \"ppt\")\n)\n\nThat’s it. Of course, you can do the same thing for tmax by this:\n\n#--- run the above code in parallel ---#\nfuture.apply::future_lapply(\n  1:nrow(month_year_data),\n  function(x) get_save_prism(x, \"tmax\")\n)\n\nNow that you have PRISM datasets, you can extract values from the raster layers for vector data for your analysis, which is covered extensively in Chapters Chapter 5 and Chapter 6 (for stars objects).\n\nIf you want to save the data by year (each file would be about 168 Mb). You could do this.\n\n#--- define a function to download and save PRISM data stacked by year ---#\nget_save_prism_y &lt;- function(temp_year, var_type) {\n  print(paste0(\"working on \", temp_year))\n\n  #--- starting date of the working month-year ---#\n  start_date &lt;- dmy(paste0(\"1/1/\", temp_year))\n  #--- end date ---#\n  end_date &lt;- dmy(paste0(\"1/1/\", temp_year + 1)) - 1\n\n  #--- download daily PRISM data for the working month-year ---#\n  prism::get_prism_dailys(\n    type = var_type,\n    minDate = as.character(start_date),\n    maxDate = as.character(end_date),\n    keepZip = FALSE\n  )\n\n  #--- list of dates of the working month-year ---#\n  dates_ls &lt;- seq(start_date, end_date, \"days\")\n\n  #--- remove dashes ---#\n  dates_prism_txt &lt;- stringr::str_remove_all(dates_ls, \"-\")\n\n  #--- folder names ---#\n  folder_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil\")\n  #--- the file name of the downloaded data ---#\n  file_name &lt;- paste0(\"PRISM_\", var_type, \"_stable_4kmD2_\", dates_prism_txt, \"_bil.bil\")\n  #--- complete path to the downloaded files ---#\n  file_path &lt;- paste0(\"Data/PRISM/\", folder_name, \"/\", file_name)\n\n  #--- combine all the PRISM files as a SpatRaster ---#\n  temp_stars &lt;- \n    terra::rast(file_path) %&gt;%\n    #--- convert to stars ---#\n    stars::st_as_stars() %&gt;%\n    #--- set the third dimension as data ---#\n    stars::st_set_dimensions(\"band\", values = dates_ls, name = \"date\")\n\n  #--- save the stars as an rds file ---#\n  saveRDS(\n    temp_stars,\n    paste0(\"Data/PRISM/PRISM_\", var_type, \"_y\", temp_year, \".rds\")\n  )\n\n  #--- delete all the downloaded files ---#\n  unlink(paste0(\"Data/PRISM/\", folder_name), recursive = TRUE)\n}\n\n#--- run the above code in parallel ---#\nfuture_lapply(\n  1990:2018,\n  function(x) get_save_prism_y(x, \"tmax\")\n)",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#sec-daymetr",
    "href": "chapters/08-DownloadSpatialData.html#sec-daymetr",
    "title": "8  Download and process spatial datasets from within R",
    "section": "\n8.4 Daymet with daymetr and FedData\n",
    "text": "8.4 Daymet with daymetr and FedData\n\nFor this section, we use the daymetr (Hufkens et al. 2018) and FedData packages (Bocinsky 2016).\n\nlibrary(daymetr)\nlibrary(FedData)\n\nDaymet data consists of “tiles,” each of which consisting of raster cells of 1km by 1km. Figure 8.8 is the map of the tiles.\n\n\n\n\nCodeUS_map &lt;- \n  sf::st_as_sf(maps::map(database = \"state\", plot = FALSE, fill = TRUE)) %&gt;%\n  sf::st_transform(st_crs(tile_outlines))\n\nggplot() +\n  geom_sf(data = st_as_sf(tile_outlines), fill = NA, size = 0.7) +\n  geom_sf(data = US_map, fill = \"red\", alpha = 0.2) +\n  theme_for_map\n\n\n\n\n\n\nFigure 8.8: Daymet Tiles\n\n\n\n\nHere is the list of weather variables:\n\nvapor pressure\nminimum and maximum temperature\nsnow water equivalent\nsolar radiation\nprecipitation\nday length\n\nDaymet provides more weather variables than PRISM, which is useful for calculating weather-dependent metrics such as evapotranspiration.\nThe easiest way to find Daymet values for your vector data depends on whether you are working with points or polygons. For point data, daymetr::download_daymet() is the simplest option, as it directly returns weather values for the points of interest. Internally, it identifies the cell in which the point is located and returns the values for that cell over the specified period. daymetr::download_daymet() handles all of this automatically.\nFor polygons, however, you first need to download the relevant Daymet data for the region of interest and then extract the values for each polygon, a process covered in FedData::get_daymet() downloads the requested Daymet data and saves it as a RasterBrick object, which can be easily converted into a stars object using stars::st_as_stars().\n\n8.4.1 For points data\nFor points data, the easiest way to associate daily weather values to them is to use daymetr::download_daymet().\nHere are key parameters for the function:\n\n\nlat: latitude\n\nlon: longitude\n\nstart: start_year\n\nend: end_year\n\ninternal: TRUE (dafault) or FALSE\n\n\nFor example, the code below downloads daily weather data for a point (lat = \\(36\\), longitude = \\(-100\\)) starting from 2000 through 2002 and assigns the downloaded data to temp_daymet.\n\n#--- download daymet data ---#\ntemp_daymet &lt;- \n  daymetr::download_daymet(\n    lat = 36,\n    lon = -100,\n    start = 2000,\n    end = 2002\n  )\n\n#--- structure ---#\nstr(temp_daymet)\n\nList of 7\n $ site     : chr \"Daymet\"\n $ tile     : num 11380\n $ latitude : num 36\n $ longitude: num -100\n $ altitude : num 746\n $ tile     : num 11380\n $ data     :'data.frame':  1095 obs. of  9 variables:\n  ..$ year         : int [1:1095] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n  ..$ yday         : int [1:1095] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ dayl..s.     : num [1:1095] 34571 34606 34644 34685 34729 ...\n  ..$ prcp..mm.day.: num [1:1095] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ srad..W.m.2. : num [1:1095] 334 231 148 319 338 ...\n  ..$ swe..kg.m.2. : num [1:1095] 18.2 15 13.6 13.6 13.6 ...\n  ..$ tmax..deg.c. : num [1:1095] 20.9 13.88 4.49 9.18 13.37 ...\n  ..$ tmin..deg.c. : num [1:1095] -2.73 1.69 -2.6 -10.16 -8.97 ...\n  ..$ vp..Pa.      : num [1:1095] 500 690 504 282 310 ...\n - attr(*, \"class\")= chr \"daymetr\"\n\n\nAs you can see, temp_daymet has bunch of site information other than the weather data. You can get the weather data portion of temp_daymet by accessing its data element.\n\n#--- get the data part ---#\ntemp_daymet_data &lt;- temp_daymet$data\n\n#--- take a look ---#\nhead(temp_daymet_data)\n\n  year yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n1 2000    1 34571.11             0       334.34        18.23        20.90\n2 2000    2 34606.19             0       231.20        15.00        13.88\n3 2000    3 34644.13             0       148.29        13.57         4.49\n4 2000    4 34684.93             0       319.13        13.57         9.18\n5 2000    5 34728.55             0       337.67        13.57        13.37\n6 2000    6 34774.96             0       275.72        13.11         9.98\n  tmin..deg.c. vp..Pa.\n1        -2.73  499.56\n2         1.69  689.87\n3        -2.60  504.40\n4       -10.16  282.35\n5        -8.97  310.05\n6        -4.89  424.78\n\n\nAs you might have noticed, yday is not the date of each observation, but the day of the year. You can easily convert it into dates like this:\n\ntemp_daymet_data &lt;- dplyr::mutate(temp_daymet_data, date = as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\"))\n\nOnce dates are obtained, you can use the lubridate package to extract day, month, and year using day(), month(), and year(), respectively.\n\ntemp_daymet_data &lt;- \n  dplyr::mutate(temp_daymet_data,\n    day = lubridate::day(date),\n    month = lubridate::month(date),\n    #--- this is already there though ---#\n    year = lubridate::year(date)\n  )\n\n#--- take a look ---#\ndplyr::select(temp_daymet_data, year, month, day) %&gt;% head()\n\n  year month day\n1 2000     1   1\n2 2000     1   2\n3 2000     1   3\n4 2000     1   4\n5 2000     1   5\n6 2000     1   6\n\n\nThis helps you find group statistics like monthly precipitation.\n\ntemp_daymet_data %&gt;%\n  dplyr::group_by(month) %&gt;%\n  dplyr::summarize(prcp = mean(prcp..mm.day.))\n\n# A tibble: 12 × 2\n   month  prcp\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     1 0.762\n 2     2 1.20 \n 3     3 2.08 \n 4     4 1.24 \n 5     5 3.53 \n 6     6 3.45 \n 7     7 1.78 \n 8     8 1.19 \n 9     9 1.32 \n10    10 4.78 \n11    11 0.407\n12    12 0.743\n\n\nDownloading Daymet data for many points is just applying the same operations above to them using a loop. Let’s create random points within California and get their coordinates.\n\nset.seed(389548)\n\nrandom_points &lt;-\n  tigris::counties(state = \"CA\", progress_bar = FALSE) %&gt;%\n  st_as_sf() %&gt;%\n  #--- 10 points ---#\n  st_sample(10) %&gt;%\n  #--- get the coordinates ---#\n  st_coordinates() %&gt;%\n  #--- as tibble (data.frame) ---#\n  as_tibble() %&gt;%\n  #--- assign site id ---#\n  mutate(site_id = 1:n())\n\nTo loop over the points, you can first write a function like this:\n\nget_daymet &lt;- function(i) {\n  temp_lat &lt;- random_points[i, ] %&gt;% pull(Y)\n  temp_lon &lt;- random_points[i, ] %&gt;% pull(X)\n  temp_site &lt;- random_points[i, ] %&gt;% pull(site_id)\n\n  temp_daymet &lt;- download_daymet(\n    lat = temp_lat,\n    lon = temp_lon,\n    start = 2000,\n    end = 2002\n  ) %&gt;%\n    #--- just get the data part ---#\n    .$data %&gt;%\n    #--- convert to tibble (not strictly necessary) ---#\n    as_tibble() %&gt;%\n    #--- assign site_id so you know which record is for which site_id ---#\n    mutate(site_id = temp_site) %&gt;%\n    #--- get date from day of the year ---#\n    mutate(date = as.Date(paste(year, yday, sep = \"-\"), \"%Y-%j\"))\n\n  return(temp_daymet)\n}\n\nHere is what the function returns for the 1st row of random_points:\n\nget_daymet(1)\n\n# A tibble: 1,095 × 11\n    year  yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1  2000     1   34131.             0         246.            0         7.02\n 2  2000     2   34168.             0         251.            0         4.67\n 3  2000     3   34208.             0         247.            0         7.32\n 4  2000     4   34251              0         267.            0        10   \n 5  2000     5   34297              0         240.            0         4.87\n 6  2000     6   34346.             0         274.            0         7.79\n 7  2000     7   34398.             0         262.            0         7.29\n 8  2000     8   34453.             0         291.            0        11.2 \n 9  2000     9   34510.             0         268.            0        10.0 \n10  2000    10   34570.             0         277.            0        11.8 \n# ℹ 1,085 more rows\n# ℹ 4 more variables: tmin..deg.c. &lt;dbl&gt;, vp..Pa. &lt;dbl&gt;, site_id &lt;int&gt;,\n#   date &lt;date&gt;\n\n\nYou can now simply loop over the rows.\n\n(\n  daymet_all_points &lt;- \n    lapply(1:nrow(random_points), get_daymet) %&gt;%\n    #--- need to combine the list of data.frames into a single data.frame ---#\n    dplyr::bind_rows()\n)\n\n\n\n# A tibble: 10,950 × 11\n    year  yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c.\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1  2000     1   33523.             0         230.            0         12  \n 2  2000     2   33523.             0         240             0         11.5\n 3  2000     3   33523.             0         243.            0         13.5\n 4  2000     4   33523.             1         230.            0         13  \n 5  2000     5   33523.             0         243.            0         14  \n 6  2000     6   33523.             0         243.            0         13  \n 7  2000     7   33523.             0         246.            0         14  \n 8  2000     8   33869.             0         230.            0         13  \n 9  2000     9   33869.             0         227.            0         12.5\n10  2000    10   33869.             0         214.            0         14  \n# ℹ 10,940 more rows\n# ℹ 4 more variables: tmin..deg.c. &lt;dbl&gt;, vp..Pa. &lt;dbl&gt;, site_id &lt;int&gt;,\n#   date &lt;date&gt;\n\n\nOr better yet, you can easily parallelize this process as follows (see Appendix A if you are not familiar with parallelization in R):\n\n#--- parallelization planning ---#\nfuture::plan(multisession, workers = parallel::detectCores() - 1)\n\n#--- parallelized lapply ---#\ndaymet_all_points &lt;- \n  future.apply::future_lapply(1:nrow(random_points), get_daymet) %&gt;%\n  #--- need to combine the list of data.frames into a single data.frame ---#\n  dplyr::bind_rows()\n\n\n8.4.2 For polygons data\nSuppose you are interested in getting Daymet data for select counties in Michigan (Figure 8.9).\n\n#--- entire MI ---#\nMI_counties &lt;- tigris::counties(state = \"MI\", cb = TRUE, progress_bar = FALSE)\n\n#--- select counties ---#\nMI_counties_select &lt;- dplyr::filter(MI_counties, NAME %in% c(\"Luce\", \"Chippewa\", \"Mackinac\"))\n\n\n\n\n\nCodeggplot() +\n  geom_sf(data = MI_counties, fill = NA) +\n  geom_sf(data = MI_counties_select, fill = \"blue\") +\n  theme_void()\n\n\n\n\n\n\nFigure 8.9: Select Michigan counties for which we download Daymet data\n\n\n\n\nWe can use FedData::get_daymet() to download Daymet data that covers the spatial extent of the polygon data. The downloaded dataset can be assigned to an R object as a RasterBrick, or alternatively, you could write the downloaded data to a file. To specify the spatial extent for which Daymet data should be downloaded, we provide a SpatialPolygonsDataFrame object supported by the sp package. Since we primarily handle vector data with the sf package, we need to convert the sf object to an sp object.\nThe code below downloads prcp and tmax for the spatial extent of Michigan counties for 2000 and 2001:\n\n(\n  MI_daymet_select &lt;- \n    FedData::get_daymet(\n      #--- supply the vector data in sp ---#\n      template = as(MI_counties_select, \"Spatial\"),\n      #--- label ---#\n      label = \"MI_counties_select\",\n      #--- variables to download ---#\n      elements = c(\"prcp\", \"tmax\"),\n      #--- years ---#\n      years = 2000:2001\n    )\n)\n\n\n\n$prcp\nclass      : RasterBrick \ndimensions : 96, 156, 14976, 730  (nrow, ncol, ncell, nlayers)\nresolution : 1000, 1000  (x, y)\nextent     : 1027250, 1183250, 455500, 551500  (xmin, xmax, ymin, ymax)\ncrs        : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 \nsource     : memory\nnames      : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... \nmin values :           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0, ... \nmax values :           6,          26,          19,          17,           6,           9,           6,           1,           0,          13,          15,           6,           0,           4,           7, ... \n\n\n$tmax\nclass      : RasterBrick \ndimensions : 96, 156, 14976, 730  (nrow, ncol, ncell, nlayers)\nresolution : 1000, 1000  (x, y)\nextent     : 1027250, 1183250, 455500, 551500  (xmin, xmax, ymin, ymax)\ncrs        : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 \nsource     : memory\nnames      : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... \nmin values :         0.5,        -4.5,        -8.0,        -8.5,        -7.0,        -2.5,        -6.5,        -1.5,         1.5,         2.0,        -1.5,        -8.5,       -14.0,        -9.5,        -3.0, ... \nmax values :         3.0,         3.0,         0.5,        -2.5,        -2.5,         0.0,         0.5,         1.5,         4.0,         3.0,         3.5,         0.5,        -6.5,        -4.0,         0.0, ... \n\n\nAs you can see, Daymet prcp and tmax data are stored separately as RasterBrick in a single list. The following code access tmax:\n\nMI_daymet_select$tmax\n\nclass      : RasterBrick \ndimensions : 96, 156, 14976, 730  (nrow, ncol, ncell, nlayers)\nresolution : 1000, 1000  (x, y)\nextent     : 1027250, 1183250, 455500, 551500  (xmin, xmax, ymin, ymax)\ncrs        : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 \nsource     : memory\nnames      : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... \nmin values :         0.5,        -4.5,        -8.0,        -8.5,        -7.0,        -2.5,        -6.5,        -1.5,         1.5,         2.0,        -1.5,        -8.5,       -14.0,        -9.5,        -3.0, ... \nmax values :         3.0,         3.0,         0.5,        -2.5,        -2.5,         0.0,         0.5,         1.5,         4.0,         3.0,         3.5,         0.5,        -6.5,        -4.0,         0.0, ... \n\n\nIf you prefer SpatRaster from the terra package, convert it:\n\ntmax &lt;- terra::rast(MI_daymet_select$tmax)\n\nYou can extract values for your target polygons data using the methods described in Chapter 5.\nIf you use the stars package for raster data handling (see Chapter 6), you can convert them into stars object using st_as_stars().\n\n#--- tmax as stars ---#\ntmax_stars &lt;- st_as_stars(MI_daymet_select$tmax)\n\n#--- prcp as stars ---#\nprcp_stars &lt;- st_as_stars(MI_daymet_select$prcp)\n\nNow, the third dimension (band) is not recognized as dates. We can use st_set_dimension() to change that (see Section 6.5). Before that, we first need to recover Date values from the “band” values as follows:\n\ndate_values &lt;- \n  tmax_stars %&gt;%\n  #--- get band values ---#\n  stars::st_get_dimension_values(., \"band\") %&gt;%\n  #--- remove X ---#\n  gsub(\"X\", \"\", .) %&gt;%\n  #--- convert to date ---#\n  lubridate::ymd(.)\n\n#--- take a look ---#\nhead(date_values)\n\n[1] \"2000-01-01\" \"2000-01-02\" \"2000-01-03\" \"2000-01-04\" \"2000-01-05\"\n[6] \"2000-01-06\"\n\n\n\n#--- tmax ---#\nstars::st_set_dimensions(tmax_stars, 3, values = date_values, names = \"date\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n             Min. 1st Qu. Median      Mean 3rd Qu. Max. NA's\nX2000.01.01  -8.5      -4     -2 -1.884266       0    3  198\ndimension(s):\n     from  to  offset delta                       refsys\nx       1 156 1027250  1000 +proj=lcc +lon_0=-100 +la...\ny       1  96  551500 -1000 +proj=lcc +lon_0=-100 +la...\ndate    1 730      NA    NA                         Date\n                        values x/y\nx                         NULL [x]\ny                         NULL [y]\ndate 2000-01-01,...,2001-12-31    \n\n#--- prcp ---#\nstars::st_set_dimensions(prcp_stars, 3, values = date_values, names = \"date\")\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n             Min. 1st Qu. Median     Mean 3rd Qu. Max. NA's\nX2000.01.01     0       0      3 6.231649      12   26  198\ndimension(s):\n     from  to  offset delta                       refsys\nx       1 156 1027250  1000 +proj=lcc +lon_0=-100 +la...\ny       1  96  551500 -1000 +proj=lcc +lon_0=-100 +la...\ndate    1 730      NA    NA                         Date\n                        values x/y\nx                         NULL [x]\ny                         NULL [y]\ndate 2000-01-01,...,2001-12-31    \n\n\nNotice that the date dimension has NA for delta. This is because Daymet removes observations for December 31 in leap years to make the time dimension 365 consistently across years. This means that there is a one-day gap between “2000-12-30” and “2000-01-01” as you can see below:\n\ndate_values[364:367]\n\n[1] \"2000-12-29\" \"2000-12-30\" \"2001-01-01\" \"2001-01-02\"",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/08-DownloadSpatialData.html#sec-gridMET",
    "href": "chapters/08-DownloadSpatialData.html#sec-gridMET",
    "title": "8  Download and process spatial datasets from within R",
    "section": "\n8.5 gridMET",
    "text": "8.5 gridMET\ngridMET is a dataset of daily meteorological data covering the contiguous U.S. since 1979. Its spatial resolution matches that of PRISM data at 4 km by 4 km. In fact, gridMET is a product of combining PRISM with the Land Data Assimilation System (specifically, NLDAS-2).\ngridMET offers a broader range of variables than PRISM, including maximum temperature, minimum temperature, precipitation accumulation, downward surface shortwave radiation, wind velocity, and humidity (both maximum/minimum relative humidity and specific humidity). It also provides derived products, such as reference evapotranspiration, calculated using the Penman-Monteith equation.\nYou can use the downloadr::download() function to download gridMET data by variable-year. For example, to download precipitation data for 2018, you can run the following code:\n\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/pr_2018.nc\",\n  destfile = \"Data/pr_2018.nc\",\n  mode = \"wb\"\n)\n\nWe set the url of the dataset of interest in the url option, set the destination file name in the destfile option, and the mode to the wb option for a binary download.\nAll the gridMET datasets for direct download has “http://www.northwestknowledge.net/metdata/data/” at the beginning, followed by the file name (here, pr_2018.nc). The file names follow the convention of variable_abbreviation_year.nc. So, we can easily write a loop to get data for multiple variables over multiple years.\nHere is the list of variable abbreviations:\n\nsph: (Near-Surface Specific Humidity)\nvpd: (Mean Vapor Pressure Deficit)\npr: (Precipitation)\nrmin: (Minimum Near-Surface Relative Humidity)\nrmax: (Maximum Near-Surface Relative Humidity)\nsrad: (Surface Downwelling Solar Radiation)\ntmmn: (Minimum Near-Surface Air Temperature)\ntmmx: (Maximum Near-Surface Air Temperature)\nvs: (Wind speed at 10 m)\nth: (Wind direction at 10 m)\npdsi: (Palmer Drought Severity Index)\npet: (Reference grass evaportranspiration)\netr: (Reference alfalfa evaportranspiration)\nerc: (model-G)\nbi: (model-G)\nfm100: (100-hour dead fuel moisture)\nfm1000: (1000-hour dead fuel moisture)\n\nAs another example, if you are interested in downloading the wind speed data for 2020, then you can use the following code.\n\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/vs_2020.nc\",\n  destfile = \"Data/vs_2020.nc\",\n  mode = \"wb\"\n)\n\n\n8.5.1 Practical Examples\nSuppose your final goal is to get average daily precipitation (pr) and reference grass evapotranspiration (pet) from 2015 through 2020 for each of the counties in California.\nFirst get county boundaries for California:\n\nCA_counties &lt;- \n  tigris::counties(state = \"CA\", progress_bar = FALSE) %&gt;%\n  dplyr::select(STATEFP, COUNTYFP)\n\nBefore writing a loop, let’s work on a single case (pet for 2015). First, we download and read the data.\n\n#--- download data ---#\ndownloader::download(\n  url = \"http://www.northwestknowledge.net/metdata/data/pet_2015.nc\",\n  destfile = \"Data/pet_2015.nc\",\n  mode = \"wb\"\n)\n\n#--- read the raster data ---#\n(\n  pet_2015 &lt;- terra::rast(\"Data/pet_2015.nc\")\n)\n\n\n\nclass       : SpatRaster \ndimensions  : 585, 1386, 365  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -124.7875, -67.0375, 25.04583, 49.42083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : pet_2015.nc \nvarname     : potential_evapotranspiration (pet) \nnames       : poten~42003, poten~42004, poten~42005, poten~42006, poten~42007, poten~42008, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \n\n\nAs you can see, it is a multi-layer raster object where each layer represents a single day in 2015 (Figure 8.10 is a quick visualization of the first layer).\n\n\n\n\nCodeplot(pet_2015[[1]])\n\n\n\n\n\n\nFigure 8.10: Potential evapotranspiration in January 1st, 2015\n\n\n\n\nNow, we can use exactexactr::exact_extract() (or terra::extract) to assign cell values to each county and transform it to a more convenient form:\n\npet_county &lt;-\n  #--- extract data for each county ---#\n  exactextractr::exact_extract(pet_2015, CA_counties, progress = FALSE) %&gt;%\n  #--- list of data.frames into data.table ---#\n  data.table::rbindlist(idcol = \"rowid\")\n\n#--- check the dimension of the output ---#\ndim(pet_county)\n\n[1] 28203   367\n\n\nAs you can see the data has 367 columns: 365 (days) + 1 (rowid) + 1 (coverage fraction). Let’s take a look at the name of the first six variables.\n\nhead(names(pet_county))\n\n[1] \"rowid\"                                 \n[2] \"potential_evapotranspiration_day=42003\"\n[3] \"potential_evapotranspiration_day=42004\"\n[4] \"potential_evapotranspiration_day=42005\"\n[5] \"potential_evapotranspiration_day=42006\"\n[6] \"potential_evapotranspiration_day=42007\"\n\n\nThe 5-digit number at the end of the name of the variables for evapotranspiration represents days since Jan 1st, 1900. This can be confirmed using ncdf4:nc_open() (see the middle of the output below under day of 4 dimensions):\n\nncdf4::nc_open(\"Data/pet_2015.nc\")\n\nFile Data/pet_2015.nc (NC_FORMAT_NETCDF4):\n\n     1 variables (excluding dimension variables):\n        unsigned short potential_evapotranspiration[lon,lat,day]   (Chunking: [231,98,61])  (Compression: level 9)\n            _FillValue: 32767\n            units: mm\n            description: Daily reference evapotranspiration (short grass)\n            long_name: pet\n            standard_name: pet\n            missing_value: 32767\n            dimensions: lon lat time\n            grid_mapping: crs\n            coordinate_system: WGS84,EPSG:4326\n            scale_factor: 0.1\n            add_offset: 0\n            coordinates: lon lat\n            _Unsigned: true\n\n     4 dimensions:\n        lon  Size:1386 \n            units: degrees_east\n            description: longitude\n            long_name: longitude\n            standard_name: longitude\n            axis: X\n        lat  Size:585 \n            units: degrees_north\n            description: latitude\n            long_name: latitude\n            standard_name: latitude\n            axis: Y\n        day  Size:365 \n            description: days since 1900-01-01\n            units: days since 1900-01-01 00:00:00\n            long_name: time\n            standard_name: time\n            calendar: gregorian\n        crs  Size:1 \n            grid_mapping_name: latitude_longitude\n            longitude_of_prime_meridian: 0\n            semi_major_axis: 6378137\n            long_name: WGS 84\n            inverse_flattening: 298.257223563\n            GeoTransform: -124.7666666333333 0.041666666666666 0  49.400000000000000 -0.041666666666666\n            spatial_ref: GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n    19 global attributes:\n        geospatial_bounds_crs: EPSG:4326\n        Conventions: CF-1.6\n        geospatial_bounds: POLYGON((-124.7666666333333 49.400000000000000, -124.7666666333333 25.066666666666666, -67.058333300000015 25.066666666666666, -67.058333300000015 49.400000000000000, -124.7666666333333 49.400000000000000))\n        geospatial_lat_min: 25.066666666666666\n        geospatial_lat_max: 49.40000000000000\n        geospatial_lon_min: -124.7666666333333\n        geospatial_lon_max: -67.058333300000015\n        geospatial_lon_resolution: 0.041666666666666\n        geospatial_lat_resolution: 0.041666666666666\n        geospatial_lat_units: decimal_degrees north\n        geospatial_lon_units: decimal_degrees east\n        coordinate_system: EPSG:4326\n        author: John Abatzoglou - University of Idaho, jabatzoglou@uidaho.edu\n        date: 04 July 2019\n        note1: The projection information for this file is: GCS WGS 1984.\n        note2: Citation: Abatzoglou, J.T., 2013, Development of gridded surface meteorological data for ecological applications and modeling, International Journal of Climatology, DOI: 10.1002/joc.3413\n        note3: Data in slices after last_permanent_slice (1-based) are considered provisional and subject to change with subsequent updates\n        note4: Data in slices after last_provisional_slice (1-based) are considered early and subject to change with subsequent updates\n        note5: Days correspond approximately to calendar days ending at midnight, Mountain Standard Time (7 UTC the next calendar day)\n\n\nThis is universally true for all the gridMET data. We can use this information to recover date. First, let’s transform the data from a wide format to a long format for easier operations:\n\npet_county &lt;-\n  #--- wide to long ---#\n  melt(pet_county, id.var = c(\"rowid\", \"coverage_fraction\")) %&gt;%\n  #--- remove observations with NA values ---#\n  .[!is.na(value), ]\n\n#--- take a look ---#\npet_county\n\n          rowid coverage_fraction                               variable value\n          &lt;int&gt;             &lt;num&gt;                                 &lt;fctr&gt; &lt;num&gt;\n       1:     1       0.004266545 potential_evapotranspiration_day=42003   2.3\n       2:     1       0.254054248 potential_evapotranspiration_day=42003   2.2\n       3:     1       0.175513789 potential_evapotranspiration_day=42003   2.0\n       4:     1       0.442011684 potential_evapotranspiration_day=42003   2.1\n       5:     1       1.000000000 potential_evapotranspiration_day=42003   2.2\n      ---                                                                     \n10116701:    58       0.538234591 potential_evapotranspiration_day=42367   2.1\n10116702:    58       0.197555855 potential_evapotranspiration_day=42367   2.7\n10116703:    58       0.224901110 potential_evapotranspiration_day=42367   2.2\n10116704:    58       0.554238617 potential_evapotranspiration_day=42367   2.2\n10116705:    58       0.272462875 potential_evapotranspiration_day=42367   2.1\n\n\nWe now use stringr::str_sub() to get 5-digit numbers from variable, which represents days since Jan 1st, 1900. We can then recover dates using the lubridate package.\n\npet_county[, variable := str_sub(variable, -5, -1) %&gt;% as.numeric()] %&gt;%\n  #--- recover dates ---#\n  .[, date := variable + lubridate::ymd(\"1900-01-01\")]\n\n#--- take a look ---#\npet_county\n\n          rowid coverage_fraction variable value       date\n          &lt;int&gt;             &lt;num&gt;    &lt;num&gt; &lt;num&gt;     &lt;Date&gt;\n       1:     1       0.004266545    42003   2.3 2015-01-01\n       2:     1       0.254054248    42003   2.2 2015-01-01\n       3:     1       0.175513789    42003   2.0 2015-01-01\n       4:     1       0.442011684    42003   2.1 2015-01-01\n       5:     1       1.000000000    42003   2.2 2015-01-01\n      ---                                                  \n10116701:    58       0.538234591    42367   2.1 2015-12-31\n10116702:    58       0.197555855    42367   2.7 2015-12-31\n10116703:    58       0.224901110    42367   2.2 2015-12-31\n10116704:    58       0.554238617    42367   2.2 2015-12-31\n10116705:    58       0.272462875    42367   2.1 2015-12-31\n\n\nFinally, let’s calculate the coverage-weighted average of pet by county-date.\n\npet_county_avg &lt;-\n  pet_county[,\n    .(value = sum(value * coverage_fraction) / sum(coverage_fraction)),\n    by = .(rowid, date)\n  ] %&gt;%\n  setnames(\"value\", \"pet\")\n\nSince rowid value of n corresponds to the nth row in CA_counties, it is easy to merge pet_county_avg with CA_counties (alternatively, you can use cbind()).\n\nCA_pet &lt;- \n  CA_counties %&gt;%\n  mutate(rowid = seq_len(nrow(.))) %&gt;%\n  left_join(pet_county_avg, ., by = \"rowid\")\n\nNow that we know how to process a single gridMET dataset, we are ready to write a function that goes through the same for a choice of gridMET dataset and then write a loop to achieve our goal. Here is the function:\n\nget_grid_MET &lt;- function(var_name, year) {\n  #--- for testing ---#\n  # var_name &lt;- \"pet\"\n  # year &lt;- 2020\n\n  target_url &lt;-\n    paste0(\n      \"http://www.northwestknowledge.net/metdata/data/\",\n      var_name, \"_\", year,\n      \".nc\"\n    )\n\n  file_name &lt;-\n    paste0(\n      \"Data/\",\n      var_name, \"_\", year,\n      \".nc\"\n    )\n\n  downloader::download(\n    url = target_url,\n    destfile = file_name,\n    mode = \"wb\"\n  )\n\n  #--- read the raster data ---#\n  temp_rast &lt;- terra::rast(file_name)\n\n  temp_data &lt;-\n    #--- extract data for each county ---#\n    exactextractr::exact_extract(temp_rast, CA_counties) %&gt;%\n    #--- list of data.frames into data.table ---#\n    data.table::rbindlist(idcol = \"rowid\") %&gt;%\n    #--- wide to long ---#\n    data.table::melt(id.var = c(\"rowid\", \"coverage_fraction\")) %&gt;%\n    #--- remove observations with NA values ---#\n    .[!is.na(value), ] %&gt;%\n    #--- get only the numeric part ---#\n    .[, variable := stringr::str_sub(variable, -5, -1) %&gt;% as.numeric()] %&gt;%\n    #--- recover dates ---#\n    .[, date := variable + lubridate::ymd(\"1900-01-01\")] %&gt;%\n    #--- find daily coverage-weight average by county ---#\n    .[,\n      .(value = sum(value * coverage_fraction) / sum(coverage_fraction)),\n      by = .(rowid, date)\n    ] %&gt;%\n    .[, var := var_name]\n\n  return(temp_data)\n}\n\nLet’s now loop over the variables and years of interest. We first set up a dataset of variable-year combinations for which we loop over.\n\n#--- create a dataset of parameters to be looped over---#\n(\n  par_data &lt;-\n    expand.grid(\n      var_name = c(\"pr\", \"pet\"),\n      year = 2015:2020\n    ) %&gt;%\n    data.table::data.table() %&gt;%\n    .[, var_name := as.character(var_name)]\n)\n\nWe now loop over the rows of par_data in parallel:\n\n#--- parallel processing ---#\nfuture::plan(multisession, workers = parallel::detectCores() - 2)\n\n(\n  all_data &lt;-\n    future.apply::future_lapply(\n      seq_len(nrow(par_data)),\n      \\(x) get_grid_MET(var_name = par_data[x, var_name], year = par_data[x, year])\n    ) %&gt;%\n    data.table::rbindlist() %&gt;%\n    data.table::dcast(rowid + date ~ var, value.var = \"value\")\n)\n\n\n\n\n\n\n\n\n\n\nBocinsky, R. Kyle. 2016. FedData: Functions to Automate Downloading Geospatial Data Available from Several Federated Data Sources. http://CRAN.R-project.org/package=FedData.\n\n\nChen, Bowen. 2020. CropScapeR: Access Cropland Data Layer Data via the ’CropScape’ Web Service.\n\n\nHart, Edmund M., and Kendon Bell. 2015. Prism: Download Data from the Oregon Prism Project. https://doi.org/10.5281/zenodo.33663.\n\n\nHufkens, Koen, David Basler, Tom Milliman, Eli K. Melaas, and Andrew D. Richardson. 2018. “An Integrated Phenology Modelling Framework in r: Modelling Vegetation Phenology with phenor.” Methods in Ecology & Evolution 9: 1–10. http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12970/full.\n\n\nLindblad, Brad. 2020. tidyUSDA: A Minimal Tool Set for Gathering USDA Quick Stat Data for Analysis and Visualization. https://bradlindblad.github.io/tidyUSDA/.",
    "crumbs": [
      "Extensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Download and process spatial datasets from within R</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html",
    "href": "chapters/09-SpeedThingsUp.html",
    "title": "9  Extraction Speed Considerations",
    "section": "",
    "text": "Before you start\nIn this chapter, we will examine the speed of raster value extraction for vector data under various conditions and using different functions. Repeated raster value extraction is often necessary, such as when calculating county-level daily evapotranspiration for the past 30 years using PRISM data. In such cases, choosing the right strategy for minimizing extraction time can significantly impact performance.\nTo optimize extraction speed, we will explore parallelizing raster data extraction for polygon data. Parallelization for point data extraction will not be covered, as point extractions are typically very fast and unlikely to become a bottleneck in most workflows. We will start by discussing parallel extraction for single-layer raster data before progressing to multi-layer raster data.\nThere are several ways to parallelize the extraction process, and we will evaluate different approaches in terms of speed and memory usage. You’ll learn that the method of parallelization is crucial—naive parallelization can sometimes increase extraction time, while a more efficient approach can save hours or even days, depending on the scale of the task.\nWe will use the future.apply and parallel packages for parallelization. A basic understanding of these packages is assumed. If you are unfamiliar with looping via lapply() or parallelization methods like mclapply() (for Mac and Linux users) or future.apply::future_lapply() (for Windows and others), refer to Appendix A for an introduction.",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#before-you-start",
    "href": "chapters/09-SpeedThingsUp.html#before-you-start",
    "title": "9  Extraction Speed Considerations",
    "section": "",
    "text": "Objectives\n\n\n\n\nCompare extraction speed of terra::extract(), stars::extract(), exactextractr::exact_extract(), and aggregate.stars() under different conditions:\n\nVarying raster data size (number of cells)\nDifferent numbers of points and polygons for which raster values are extracted\nWhether the raster data is cropped to the area of interest first or not\n\n\nLearn how to parallelize the extraction process\n\n\n\n\n\n\n\nDirection for replication\nDatasets\nAll the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:\n\nset a folder (any folder) as the working directory using setwd()\n\ncreate a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step)\ndownload the pertinent datasets from here\n\nplace all the files in the downloaded folder in the “Data” folder\n\nWarning: the folder includes a series of daily PRISM datasets stored by month for 10 years. They amount to \\(12.75\\) GB of data.\nPackages\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  parallel, # for parallelization\n  future.apply, # for parallelization\n  terra, # handle raster data\n  raster, # handle raster data\n  stars, # handle raster data \n  exactextractr, # fast extractions\n  sf, # vector data operations\n  dplyr, # data wrangling\n  data.table, # data wrangling\n  prism, # download PRISM data\n  ggplot2, # mapping\n  tictoc # timing codes\n)",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#data-preparation",
    "href": "chapters/09-SpeedThingsUp.html#data-preparation",
    "title": "9  Extraction Speed Considerations",
    "section": "\n9.1 Data preparation",
    "text": "9.1 Data preparation\nWe use the following datasets in the first part of this Chapter:\nWells (points) in Kansas\n\n#--- read in the KS points data ---#\n(\nKS_wells &lt;- readRDS(\"Data/Chap_5_wells_KS.rds\")\n)\n\nSimple feature collection with 37647 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id                   geometry\n1        1 POINT (-100.4423 37.52046)\n2        3 POINT (-100.7118 39.91526)\n3        5 POINT (-99.15168 38.48849)\n4        7 POINT (-101.8995 38.78077)\n5        8  POINT (-100.7122 38.0731)\n6        9 POINT (-97.70265 39.04055)\n7       11 POINT (-101.7114 39.55035)\n8       12 POINT (-95.97031 39.16121)\n9       15 POINT (-98.30759 38.26787)\n10      17 POINT (-100.2785 37.71539)\n\n\nDaily PRISM tmax (January, 2009) as stars and SpatRaster\n\ntmax_m8_y09_stars &lt;- \n  stars::read_stars(\"Data/PRISM_tmax_y2009_m1.tif\") %&gt;%\n  #--- change the attribute name ---#\n  setNames(\"tmax\")\n(\ntmax_m8_y09_sr &lt;- as(tmax_m8_y09_stars, \"SpatRaster\")\n)\n\nclass       : SpatRaster \ndimensions  : 621, 1405, 31  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -125.0208, -66.47917, 24.0625, 49.9375  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nnames       : PRISM~1_bil, PRISM~2_bil, PRISM~3_bil, PRISM~4_bil, PRISM~5_bil, PRISM~6_bil, ... \nmin values  :     -17.898,     -21.161,     -20.686,     -25.589,     -26.769,      -20.63, ... \nmax values  :      28.271,      26.871,      29.709,      31.683,      31.397,       30.42, ... \n\n\nKansas county borders\n\n(\nKS_county_sf &lt;-\n  tigris::counties(state = \"Kansas\", cb = TRUE, progress_bar = FALSE) %&gt;%\n  dplyr::select(geometry) %&gt;%\n  #--- transform using the CRS of the PRISM stars data  ---#\n  sf::st_transform(sf::st_crs(tmax_m8_y09_stars)) %&gt;%\n  #--- generate unique id ---#\n  dplyr::mutate(id = 1:nrow(.))\n)\n\nSimple feature collection with 105 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n                         geometry id\n1  MULTIPOLYGON (((-101.0681 3...  1\n2  MULTIPOLYGON (((-97.3707 39...  2\n3  MULTIPOLYGON (((-101.1284 3...  3\n4  MULTIPOLYGON (((-99.56988 3...  4\n5  MULTIPOLYGON (((-99.62821 3...  5\n6  MULTIPOLYGON (((-96.72774 3...  6\n7  MULTIPOLYGON (((-101.103 37...  7\n8  MULTIPOLYGON (((-99.04234 3...  8\n9  MULTIPOLYGON (((-100.2477 3...  9\n10 MULTIPOLYGON (((-101.5419 3... 10\n\n\nDaily PRISM tmax (January, 2009) cropped to Kansas as stars and SpatRaster\n\n(\ntmax_m8_y09_KS_stars &lt;- sf::st_crop(tmax_m8_y09_stars, sf::st_bbox(KS_county_sf)) \n)\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu. Median     Mean 3rd Qu.   Max.\ntmax  -13.477   1.154   7.66 6.615802  13.127 23.685\ndimension(s):\n     from  to offset    delta refsys point\nx     552 731   -125  0.04167  NAD83 FALSE\ny     239 311  49.94 -0.04167  NAD83 FALSE\nband    1  31     NA       NA     NA    NA\n                                                                            values\nx                                                                             NULL\ny                                                                             NULL\nband PRISM_tmax_stable_4kmD2_20090101_bil,...,PRISM_tmax_stable_4kmD2_20090131_bil\n     x/y\nx    [x]\ny    [y]\nband    \n\ntmax_m8_y09_KS_sr &lt;- as(tmax_m8_y09_KS_stars, \"SpatRaster\")",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#sec-crop-first",
    "href": "chapters/09-SpeedThingsUp.html#sec-crop-first",
    "title": "9  Extraction Speed Considerations",
    "section": "\n9.2 Should we crop first?",
    "text": "9.2 Should we crop first?\n\n9.2.1 Extract for points\n\n\n\nHere is the results of benchmarking:\n\nCodemicrobenchmark::microbenchmark(\n  \"terra-no-crop\" = {\n    extracted_values &lt;- terra::extract(tmax_m8_y09_sr, KS_wells)\n  },\n  \"terra-crop\" = {\n    temp &lt;- terra::extract(terra::crop(tmax_m8_y09_sr, KS_wells), KS_wells)\n  },\n  \"stars-no-crop\" = {\n    extracted_values &lt;- stars::st_extract(tmax_m8_y09_stars, KS_wells)\n  },\n  \"stars-crop\" = {\n    extracted_values &lt;- stars::st_extract(sf::st_crop(tmax_m8_y09_stars, sf::st_bbox(KS_wells)), KS_wells)\n  },\n  times = 100\n)\n\n\n\n\nUnit: milliseconds\n          expr       min        lq      mean    median        uq      max neval\n terra-no-crop 229.26191 241.87681 257.39702 250.24133 263.98649 407.0301   100\n    terra-crop 228.96151 242.73248 258.61140 250.87914 263.29618 466.8389   100\n stars-no-crop  59.00630  63.25863  74.63379  67.79081  78.62381 210.9096   100\n    stars-crop  50.53406  52.29614  58.00831  54.21836  56.78662 124.1551   100\n cld\n a  \n a  \n  b \n   c\n\n\n\n9.2.1.1 terra::extract()\n\nwithout cropping\n\ntic()\nextracted_values &lt;- terra::extract(tmax_m8_y09_sr, KS_wells, FUN = mean)\ntoc()\n\n0.259 sec elapsed\n\n\nwith cropping\n\ntic()\nextracted_values &lt;- terra::extract(terra::crop(tmax_m8_y09_sr, KS_wells), KS_wells, FUN = mean)\ntoc()\n\n0.297 sec elapsed\n\n\nAs you can see, the difference in computation time is not large.\n\n9.2.1.2 stars::extract()\n\nwithout cropping\n\ntic()\nextracted_values &lt;- \n  stars::st_extract(\n    tmax_m8_y09_stars, \n    KS_wells, \n    FUN = mean\n  )\ntoc()\n\n0.053 sec elapsed\n\n\nwith cropping1\n1 Remember, if you crop to KS_wells instead of sf::st_bbox(KS_wells), it would take a lot longer (see Section 6.10.1).\ntic()\nextracted_values &lt;- \n  stars::st_extract(\n    sf::st_crop(tmax_m8_y09_stars, sf::st_bbox(KS_wells)), KS_wells,\n    FUN = mean\n  )\ntoc()\n\n0.039 sec elapsed\n\n\nAs you can see, the difference in computation time is not large either here.\n\n9.2.2 Extract for polygons\nWhen extracting for polygons, it typically pays off to first crop the raster data to the extent of the polygons data first before extraction.\naggregate.stars()\nHere, the raster dataset is tmax_m8_y09_stars which covers the entire contiguous U.S. even though you are extracting values for Kansas (KS_county_sf).\n\ntic()\nextracted_values &lt;- aggregate(tmax_m8_y09_stars, KS_county_sf, FUN = mean)\ntoc()\n\n\n\nelapsed \n  5.418 \n\n\nThis one first crops the raster data to the extent of Kansas and then extract.\n\ntic()\nextracted_values &lt;- \n  tmax_m8_y09_KS_stars %&gt;% \n  sf::st_crop(sf::st_bbox(KS_county_sf)) %&gt;% \n  aggregate(KS_county_sf, FUN = mean)\ntoc() \n\n0.127 sec elapsed\n\n\nYou can see a noticeable improvement.\nexactextractr::exact_extract()\nWithout cropping,\n\ntic()\nextracted_values &lt;- exactextractr::exact_extract(as(tmax_m8_y09_stars, \"SpatRaster\"), KS_county_sf, \"mean\", progress = FALSE)\ntoc()\n\n0.199 sec elapsed\n\n\nWith cropping,\n\ntic()\nresults &lt;- tmax_m8_y09_KS_stars %&gt;% \n  st_crop(st_bbox(KS_county_sf)) %&gt;% \n  as(\"SpatRaster\") %&gt;% \n  exactextractr::exact_extract(KS_county_sf, \"mean\", progress = FALSE)\ntoc() \n\n0.099 sec elapsed\n\n\nSo, it is still worthwhile to crop first, but the benefit of doing so is not as large as aggregate.stars() experienced. This is because exactextractr::exact_extract does chunk-by-chunk operations where the unnecessary parts of the data are hardly relevant in the entire process.\nterra::extract()\n\ntic()\nextracted_values &lt;- terra::extract(tmax_m8_y09_sr, KS_county_sf, fun = mean)\ntoc()\n\n0.036 sec elapsed\n\n\n\ntic()\nextracted_values &lt;- terra::extract(terra::crop(tmax_m8_y09_sr, KS_county_sf), KS_county_sf, fun = mean)\ntoc()\n\n0.047 sec elapsed\n\n\nVirtually no time difference between the two.\n\nGiven, how fast terra::extract() is, you might wonder if you should convert the stars object to a SpatRaster object, and then extract with terra::extract() instead of aggregate.stars().\n\n#--- terra::extract() with internal conversion to \"SpatRaster\" ---#\ntic()\nextracted_values &lt;- terra::extract(as(tmax_m8_y09_sr, \"SpatRaster\"), KS_county_sf, fun = mean)\ntoc()\n\n0.04 sec elapsed\n\n#--- aggregate.stars() with cropping ---#\ntic()\nextracted_values &lt;-\n  tmax_m8_y09_KS_stars %&gt;%\n  sf::st_crop(sf::st_bbox(KS_county_sf)) %&gt;%\n  aggregate(KS_county_sf, FUN = mean)\ntoc()\n\n0.134 sec elapsed\n\n\nWell, the winner is clear here. Even if you mainly use stars to handle raster data, you might want to consider using terra::extract() if you need to repeat raster value extraction many many times given how simple it is to convert a stars object to a SpatRaster with as(stars, \"SpatRaster\").",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#sec-num-cells-geometries",
    "href": "chapters/09-SpeedThingsUp.html#sec-num-cells-geometries",
    "title": "9  Extraction Speed Considerations",
    "section": "\n9.3 The number of raster cells and vector geometries",
    "text": "9.3 The number of raster cells and vector geometries\n\n9.3.1 Base line\nLet’s start with the example we used above in Section 6.10.3 using KS_county_sf as the polygons data and tmax_m8_y09_KS_stars and tmax_m8_y09_KS_sr (they are already cropped to Kansas) as the raster data.\nterra::extract()\n\ntic()\nextracted_values &lt;- terra::extract(tmax_m8_y09_KS_sr, KS_county_sf, FUN = mean)\ntoc()\n\n0.048 sec elapsed\n\n\naggregate.stars()\n\ntic()\nextracted_values &lt;- aggregate(sf::st_crop(tmax_m8_y09_KS_stars, KS_county_sf), KS_county_sf, FUN = mean) %&gt;%\n  st_as_sf()\ntoc()\n\n0.294 sec elapsed\n\n\nexactextractr::exact_extract()\n\ntic()\nextracted_values &lt;- exactextractr::exact_extract(tmax_m8_y09_KS_sr, KS_county_sf, \"mean\", progress = FALSE)\ntoc()\n\n0.03 sec elapsed\n\n\nAll of them are quite fast, but terra::extract and exactextractr::exact_extract() are clearly faster than aggregate.stars().\n\n9.3.2 Large number of polygons\nNow, let’s increase the number of polygons without changing the spatial extent of the polygons data. This is done by creating lots of regular grids over Kansas.\n\n(\ngrids_in_KS &lt;- \n  sf::st_make_grid(KS_county_sf, n = c(200, 200)) %&gt;%\n  st_as_sf()\n)\n\nSimple feature collection with 40000 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n                                x\n1  POLYGON ((-102.0517 36.9930...\n2  POLYGON ((-102.0144 36.9930...\n3  POLYGON ((-101.9771 36.9930...\n4  POLYGON ((-101.9398 36.9930...\n5  POLYGON ((-101.9025 36.9930...\n6  POLYGON ((-101.8652 36.9930...\n7  POLYGON ((-101.8278 36.9930...\n8  POLYGON ((-101.7905 36.9930...\n9  POLYGON ((-101.7532 36.9930...\n10 POLYGON ((-101.7159 36.9930...\n\n\nIn total, grids_in_KS has 40,000 polygons (Figure 9.1 shows what the grids look like).\n\n\n\n\nCodeggplot() +\n  geom_stars(data = tmax_m8_y09_KS_stars[,,,1]) +\n  scale_fill_viridis_c() +\n  geom_sf(data = grids_in_KS, fill = NA) +\n  theme_void() \n\n\n\n\n\n\nFigure 9.1: 40,000 regular grids over tmax data for Kansas\n\n\n\n\nNow, let’s compare the three approaches.\nterra::extract()\n\ntic()\nextracted_values &lt;- terra::extract(tmax_m8_y09_KS_sr, grids_in_KS, FUN = mean)\ntoc()\n\n3.592 sec elapsed\n\n\naggregate()\n\ntic()\nextracted_values &lt;- aggregate(tmax_m8_y09_KS_stars, grids_in_KS, FUN = mean) %&gt;% \n  st_as_sf\ntoc()\n\n3.98 sec elapsed\n\n\nexact_extract()\n\ntic()\nextracted_values &lt;- exactextractr::exact_extract(tmax_m8_y09_KS_sr, grids_in_KS, \"mean\", progress = FALSE)\ntoc()\n\n3.545 sec elapsed\n\n\nInterestingly, exactextractr::exact_extract() is affected by an increase in the number of polygons more than aggregate().\n\n9.3.3 Large number of raster cells\nNow, let’s make tmax_m8_y09_KS_sr much larger by disaggregating it by a factor of 10 (100 times more cells).\n\n(\ntmax_m8_y09_KS_sr_large &lt;- terra::disagg(tmax_m8_y09_KS_sr, fact = 10)\n)\n\nclass       : SpatRaster \ndimensions  : 730, 1800, 31  (nrow, ncol, nlyr)\nresolution  : 0.004166667, 0.004166667  (x, y)\nextent      : -102.0625, -94.5625, 36.97917, 40.02083  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nnames       : PRISM~1_bil, PRISM~2_bil, PRISM~3_bil, PRISM~4_bil, PRISM~5_bil, PRISM~6_bil, ... \nmin values  :      -2.409,       3.844,       7.267,      -1.286,      -6.891,      -2.286, ... \nmax values  :      10.005,      16.707,      21.849,      23.120,       1.519,       9.483, ... \n\n#--- stars version ---#\ntmax_m8_y09_KS_stars_large &lt;- st_as_stars(tmax_m8_y09_KS_sr_large)\n\nterra::extract()\n\ntic()\nextracted_values &lt;- terra::extract(tmax_m8_y09_KS_sr_large, grids_in_KS, FUN = mean)\ntoc()\n\n\n\nelapsed \n  5.967 \n\n\naggregate()\n\ntic()\nextracted_values &lt;- aggregate(tmax_m8_y09_KS_stars_large, grids_in_KS, FUN = mean)\ntoc()\n\n\n\nelapsed \n157.947 \n\n\nexact_extract()\n\ntic()\nextracted_values &lt;- exactextractr::exact_extract(tmax_m8_y09_KS_sr_large, grids_in_KS, \"mean\", progress = FALSE)\ntoc()\n\n\n\nelapsed \n   4.18 \n\n\nHere, exactextractr::exact_extract() outperforms terra::extract(), both of which outperform significantly aggregate.stars(). Indeed, aggregate.stars() is painfully slow.",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#parallelization-on-a-single-raster-layer",
    "href": "chapters/09-SpeedThingsUp.html#parallelization-on-a-single-raster-layer",
    "title": "9  Extraction Speed Considerations",
    "section": "\n9.4 Parallelization on a single raster layer",
    "text": "9.4 Parallelization on a single raster layer\nLet’s prepare for parallel processing for the rest of the section.\n\n#--- get the number of logical cores to use ---#\n(\n  num_cores &lt;- parallel::detectCores() - 2\n)\n\n[1] 18\n\n\n\n9.4.1 Datasets\nWe will use the following datasets:\n\n\nraster: Iowa Cropland Data Layer (CDL) data in 2015\n\n\npolygons: Regular polygon grids over Iowa\n\nIowa CDL data in 2015 (Figure 9.2)\n\n#--- Iowa CDL in 2015 ---#\n(\nIA_cdl_15 &lt;- terra::rast(\"Data/IA_cdl_2015.tif\")\n)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs \nsource      : IA_cdl_2015.tif \ncolor table : 1 \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n\n\n\n\n\n\nCodeplot(IA_cdl_15)\n\n\n\n\n\n\nFigure 9.2: Land use type in Iowa in 2105\n\n\n\n\nValues recorded in the raster data are integers representing land use type.\nRegularly-sized grids over Iowa (Figure 9.3)\n\n#--- regular grids over Iowa ---#\nIA_grids &lt;-\n  tigris::counties(state = \"IA\", cb = TRUE) %&gt;%\n  #--- create regularly-sized grids ---#\n  sf::st_make_grid(n = c(100, 100)) %&gt;%\n  sf::st_as_sf() %&gt;%\n  dplyr::rename(geometry = x) %&gt;%\n  #--- project to the CRS of the CDL data ---#\n  st_transform(terra::crs(IA_cdl_15))\n\n\n\n\n\nCodeggplot(IA_grids) +\n  geom_sf(fill = NA) +\n  theme_void()\n\n\n\n\n\n\nFigure 9.3: Regular grids over Iowas as sf\n\n\n\n\n\n9.4.2 Parallelization\nHere is how long it takes to extract raster data values for the polygon grids using exactextractr::exact_extract() (terra::extract() takes too much time and is not practical for this set of datasets).\n\ntic()\ntemp &lt;-\n  exactextractr::exact_extract(IA_cdl_15, IA_grids) %&gt;% \n  data.table::rbindlist()\ntoc()\n\n\n\nelapsed \n 17.898 \n\n\n\nOne way to parallelize this process is to let each core work on one polygon at a time. Let’s first define the function to extract values for one polygon and then run it for all the polygons parallelized.\n\n#--- function to extract raster values for a single polygon ---#\nget_values_i &lt;- function(i) {\n  temp &lt;- \n    exactextractr::exact_extract(IA_cdl_15, IA_grids[i, ]) %&gt;%\n    data.table::rbindlist()\n\n  return(temp)\n}\n\n#--- parallelized ---#\ntic()\ntemp &lt;- parallel::mclapply(1:nrow(IA_grids), get_values_i, mc.cores = num_cores)\ntoc()\n\n\n\nelapsed \n 51.192 \n\n\nAs you can see, this is not a good way to parallelize the computation process. To see why, let’s look at the computation time of extracting from one polygon, two polygons, and up to five polygons.\n\nmb &lt;- \n  microbenchmark::microbenchmark(\n    \"p_1\" = {\n      temp &lt;- exactextractr::exact_extract(IA_cdl_15, IA_grids[1, ])\n    },\n    \"p_2\" = {\n      temp &lt;- exactextractr::exact_extract(IA_cdl_15, IA_grids[1:2, ])\n    },\n    \"p_3\" = {\n      temp &lt;- exactextractr::exact_extract(IA_cdl_15, IA_grids[1:3, ])\n    },\n    \"p_4\" = {\n      temp &lt;- exactextractr::exact_extract(IA_cdl_15, IA_grids[1:4, ])\n    },\n    \"p_5\" = {\n      temp &lt;- exactextractr::exact_extract(IA_cdl_15, IA_grids[1:5, ])\n    },\n    times = 100\n  )\n\nFigure 9.4 shows the results of the benchmarking.\n\nCodemb %&gt;%\n  data.table() %&gt;%\n  .[, expr := gsub(\"p_\", \"\", expr)] %&gt;%\n  ggplot(.) +\n  geom_boxplot(aes(y = time / 1e9, x = expr)) +\n  ylim(0, NA) +\n  ylab(\"seconds\") +\n  xlab(\"number of polygons to process\")\n\n\n\n\n\n\nFigure 9.4: Comparison of the computation time of raster data extractions\n\n\n\n\nAs you can see, there is a significant overhead (about 0.02 seconds) regardless of the number of polygons being processed for data extraction. Once the process is initiated and ready to begin extracting values for the polygons, the additional time required to process extra units is minimal. This serves as a prime example of how not to parallelize a task. Since each core processes approximately 555 polygons, simple math suggests that you would spend at least 11.1 seconds (calculated as 0.02 \\(\\times\\) 555) just in preparing the extraction jobs.\n\nWe can minimize this overhead as much as possible by having each core use exactextract::exact_extract() only once in which multiple polygons are processed in the single call. Specifically, we will split the collection of the polygons into 18 groups and have each core extract for one group.\n\n#--- number of polygons in a group ---#\nnum_in_group &lt;- floor(nrow(IA_grids) / num_cores)\n\n#--- assign group id to polygons ---#\nIA_grids &lt;- \n  IA_grids %&gt;%\n  dplyr::mutate(\n    #--- create grid id ---#\n    grid_id = 1:nrow(.),\n    #--- assign group id  ---#\n    group_id = grid_id %/% num_in_group + 1\n  )\n\ntic()\n#--- parallelized processing by group ---#\ntemp &lt;- \n  parallel::mclapply(\n    1:num_cores,\n    \\(x) {\n      exactextractr::exact_extract(IA_cdl_15, dplyr::filter(IA_grids, group_id == x)) %&gt;%\n      data.table::rbindlist()\n    },\n    mc.cores = num_cores\n  )\ntoc()\n\n\n\nelapsed \n  5.249 \n\n\nOkay, this is much better.\n\nNow, we can further reduce the processing time by reducing the size of the object that is returned from each core to be collated into one. In the code above, each core returns a list of data.frames where each grid of the same group has multiple values from the intersecting raster cells.\nIn total, about 2.3GB of data has to be collated into one list from 18 cores. It turns out, this process is costly. To see this, take a look at the following example where the same exactextractr::exact_extrct() processes are run, yet nothing is returned by each core.\n\n#--- define the function to extract values by block of polygons ---#\nextract_by_group &lt;- function(i) {\n  temp &lt;- \n    exactextractr::exact_extract(IA_cdl_15, filter(IA_grids, group_id == i)) %&gt;%\n    data.table::rbindlist()\n\n  #--- returns nothing! ---#\n  return(NULL)\n}\n\n#--- parallelized processing by group ---#\ntic()\ntemp &lt;- parallel::mclapply(\n  1:num_cores,\n  function(i) extract_by_group(i),\n  mc.cores = num_cores\n)\ntoc()\n\n\n\nelapsed \n   2.07 \n\n\nApproximately 3.179 seconds were used just to collect the 2.3GB worth of data from the cores into one.\nIn most cases, we do not have to carry around all the individual cell values of land use types for our subsequent analysis. For example, in Demonstration 3 (Section 1.3) we just need a summary (count) of each unique land use type by polygon. So, let’s get the summary before we have the computer collect the objects returned from each core as follows:\n\nextract_by_group_reduced &lt;- function(i) {\n  temp_return &lt;- \n    exactextractr::exact_extract(\n      IA_cdl_15,\n      filter(IA_grids, group_id == i)\n    ) %&gt;%\n    #--- combine the list of data.frames into one with polygon id ---#\n    data.table::rbindlist(idcol = \"id_within_group\") %&gt;%\n    #--- find the count of land use type values by polygon ---#\n    .[, .(num_value = .N), by = .(value, id_within_group)]\n\n  return(temp_return)\n}\n\ntic()\n#--- parallelized processing by group ---#\ntemp &lt;- parallel::mclapply(\n  1:num_cores,\n  function(i) extract_by_group_reduced(i),\n  mc.cores = num_cores\n)\ntoc()\n\n\n\nelapsed \n  2.994 \n\n\nIt is of course slower than the one that returns nothing, but it is faster than the one that does not reduce the size before the outcome collation.\n\nAs you can see, the computation time of the fastest approach is now significantly reduced, but you only saved 48.2 seconds. How much time did I spend writing the code to implement the parallelized group processing? About three minutes. What truly matters is the total time you spend (coding time plus processing time) to get the desired outcome. The maximum time you could save with clever coding is 51.19 seconds. If writing code to make it faster takes more time than that, it’s simply not worth the effort. So, don’t try to optimize your code if the processing time is already short. Before you dive into parallelization, think through the coding steps in your head and assess whether it’s really worth the time investment.\nHowever, imagine processing CDL data for all U.S. states from 2009 to 2020. The entire process would take approximately 8.7 hours (calculated as \\(51 \\times 12 \\times 51.192/60/60\\)). A rough estimate suggests that with parallelization, using the best approach we discussed, the process could be completed in about 0.51 hours. While 8.7 hours is still manageable (you could start the process before bed and have the results ready by the next afternoon), it becomes worthwhile to parallelize the process, especially considering the time savings from parallelization, even after accounting for the time spent coding it.\n\n\n\n\n\n\nSummary\n\n\n\n\nDo not let each core runs small tasks over and over again (e.g., extracting raster values for one polygon at a time), or you will suffer from significant overhead.\nBlocking is one way to avoid the problem above.\nReduce the size of the outcome of each core as much as possible to spend less time to simply collating them into one.\nDo not forget about the time you would spend on coding parallelized processes.\nIf you are extracting from a single layer, it is likely that you should not parallelize.",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/09-SpeedThingsUp.html#sec-many-multi-layer",
    "href": "chapters/09-SpeedThingsUp.html#sec-many-multi-layer",
    "title": "9  Extraction Speed Considerations",
    "section": "\n9.5 Parallelization on many multi-layer raster files",
    "text": "9.5 Parallelization on many multi-layer raster files\nIn this section, we discuss various methods to parallelize the process of extracting values from many multi-layer raster files.\n\n9.5.1 Datasets\nWe will use the following datasets:\n\n\nraster: daily PRISM data 2010 through 2019 stacked by month\n\npolygons: US County polygons\n\ndaily PRISM precipitation 2010 through 2019\nYou can download all the prism files from here. For those who are interested in learning how to generate the series of daily PRISM data files stored by month, see Section 8.3 for the code.\nLet’s retrieve the U.S. counties data (see Figure 9.5 for the map).\n\n(\n  US_county &lt;-\n    tigris::counties(cb = TRUE, progress_bar = FALSE) %&gt;%\n    #--- only keep geometry ---#\n    dplyr::select(geometry) %&gt;%\n    #--- project to the CRS of the CDL data ---#\n    sf::st_transform(terra::crs(terra::rast(\"Data/PRISM_ppt_y2009_m1.tif\")))\n)\n\nSimple feature collection with 3235 features and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\nFirst 10 features:\n                         geometry\n1  MULTIPOLYGON (((-85.71209 3...\n2  MULTIPOLYGON (((-88.47323 3...\n3  MULTIPOLYGON (((-85.74803 3...\n4  MULTIPOLYGON (((-88.34043 3...\n5  MULTIPOLYGON (((-88.13925 3...\n6  MULTIPOLYGON (((-114.7312 3...\n7  MULTIPOLYGON (((-110.0007 3...\n8  MULTIPOLYGON (((-94.48558 3...\n9  MULTIPOLYGON (((-91.40687 3...\n10 MULTIPOLYGON (((-118.6044 3...\n\n\n\n\n\n\nCodeggplot(US_county) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\nFigure 9.5: U.S. counties\n\n\n\n\n\n9.5.2 Non-parallelized extraction\nAs we learned in Section 5.3, extracting values from stacked raster layers (multi-layer SpatRaster) is faster than extracting from multiple single-layer raster datasets one at a time. In this case, daily precipitation datasets are stacked by year and month and saved as multi-layer GeoTIFF files. For example, PRISM_ppt_y2009_m1.tif contains the daily precipitation data for January 2009. Below is an example of how long it takes to extract values for U.S. counties from a month of daily PRISM precipitation data.\n\ntic()\ntemp &lt;- \n  exactextractr::exact_extract(\n    terra::rast(\"Data/PRISM_ppt_y2009_m1.tif\"),\n    US_county,\n    \"mean\",\n    progress = F\n  )\ntoc()\n\n\n\nelapsed \n  5.556 \n\n\nNow, to process all the precipitation data from 2009-2018, we consider two approaches in this section are:\n\nparallelize over polygons (blocked) and do regular loop over year-month\nparallelize over year-month\n\n9.5.3 Approach 1: parallelize over polygons and do regular loop over year-month\nFor this approach, let’s measure the time spent on processing one year-month PRISM dataset and then guess how long it would take to process 120 year-month PRISM datasets.\n\n#--- number of polygons in a group ---#\nnum_in_group &lt;- floor(nrow(US_county) / num_cores)\n\n#--- define group id ---#\nUS_county &lt;- US_county %&gt;%\n  mutate(\n    #--- create grid id ---#\n    poly_id = 1:nrow(.),\n    #--- assign group id  ---#\n    group_id = poly_id %/% num_in_group + 1\n  )\n\nextract_by_group &lt;- function(i) {\n  temp_return &lt;- exactextractr::exact_extract(\n    terra::rast(\"Data/PRISM_ppt_y2009_m1.tif\"),\n    dplyr::filter(US_county, group_id == i)\n  ) %&gt;%\n    #--- combine the list of data.frames into one with polygon id ---#\n    data.table::rbindlist(idcol = \"id_within_group\") %&gt;%\n    #--- find the count of land use type values by polygon ---#\n    data.table::melt(id.var = c(\"id_within_group\", \"coverage_fraction\")) %&gt;%\n    .[, sum(value * coverage_fraction) / sum(coverage_fraction), by = .(id_within_group, variable)]\n\n  return(temp_return)\n}\n\ntic()\ntemp &lt;- parallel::mclapply(1:num_cores, extract_by_group, mc.cores = num_cores)\ntoc()\n\n\n\nelapsed \n  0.757 \n\n\nOkay, this approach is not bad at all. If we are to process 10 years of daily PRISM data, then it would take roughly 1.51 minutes.\n\n9.5.4 Approach 2: parallelize over the temporal dimension (year-month)\nInstead of parallelize over polygons, let’s parallelize over time (year-month). To do so, we first create a data.frame that has all the year-month combinations we will work on.\n\n(\n  month_year_data &lt;- data.table::CJ(month = 1:12, year = 2009:2018)\n)\n\nThe following function extract data from a single year-month case:\n\nget_prism_by_month &lt;- function(i, vector) {\n  temp_month &lt;- month_year_data[i, month] # month to work on\n  temp_year &lt;- month_year_data[i, year] # year to work on\n\n  #--- import raster data ---#\n  temp_raster &lt;- terra::rast(paste0(\"Data/PRISM/PRISM_ppt_y\", temp_year, \"_m\", temp_month, \".tif\"))\n\n  temp &lt;- exactextractr::exact_extract(temp_raster, vector, \"mean\")\n\n  return(temp)\n\n  gc()\n}\n\nWe then loop over the rows of month_year_data in parallel.\n\ntic()\ntemp &lt;-\n   parallel::mclapply(\n     1:nrow(month_year_data),\n     \\(x) get_prism_by_month(x, US_county),\n     mc.cores = num_cores\n   )\ntoc()\n\n\n\nelapsed \n 13.736 \n\n\nIt took 0.23 minutes. So, Approach 2 is the clear winner.\n\n9.5.5 Memory consideration\nSo far, we have not addressed the memory footprint of the parallelized processes, but it becomes crucial when working with many large datasets. Approaches 1 and 2 differ significantly in their memory usage.\n\n\nApproach 1: divides the polygons into groups and parallelizes over these groups when extracting raster values.\n\nApproach 2: extracts and holds raster values for r num_cores of the entire U.S. polygons at once.\n\nClearly, Approach 1 has a smaller memory footprint. Approach 2, on the other hand, used about 40 GB of memory, nearly maxing out my computer’s 64 GB of RAM (with other processes also consuming memory). As long as you stay within the memory limits, Approach 2 is more efficient. However, if I had only 32 GB of RAM, Approach 2 would have experienced a significant performance drop, while Approach 1 would not. Similarly, if the raster data had twice as many cells within the same spatial extent, Approach 2 would suffer, whereas Approach 1 would not.\nIt’s easy to imagine situations where Approach 1 is preferable. For example, if you have multiple 10-GB raster layers and only 16 GB of RAM, Approach 2 would clearly be impractical, making Approach 1 the better, and perhaps only, choice—far better than not parallelizing at all.\nIn summary, while processing larger datasets with each core can improve performance, you must be cautious not to exceed your computer’s RAM limits.\n\n\n\n\n\n\nSummary\n\n\n\n\nParallelize over time rathe than space as long as your RAM memory allows it\nParallelizing over space is still better than not parallelizing at all",
    "crumbs": [
      "(slightly) Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Extraction Speed Considerations</span>"
    ]
  },
  {
    "objectID": "chapters/A1-ParallelComputing.html",
    "href": "chapters/A1-ParallelComputing.html",
    "title": "Appendix A — Loop and Parallel Computing",
    "section": "",
    "text": "Before you start\nHere we will learn how to program repetitive operations effectively and fast. We start from the basics of a loop for those who are not familiar with the concept. We then cover parallel computation using the future.lapply and parallel package. Those who are familiar with lapply() can go straight to Chapter @ref(parcomp).\nHere are the specific learning objectives of this chapter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Loop and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "chapters/A1-ParallelComputing.html#before-you-start",
    "href": "chapters/A1-ParallelComputing.html#before-you-start",
    "title": "Appendix A — Loop and Parallel Computing",
    "section": "",
    "text": "Learn how to use for loop and lapply() to complete repetitive jobs\nLearn how not to loop things that can be easily vectorized\nLearn how to parallelize repetitive jobs using the future_lapply() function from the future.apply package\n\nDirection for replication\nAll the data in this Chapter is generated.\nPackages to install and load\nRun the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function.\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  dplyr, # data wrangling\n  data.table # data wrangling\n)\n\nThere are other packages that will be loaded during the demonstration.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Loop and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "chapters/A1-ParallelComputing.html#repetitive-processes-and-looping",
    "href": "chapters/A1-ParallelComputing.html#repetitive-processes-and-looping",
    "title": "Appendix A — Loop and Parallel Computing",
    "section": "\nA.1 Repetitive processes and looping",
    "text": "A.1 Repetitive processes and looping\n\nA.1.1 What is looping?\nWe sometimes need to run the same process over and over again often with slight changes in parameters. In such a case, it is very time-consuming and messy to write all of the steps one bye one. For example, suppose you are interested in knowing the square of 1 through 5 (\\([1, 2, 3, 4, 5]\\)). The following code certainly works:\n\n1^2\n\n[1] 1\n\n2^2\n\n[1] 4\n\n3^2\n\n[1] 9\n\n4^2\n\n[1] 16\n\n5^2\n\n[1] 25\n\n\nHowever, imagine you have to do this for 1000 integers. Yes, you don’t want to write each one of them one by one as that would occupy 1000 lines of your code, and it would be time-consuming. Things will be even worse if you need to repeat much more complicated processes like Monte Carlo simulations. So, let’s learn how to write a program to do repetitive jobs effectively using loop.\nLooping is repeatedly evaluating the same (except parameters) process over and over again. In the example above, the same process is the action of squaring. This does not change among the processes you run. What changes is what you square. Looping can help you write a concise code to implement these repetitive processes.\n\nA.1.2 For loop\nHere is how for loop works in general:\n\nfor (x in a_list_of_values){\n  you do what you want to do with x\n}\n\nAs an example, let’s use this looping syntax to get the same results as the manual squaring of 1 through 5:\n\nfor (x in 1:5) {\n  print(x^2)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n\n\nHere, a list of values is \\(1, 2, 3, 4, 5]\\). For each value in the list, you square it (x^2) and then print it (print()). If you want to get the square of \\(1:1000\\), the only thing you need to change is the list of values to loop over as in:\n\n#--- evaluation not reported as it's too long ---#\nfor (x in 1:1000) {\n  print(x^2)\n}\n\nSo, the length of the code does not depend on how many repeats you do, which is an obvious improvement over manual typing of every single process one by one. Note that you do not have to use \\(x\\) to refer to an object you are going to use. It could be any combination of letters as long as you use it when you code what you want to do inside the loop. So, this would work just fine,\n\nfor (bluh_bluh_bluh in 1:5) {\n  print(bluh_bluh_bluh^2)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n\n\n\nA.1.3 For loop using the lapply() function\nYou can do for loop using the lapply() function as well.1 Here is how it works:\n1 lpply() in only one of the family of apply() functions. We do not talk about other types of apply() functions here (e.g., apply(), spply(), mapply(),, tapply()). Personally, I found myself only rarely using them. But, if you are interested in learning those, take a look at here or here.\n#--- NOT RUN ---#\nlapply(A, B)\n\nwhere \\(A\\) is the list of values you go through one by one in the order the values are stored, and \\(B\\) is the function you would like to apply to each of the values in \\(A\\). For example, the following code does exactly the same thing as the above for loop example.\n\nlapply(1:5, function(x) {\n  x^2\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nHere, \\(A\\) is \\([1, 2, 3, 4, 5]\\). In \\(B\\) you have a function that takes \\(x\\) and square it. So, the above code applies the function to each of \\([1, 2, 3, 4, 5]\\) one by one. In many circumstances, you can write the same looping actions in a much more concise manner using the lapply function than explicitly writing out the loop process as in the above for loop examples. You might have noticed that the output is a list. Yes, lapply() returns the outcomes in a list. That is where l in lapply() comes from.\nWhen the operation you would like to repeat becomes complicated (almost always the case), it is advisable that you create a function of that process first.\n\n#--- define the function first ---#\nsquare_it &lt;- function(x) {\n  return(x^2)\n}\n\n#--- lapply using the pre-defined function ---#\nlapply(1:5, square_it)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\nFinally, it is a myth that you should always use lapply() instead of the explicit for loop syntax because lapply() (or other apply() families) is faster. They are basically the same.2\n2 Check this discussion on StackOverflow. You might want to check out this video at 6:10 as well.\nA.1.4 Looping over multiple variables using lapply()\n\nlapply() allows you to loop over only one variable. However, it is often the case that you want to loop over multiple variables. However, it is easy to achieve this. The trick is to create a data.frame of the variables where the complete list of the combinations of the variables are stored, and then loop over row of the data.frame. As an example, suppose we are interested in understanding the sensitivity of corn revenue to corn price and applied nitrogen amount. We consider the range of $3.0/bu to $5.0/bu for corn price and 0 lb/acre to 300/acre for nitrogen rate.\n\n#--- corn price vector ---#\ncorn_price_vec &lt;- seq(3, 5, by = 1)\n\n#--- nitrogen vector ---#\nnitrogen_vec &lt;- seq(0, 300, by = 100)\n\nAfter creating vectors of the parameters, you combine them to create a complete combination of the parameters using the expand.grid() function, and then convert it to a data.frame object3.\n3 Converting to a data.frame is not strictly necessary.\n#--- crate a data.frame that holds parameter sets to loop over ---#\nparameters_data &lt;-\n  expand.grid(\n    corn_price = corn_price_vec,\n    nitrogen = nitrogen_vec\n  ) %&gt;%\n  #--- convert the matrix to a data.frame ---#\n  data.frame()\n\n#--- take a look ---#\nparameters_data\n\n   corn_price nitrogen\n1           3        0\n2           4        0\n3           5        0\n4           3      100\n5           4      100\n6           5      100\n7           3      200\n8           4      200\n9           5      200\n10          3      300\n11          4      300\n12          5      300\n\n\nWe now define a function that takes a row number, refer to parameters_data to extract the parameters stored at the row number, and then calculate corn yield and revenue based on the extracted parameters.\n\ngen_rev_corn &lt;- function(i) {\n\n  #--- define corn price ---#\n  corn_price &lt;- parameters_data[i, \"corn_price\"]\n\n  #--- define nitrogen  ---#\n  nitrogen &lt;- parameters_data[i, \"nitrogen\"]\n\n  #--- calculate yield ---#\n  yield &lt;- 240 * (1 - exp(0.4 - 0.02 * nitrogen))\n\n  #--- calculate revenue ---#\n  revenue &lt;- corn_price * yield\n\n  #--- combine all the information you would like to have  ---#\n  data_to_return &lt;- data.frame(\n    corn_price = corn_price,\n    nitrogen = nitrogen,\n    revenue = revenue\n  )\n\n  return(data_to_return)\n}\n\nThis function takes \\(i\\) (act as a row number within the function), extract corn price and nitrogen from the \\(i\\)th row of parameters_mat, which are then used to calculate yield and revenue4. Finally, it returns a data.frame of all the information you used (the parameters and the outcomes).\n4 Yield is generated based on the Mitscherlich-Baule functional form. Yield increases at the decreasing rate as you apply more nitrogen, and yield eventually hits the plateau.\n#--- loop over all the parameter combinations ---#\nrev_data &lt;- lapply(1:nrow(parameters_data), gen_rev_corn)\n\n#--- take a look ---#\nrev_data\n\n[[1]]\n  corn_price nitrogen   revenue\n1          3        0 -354.1138\n\n[[2]]\n  corn_price nitrogen   revenue\n1          4        0 -472.1517\n\n[[3]]\n  corn_price nitrogen   revenue\n1          5        0 -590.1896\n\n[[4]]\n  corn_price nitrogen  revenue\n1          3      100 574.6345\n\n[[5]]\n  corn_price nitrogen  revenue\n1          4      100 766.1793\n\n[[6]]\n  corn_price nitrogen  revenue\n1          5      100 957.7242\n\n[[7]]\n  corn_price nitrogen  revenue\n1          3      200 700.3269\n\n[[8]]\n  corn_price nitrogen  revenue\n1          4      200 933.7692\n\n[[9]]\n  corn_price nitrogen  revenue\n1          5      200 1167.212\n\n[[10]]\n  corn_price nitrogen  revenue\n1          3      300 717.3375\n\n[[11]]\n  corn_price nitrogen  revenue\n1          4      300 956.4501\n\n[[12]]\n  corn_price nitrogen  revenue\n1          5      300 1195.563\n\n\nSuccessful! Now, for us to use the outcome for other purposes like further analysis and visualization, we would need to have all the results combined into a single data.frame instead of a list of data.frames. To do this, use either bind_rows() from the dplyr package or rbindlist() from the data.table package.\n\n#--- bind_rows ---#\nbind_rows(rev_data)\n\n   corn_price nitrogen   revenue\n1           3        0 -354.1138\n2           4        0 -472.1517\n3           5        0 -590.1896\n4           3      100  574.6345\n5           4      100  766.1793\n6           5      100  957.7242\n7           3      200  700.3269\n8           4      200  933.7692\n9           5      200 1167.2115\n10          3      300  717.3375\n11          4      300  956.4501\n12          5      300 1195.5626\n\n#--- rbindlist ---#\nrbindlist(rev_data)\n\n    corn_price nitrogen   revenue\n         &lt;num&gt;    &lt;num&gt;     &lt;num&gt;\n 1:          3        0 -354.1138\n 2:          4        0 -472.1517\n 3:          5        0 -590.1896\n 4:          3      100  574.6345\n 5:          4      100  766.1793\n 6:          5      100  957.7242\n 7:          3      200  700.3269\n 8:          4      200  933.7692\n 9:          5      200 1167.2115\n10:          3      300  717.3375\n11:          4      300  956.4501\n12:          5      300 1195.5626\n\n\n\nA.1.5 Do you really need to loop?\nActually, we should not have used for loop or lapply() in any of the examples above in practice5 This is because they can be easily vectorized. Vectorized operations are those that take vectors as inputs and work on each element of the vectors in parallel6.\n5 By the way, note that lapply() is no magic. It’s basically a for loop and not really any faster than for loop.6 This does not mean that the process is parallelized by using multiple cores.A typical example of a vectorized operation would be this:\n\n#--- define numeric vectors ---#\nx &lt;- 1:1000\ny &lt;- 1:1000\n\n#--- element wise addition ---#\nz_vec &lt;- x + y\n\nA non-vectorized version of the same calculation is this:\n\nz_la &lt;- lapply(1:1000, function(i) x[i] + y[i]) %&gt;% unlist()\n\n#--- check if identical with z_vec ---#\nall.equal(z_la, z_vec)\n\n[1] TRUE\n\n\nBoth produce the same results. However, R is written in a way that is much better at doing vectorized operations. Let’s time them using the microbenchmark() function from the microbenchmark package. Here, we do not unlist() after lapply() to just focus on the multiplication part.\n\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  #--- vectorized ---#\n  \"vectorized\" = {\n    x + y\n  },\n  #--- not vectorized ---#\n  \"not vectorized\" = {\n    lapply(1:1000, function(i) x[i] + y[i])\n  },\n  times = 100,\n  unit = \"ms\"\n)\n\nUnit: milliseconds\n           expr      min        lq      mean   median       uq      max neval\n     vectorized 0.002788 0.0029520 0.0034153 0.003034 0.003239 0.009471   100\n not vectorized 0.294298 0.3069055 0.3531637 0.319677 0.342473 2.046351   100\n cld\n  a \n   b\n\n\nAs you can see, the vectorized version is faster. The time difference comes from R having to conduct many more internal checks and hidden operations for the non-vectorized one7. Yes, we are talking about a fraction of milliseconds here. But, as the objects to operate on get larger, the difference between vectorized and non-vectorized operations can become substantial8.\n7 See this or this to have a better understanding of why non-vectorized operations can be slower than vectorized operations.8 See here for a good example of such a case. R is often regarded very slow compared to other popular software. But, many of such claims come from not vectorizing operations that can be vectorized. Indeed, many of the base and old R functions are written in C. More recent functions relies on C++ via the Rcpp package.The lapply() examples can be easily vectorized.\nInstead of this:\n\nlapply(1:1000, square_it)\n\nYou can just do this:\n\nsquare_it(1:1000)\n\nYou can also easily vectorize the revenue calculation demonstrated above. First, define the function differently so that revenue calculation can take corn price and nitrogen vectors and return a revenue vector.\n\ngen_rev_corn_short &lt;- function(corn_price, nitrogen) {\n\n  #--- calculate yield ---#\n  yield &lt;- 240 * (1 - exp(0.4 - 0.02 * nitrogen))\n\n  #--- calculate revenue ---#\n  revenue &lt;- corn_price * yield\n\n  return(revenue)\n}\n\nThen use the function to calculate revenue and assign it to a new variable in the parameters_data data.\n\nrev_data_2 &lt;- mutate(\n  parameters_data,\n  revenue = gen_rev_corn_short(corn_price, nitrogen)\n)\n\nLet’s compare the two:\n\nmicrobenchmark(\n  #--- vectorized ---#\n  \"vectorized\" = {\n    rev_data &lt;- mutate(parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen))\n  },\n  #--- not vectorized ---#\n  \"not vectorized\" = {\n    parameters_data$revenue &lt;- lapply(1:nrow(parameters_data), gen_rev_corn)\n  },\n  times = 100,\n  unit = \"ms\"\n)\n\nUnit: milliseconds\n           expr      min        lq      mean   median        uq      max neval\n     vectorized 0.270723 0.3026415 0.3716962 0.335093 0.3847645 2.208957   100\n not vectorized 0.888101 0.9615320 1.0799720 1.043019 1.1382010 2.856552   100\n cld\n  a \n   b\n\n\nYes, the vectorized version is faster. So, the lesson here is that if you can vectorize, then vectorize instead of using lapply(). But, of course, things cannot be vectorized in many cases.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Loop and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "chapters/A1-ParallelComputing.html#parcomp",
    "href": "chapters/A1-ParallelComputing.html#parcomp",
    "title": "Appendix A — Loop and Parallel Computing",
    "section": "\nA.2 Parallelization of embarrassingly parallel processes",
    "text": "A.2 Parallelization of embarrassingly parallel processes\nParallelization of computation involves distributing the task at hand to multiple cores so that multiple processes are done in parallel. Here, we learn how to parallelize computation in R. Our focus is on the so called embarrassingly parallel processes. Embarrassingly parallel processes refer to a collection of processes where each process is completely independent of any another. That is, one process does not use the outputs of any of the other processes. The example of integer squaring is embarrassingly parallel. In order to calculate \\(1^2\\), you do not need to use the result of \\(2^2\\) or any other squares. Embarrassingly parallel processes are very easy to parallelize because you do not have to worry about which process to complete first to make other processes happen. Fortunately, most of the processes you are interested in parallelizing fall under this category9.\n9 A good example of non-embarrassingly parallel process is dynamic optimization via backward induction. You need to know the optimal solution at \\(t = T\\), before you find the optimal solution at \\(t = T-1\\).10 There are many other options including the parallel, foreach packages.We will use the future_lapply() function from the future.apply package for parallelization10. Using the package, parallelization is a piece of cake as it is basically the same syntactically as lapply().\n\n#--- load packages ---#\nlibrary(future.apply)\n\nYou can find out how many cores you have available for parallel computation on your computer using the detectCores() function from the parallel package.\n\nlibrary(parallel)\n\n#--- number of all cores ---#\ndetectCores()\n\n[1] 20\n\n\nBefore we implement parallelized lapply(), we need to declare what backend process we will be using by plan(). Here, we use plan(multisession)11. In the plan() function, we can specify the number of workers. Here I will use the total number of cores less 112.\n11 If you are a Mac or Linux user, then the multicore is also available. The multicore process is faster than the multisession process. See this lecture note on parallel programming using R by Dr. Grant McDermott’s at the University of Oregon for the distinctions between the two and many other useful concepts for parallelization. However, multicore is considered less stable than multisession. At the time of this writing, if you run R through RStudio, multicore option is not permitted because of its instability.12 This way, you can have one more core available to do other tasks comfortably. However, if you don’t mind having your computer completely devoted to the processing task at hand, then there is no reason not to use all the cores.\nplan(multisession, workers = detectCores() - 1)\n\nfuture_lapply() works exactly like lapply().\n\nsq_ls &lt;- future_lapply(1:1000, function(x) x^2)\n\nThis is it. The only difference you see from the serialized processing using lapply() is that you changed the function name to future_lapply().\nOkay, now we know how we parallelize computation. Let’s check how much improvement in implementation time we got by parallelization.\n\nmicrobenchmark(\n  #--- parallelized ---#\n  \"parallelized\" = {\n    sq_ls &lt;- future_lapply(1:1000, function(x) x^2)\n  },\n  #--- non-parallelized ---#\n  \"not parallelized\" = {\n    sq_ls &lt;- lapply(1:1000, function(x) x^2)\n  },\n  times = 100,\n  unit = \"ms\"\n)\n\nUnit: milliseconds\n             expr         min          lq         mean       median          uq\n     parallelized 1224.751139 1238.172530 1269.8089344 1273.7972660 1292.373956\n not parallelized    0.227058    0.242105    0.2615107    0.2524575    0.263589\n         max neval cld\n 1359.612029   100  a \n    0.726069   100   b\n\n\nHmmmm, okay, so parallelization made the code slower… How could this be? This is because communicating jobs to each core takes some time as well. So, if each of the iterative processes is super fast (like this example where you just square a number), the time spent on communicating with the cores outweighs the time saving due to parallel computation. Parallelization is more beneficial when each of the repetitive processes takes long.\nOne of the very good use cases of parallelization is MC simulation. The following MC simulation tests whether the correlation between an independent variable and error term would cause bias (yes, we know the answer). The MC_sim function first generates a dataset (50,000 observations) according to the following data generating process:\n\\[\ny = 1 + x + v\n\\]\nwhere \\(\\mu \\sim N(0,1)\\), \\(x \\sim N(0,1) + \\mu\\), and \\(v \\sim N(0,1) + \\mu\\). The \\(\\mu\\) term cause correlation between \\(x\\) (the covariate) and \\(v\\) (the error term). It then estimates the coefficient on \\(x\\) vis OLS, and return the estimate. We would like to repeat this process 1,000 times to understand the property of the OLS estimators under the data generating process. This Monte Carlo simulation is embarrassingly parallel because each process is independent of any other.\n\n#--- repeat steps 1-3 B times ---#\nMC_sim &lt;- function(i) {\n  N &lt;- 50000 # sample size\n\n  #--- steps 1 and 2:  ---#\n  mu &lt;- rnorm(N) # the common term shared by both x and u\n  x &lt;- rnorm(N) + mu # independent variable\n  v &lt;- rnorm(N) + mu # error\n  y &lt;- 1 + x + v # dependent variable\n  data &lt;- data.table(y = y, x = x)\n\n  #--- OLS ---#\n  reg &lt;- lm(y ~ x, data = data) # OLS\n\n  #--- return the coef ---#\n  return(reg$coef[\"x\"])\n}\n\nLet’s run one iteration,\n\ntic()\nMC_sim(1)\ntoc()\n\n\n\n       x \n1.503353 \n\n\nelapsed \n   0.01 \n\n\nOkay, so it takes 0.01 second for one iteration. Now, let’s run this 1000 times with or without parallelization.\nNot parallelized\n\n#--- non-parallel ---#\ntic()\nMC_results &lt;- lapply(1:1000, MC_sim)\ntoc()\n\n\n\nelapsed \n 10.328 \n\n\nParallelized\n\n#--- parallel ---#\ntic()\nMC_results &lt;- future_lapply(1:1000, MC_sim)\ntoc()\n\n\n\nelapsed \n  2.666 \n\n\nAs you can see, parallelization makes it much quicker with a noticeable difference in elapsed time. We made the code 3.87 times faster. However, we did not make the process 19 times faster even though we used 19 cores for the parallelized process. This is because of the overhead associated with distributing tasks to the cores. The relative advantage of parallelization would be greater if each iteration took more time. For example, if you are running a process that takes about 2 minutes for 1000 times, it would take approximately 33 hours and 20 minutes. But, it may take only 4 hours if you parallelize it on 19 cores, or maybe even 2 hours if you run it on 30 cores.\n\nA.2.1 Mac or Linux users\nFor Mac users, parallel::mclapply() is just as compelling (or pbmclapply::pbmclapply() if you want to have a nice progress report, which is very helpful particularly when the process is long). It is just as easy to use as future_lapply() because its syntax is the same as lapply(). You can control the number of cores to employ by adding mc.cores option. Here is an example code that does the same MC simulations we conducted above:\n\n#--- mclapply ---#\nlibrary(parallel)\nMC_results &lt;- mclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)\n\n#--- or with progress bar ---#\nlibrary(pbmclapply)\nMC_results &lt;- pbmclapply(1:1000, MC_sim, mc.cores = detectCores() - 1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Loop and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "chapters/A2-ggplot2-appendix.html",
    "href": "chapters/A2-ggplot2-appendix.html",
    "title": "Appendix B — ggplot2 minimals",
    "section": "",
    "text": "Note: This section does not provide a complete treatment of the basics of the ggplot2 package. Rather, it provides the minimal knowledge of the package so that readers who are not familiar with the package can still understand the codes for map making presented in Chapter 7.\nThe ggplot2 package is a general and extensive data visualization tool. It is very popular among R users due to its elegance in and ease of use in generating high-quality figures. The ggplot2 package is designed following the “grammar of graphics,” which makes it possible to visualize data in an easy and consistent manner irrespective of the type of figures generated, whether it is a simple scatter plot or a complicated map. This means that learning the basics of how ggplot2 works directly helps in creating maps as well. This chapter goes over the basics of how ggplot2 works in general.\nIn ggplot2, you first specify what data to use and then specify how to use the data for visualization depending on what types of figures you intend to make using geom_*(). As a simple example, let’s use mpg data to create a simple scatter plot. Here is what mpg dataset looks like:\n\nlibrary(ggplot2)\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThe code below creates a scatter plot of displ and hwy variables in the mpg dataset.\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\nScatter plot where observations are color-differentiated by class\n\n\n\nHowever, this one does not work because color = class is outside of aes() and R does not look for class object inside mpg.\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy), color = class)\n\nError in `geom_point()`:\n! Problem while setting up geom aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `rep()`:\n! attempt to replicate an object of type 'builtin'\n\n\nYou can still specify the color that is applied universally to all the observations in the dataset like this:\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\nThese examples should clarify what aes() does: it makes the aesthetics of the figure data-dependent.\nIn the code to create Figure B.1, the default color option was used for color-differentiation by class. You can specify the color scheme using scale_*(). The scale_*() function generally takes the form o fscale_x_y(), where x is the type of aesthetics you want to control, and y is the method for specifying the color scheme. For example, in the code above, the type of aesthetics is color. And suppose we would like to use the brewer method. Then the scale function we should be using is scale_color_brewer(). The code below uses scale_color_brewer() and the palette option to specify the color scheme by ourselves.\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) +\n  scale_color_brewer(palette = 1)\n\n\n\n\n\n\nFigure B.1: Scatter plot where the color scheme is defined by the user\n\n\n\n\nAs you can see the color scheme is now changed. There are many other different types of pallets available.\nTo create a different type of figure than scatter plot, you can pick a different geom_*(). For example, geom_histogram() creates a histogram.\n\nggplot(data = mpg) + \n  geom_histogram(aes(x = hwy), color = \"blue\", fill = \"white\")\n\n\n\n\n\n\n\nYou can save a created figure (or more precisely the data underpins the figure) as an R object as follows:\n\n#--- save the figure to g_plot ---#\ng_plot &lt;- ggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) +\n  scale_color_brewer(palette = 1)\n\n#--- see the class ---#\nclass(g_plot)\n\n[1] \"gg\"     \"ggplot\"\n\n\nYou can call the saved object to see the figure.\n\ng_plot\n\n\n\n\n\n\n\nAnother important feature of ggplot2 is that you can add layers to an existing ggplot object by + geom_*(). For example, the following code adds the linear regression line to the plot:\n\ng_plot + \n  geom_smooth(aes(x = displ, y = hwy), method = \"lm\")\n\n\n\n\n\n\n\nThis feature makes it very easy to plot different spatial objects in a single map as we will find out later.\n“Faceting” is another useful feature of the package. Faceting splits the data into groups and generates a figure for each group where the aesthetics of the figures are consistent across the groups. Faceting can be done using facet_wrap() or facet_grid(). Here is an example using facet_wrap():\n\nggplot(data = mpg) + \n  geom_point(aes(x = displ, y = hwy, color = class)) + \n  geom_smooth(aes(x = displ, y = hwy), method = \"lm\") +\n  scale_color_brewer(palette = 1) +\n  facet_wrap(year ~ .) \n\n\n\n\n\n\n\nyear ~ . inside facet_wrap() tells R to split the data by year. The . in year ~ . means “no variable”.1 So, the above code splits the mpg data by year, applies the geom_point() and geom_smooth(), applies scale_color_brewer() to each of them, and then creates a figure for each group. The created figures are then presented side-by-side.2 This feature can be handy, for example, when you would like to display changes in land use over time where faceting is done by year.\n1 You can do two-way splits by supplying another categorical variable instead of . in year ~ ..2 You can change the orientation by using facet_wrap(. ~ year).While there are other important ggplot2 features to be aware of to make informative maps, I will not discuss them here. Rather, I will introduce them when they first appear in the lecture through examples. For those who are interested in learning the basics of ggplot2, there are numerous books written about it on the market. Some prominent ones are",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>ggplot2 minimals</span>"
    ]
  }
]